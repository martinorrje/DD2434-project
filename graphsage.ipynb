{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from load_data import load_toy\n",
    "from keras.layers import Dense\n",
    "from copy import deepcopy\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f8cafbe8110>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/opt/anaconda3/envs/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3572, in run_code\n",
      "    return outflag  File \"/var/folders/9_/tjgwhcvn6nq39lnwk8yyqhwc0000gn/T/ipykernel_14810/4016639901.py\", line 61, in <module>\n",
      "    X_train = scaler.fit_transform(batch_forward( total_graph['node_feats'], training_nodes, K, model, neighborhoods).numpy())  File \"/var/folders/9_/tjgwhcvn6nq39lnwk8yyqhwc0000gn/T/ipykernel_14810/721138015.py\", line 70, in batch_forward\n",
      "    conc = tf.concat([hN, hv], axis=0)  File \"/opt/anaconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 245, in wrapped\n",
      "    error_in_function=error_in_function)\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f8cafbe8110>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/opt/anaconda3/envs/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3572, in run_code\n",
      "    return outflag  File \"/var/folders/9_/tjgwhcvn6nq39lnwk8yyqhwc0000gn/T/ipykernel_14810/4016639901.py\", line 61, in <module>\n",
      "    X_train = scaler.fit_transform(batch_forward( total_graph['node_feats'], training_nodes, K, model, neighborhoods).numpy())  File \"/var/folders/9_/tjgwhcvn6nq39lnwk8yyqhwc0000gn/T/ipykernel_14810/721138015.py\", line 70, in batch_forward\n",
      "    conc = tf.concat([hN, hv], axis=0)  File \"/opt/anaconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 245, in wrapped\n",
      "    error_in_function=error_in_function)\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_name = \"toy\"\n",
    "data_dir = \"../Data/\" + dataset_name\n",
    "\n",
    "\n",
    "total_graph = load_toy(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#only consider node degree for now\n",
    "# node_features = np.zeros((total_graph['N_nodes'],2))\n",
    "# for i in range(total_graph['N_nodes']):\n",
    "#     degree = len(total_graph['edges'][i+1])\n",
    "#     node_features[i,0] = degree\n",
    "#     node_features[i,1] = total_graph['groups'][i+1][0]\n",
    "\n",
    "def get_similarity(total_graph):\n",
    "    \"\"\"Construct a dict of similar nodes, i.e. ones that are within distance 2 of each other\"\"\"\n",
    "    similarity_dict = {i:set() for i in range(total_graph['N_nodes'])}\n",
    "    for i in range(total_graph['N_nodes']):\n",
    "        if i%1000==0:\n",
    "            print(i)\n",
    "        nb = total_graph['edges'][i]\n",
    "        similarity_dict[i].update(nb)\n",
    "        for n in nb:\n",
    "            new_neighbors = total_graph['edges'][n]\n",
    "            similarity_dict[i].update(new_neighbors)\n",
    "            #for j in new_neighbors:\n",
    "            #    similarity_dict[i].update(total_graph['edges'][j])       \n",
    "    return similarity_dict\n",
    "\n",
    "sim_dict = get_similarity(total_graph)\n",
    "\n",
    "def get_positive_samples_old(edge_dict, nodes, sim_dict):\n",
    "    \"\"\"perform random walk starting from each node, and sample 1 from the nodes that occurred \"\"\"\n",
    "    walk_length = 5\n",
    "    walk = np.zeros((5), dtype=int)\n",
    "    N_nodes = len(nodes)\n",
    "    co_occuring_ind = np.zeros((N_nodes), dtype=int)\n",
    "    for ind, v in enumerate(nodes):\n",
    "        neighbors = edge_dict[v]\n",
    "        for i in range(walk_length):\n",
    "            new_node = random.choice(neighbors)\n",
    "            neighbors = edge_dict[new_node]\n",
    "            walk[i] = new_node\n",
    "        u = random.choice(walk)\n",
    "        co_occuring_ind[ind] = u\n",
    "    return co_occuring_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_degree_count(edge_dict, N_nodes):\n",
    "    degree_count = {i:0 for i in range(N_nodes)}\n",
    "    for i in range(N_nodes):\n",
    "        nb_count = len(edge_dict[i]) \n",
    "        degree_count[i] = nb_count\n",
    "    return degree_count\n",
    "\n",
    "def concat(a, b):\n",
    "    return tf.concat([a, b], axis=0)\n",
    "\n",
    "\n",
    "def get_positive_samples(edge_dict, nodes, sim_dict):\n",
    "    \"\"\"perform random walk starting from each node, and sample 1 from the nodes that occurred \"\"\"\n",
    "    N_nodes = len(nodes)\n",
    "    co_occuring_ind = []\n",
    "    for i, v in enumerate(nodes):\n",
    "        for j,u in enumerate(nodes):\n",
    "            if u in sim_dict[v]:\n",
    "                co_occuring_ind.append(j)\n",
    "                break       \n",
    "    return co_occuring_ind\n",
    "\n",
    "def get_negative_samples(total_graph, nodes, Q):\n",
    "    batch_size = len(nodes)\n",
    "    batch_indices = [i for i in range(batch_size)]\n",
    "    neg_ind = np.zeros((batch_size, Q), dtype=int)     \n",
    "    for j in range(Q):\n",
    "        for i, v in enumerate(nodes):\n",
    "            found_negative_edge = False\n",
    "            while not found_negative_edge:\n",
    "                ind = random.choice(batch_indices)\n",
    "                node = nodes[ind]\n",
    "                if not total_graph['adj_matrix'][node, v] and not total_graph['adj_matrix'][v, node]:\n",
    "                    neg_ind[i,j] = ind\n",
    "                    found_negative_edge = True\n",
    "    return neg_ind\n",
    "\n",
    "def compute_neighborhoods(edge_dict, N_nodes, nb_size, degree_count):\n",
    "    neighborhoods = [[] for i in range(N_nodes)]\n",
    "    for v, neighbors in edge_dict.items():\n",
    "        nb = len(neighbors)\n",
    "        sample_size = tf.minimum(nb_size, nb)\n",
    "        if sample_size==1:\n",
    "            sample_neighborhood = [edge_dict[v][0]]\n",
    "        else:\n",
    "            neighborhood_ind = tf.random.uniform((sample_size,), maxval=nb-1, dtype=tf.int32)\n",
    "            sample_neighborhood = tf.gather(neighbors, neighborhood_ind).numpy().tolist()\n",
    "        neighborhoods[v] = sample_neighborhood\n",
    "    return neighborhoods\n",
    "\n",
    "\n",
    "def batch_forward(node_features, batch_nodes, K, model, neighborhoods):\n",
    "    eps = 1e-9\n",
    "    B = [[] for k in range(K)]\n",
    "    B[K-1] = batch_nodes[:]\n",
    "    for k in range(K-1, 0, -1):\n",
    "        B[k-1] = B[k][:]\n",
    "        for node in batch_nodes:\n",
    "            B[k-1].extend(neighborhoods[node])\n",
    "            \n",
    "    h = node_features\n",
    "    N_nodes = h.shape[0]\n",
    "    h_updated = tf.TensorArray(dtype=tf.float32, size=N_nodes)\n",
    "    for k in range(K):\n",
    "        bk = B[k]\n",
    "        for i, v in enumerate(bk):        \n",
    "            neighborhood = neighborhoods[v]\n",
    "            hv = tf.reshape(h[v], (1, -1))\n",
    "            hN = tf.gather(h, neighborhood)\n",
    "            conc = tf.concat([hN, hv], axis=0)\n",
    "            aggregated = tf.cast(tf.reduce_mean(conc, axis=0, keepdims=True), dtype=tf.float32)\n",
    "            layer = model.layers[k]\n",
    "            hv = layer(aggregated)\n",
    "            h_updated = h_updated.write(v, hv)\n",
    "        \n",
    "        h = tf.squeeze(h_updated.stack())\n",
    "        h = h / (tf.norm(h, axis=1, keepdims=True)+eps)\n",
    "        h_updated = tf.TensorArray(dtype=tf.float32, size=N_nodes)\n",
    "\n",
    "    return tf.gather(h, batch_nodes)\n",
    "\n",
    "\n",
    "def compute_embedding(node_features, N_nodes, edge_dict, K, model):\n",
    "    degree_count = get_degree_count(edge_dict, N_nodes)\n",
    "    fixed_neighborhood_size = 5\n",
    "    h = node_features\n",
    "    h_updated = tf.TensorArray(dtype=tf.float32, size=N_nodes)\n",
    "    for k in range(K):\n",
    "        for v in range(N_nodes):\n",
    "            if fixed_neighborhood_size>degree_count[v]:\n",
    "                neighborhood = [edge_dict[v][0]]*fixed_neighborhood_size\n",
    "            else:\n",
    "                neighborhood_ind = tf.random.uniform((fixed_neighborhood_size,), maxval=degree_count[v]-1, dtype=tf.int32)\n",
    "                neighborhood = tf.gather(edge_dict[v], neighborhood_ind)\n",
    "            hv = tf.reshape(h[v], (1, -1))\n",
    "            hN = tf.gather(h, neighborhood)\n",
    "            hv = model(tf.reduce_mean(tf.concat([hN, hv], axis=0), axis=0, keepdims=True))\n",
    "            h_updated = h_updated.write(v, tf.reshape(hv, [-1]))\n",
    "        h = h_updated.stack()\n",
    "        h = h / tf.norm(h, axis=1, keepdims=True)\n",
    "        h_updated = tf.TensorArray(dtype=tf.float32, size=N_nodes)\n",
    "    return h\n",
    "\n",
    "\n",
    "def compute_loss(Z, total_graph, nodes, Q=5, sim_dict=None):\n",
    "    positive_samples_ind = get_positive_samples(total_graph['edges'], nodes, sim_dict)  # has shape (N_nodes,)\n",
    "    negative_samples_ind = get_negative_samples(total_graph, nodes, Q)    # has shape (N_nodes, Q)  \n",
    "    Z_pos = tf.gather(Z, positive_samples_ind)\n",
    "    dot = tf.reduce_sum(tf.multiply(Z, Z_pos), axis=1)\n",
    "    term1 = -tf.math.log(tf.math.sigmoid(dot))\n",
    "    term2 = 0\n",
    "    for i in range(Q):\n",
    "        Z_neg = tf.gather(Z, negative_samples_ind[:,i])\n",
    "        term2 -= tf.math.log(tf.math.sigmoid(-tf.reduce_sum(tf.multiply(Z, Z_neg), axis=1)))\n",
    "    loss = term1 + term2\n",
    "    return tf.reduce_sum(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f8c74fda950>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/opt/anaconda3/envs/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3572, in run_code\n",
      "    return outflag  File \"/var/folders/9_/tjgwhcvn6nq39lnwk8yyqhwc0000gn/T/ipykernel_14810/2464680125.py\", line 61, in <module>\n",
      "    X_train = scaler.fit_transform(batch_forward( total_graph['node_feats'], training_nodes, K, model, neighborhoods).numpy())  File \"/var/folders/9_/tjgwhcvn6nq39lnwk8yyqhwc0000gn/T/ipykernel_14810/721138015.py\", line 69, in batch_forward\n",
      "    hN = tf.gather(h, neighborhood)  File \"/opt/anaconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 245, in wrapped\n",
      "    error_in_function=error_in_function)\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f8c74fda950>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/opt/anaconda3/envs/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3572, in run_code\n",
      "    return outflag  File \"/var/folders/9_/tjgwhcvn6nq39lnwk8yyqhwc0000gn/T/ipykernel_14810/2464680125.py\", line 61, in <module>\n",
      "    X_train = scaler.fit_transform(batch_forward( total_graph['node_feats'], training_nodes, K, model, neighborhoods).numpy())  File \"/var/folders/9_/tjgwhcvn6nq39lnwk8yyqhwc0000gn/T/ipykernel_14810/721138015.py\", line 69, in batch_forward\n",
      "    hN = tf.gather(h, neighborhood)  File \"/opt/anaconda3/envs/venv/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 245, in wrapped\n",
      "    error_in_function=error_in_function)\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tf.Tensor(2900.5698, shape=(), dtype=float32)\n",
      "1 tf.Tensor(2882.1663, shape=(), dtype=float32)\n",
      "2 tf.Tensor(2782.7014, shape=(), dtype=float32)\n",
      "3 tf.Tensor(2720.6172, shape=(), dtype=float32)\n",
      "4 tf.Tensor(2736.8096, shape=(), dtype=float32)\n",
      "5 tf.Tensor(2675.27, shape=(), dtype=float32)\n",
      "6 tf.Tensor(2642.4268, shape=(), dtype=float32)\n",
      "7 tf.Tensor(2597.4336, shape=(), dtype=float32)\n",
      "8 tf.Tensor(2576.5996, shape=(), dtype=float32)\n",
      "9 tf.Tensor(2536.595, shape=(), dtype=float32)\n",
      "10 tf.Tensor(2519.543, shape=(), dtype=float32)\n",
      "11 tf.Tensor(2500.136, shape=(), dtype=float32)\n",
      "12 tf.Tensor(2479.4822, shape=(), dtype=float32)\n",
      "13 tf.Tensor(2443.276, shape=(), dtype=float32)\n",
      "14 tf.Tensor(2417.5837, shape=(), dtype=float32)\n",
      "15 tf.Tensor(2411.29, shape=(), dtype=float32)\n",
      "16 tf.Tensor(2392.9, shape=(), dtype=float32)\n",
      "17 tf.Tensor(2370.1404, shape=(), dtype=float32)\n",
      "18 tf.Tensor(2346.3213, shape=(), dtype=float32)\n",
      "19 tf.Tensor(2319.8677, shape=(), dtype=float32)\n",
      "20 tf.Tensor(2306.5684, shape=(), dtype=float32)\n",
      "21 tf.Tensor(2272.434, shape=(), dtype=float32)\n",
      "22 tf.Tensor(2265.5674, shape=(), dtype=float32)\n",
      "23 tf.Tensor(2243.6565, shape=(), dtype=float32)\n",
      "24 tf.Tensor(2230.9578, shape=(), dtype=float32)\n",
      "25 tf.Tensor(2218.7405, shape=(), dtype=float32)\n",
      "26 tf.Tensor(2194.7107, shape=(), dtype=float32)\n",
      "27 tf.Tensor(2197.023, shape=(), dtype=float32)\n",
      "28 tf.Tensor(2161.535, shape=(), dtype=float32)\n",
      "29 tf.Tensor(2164.06, shape=(), dtype=float32)\n",
      "30 tf.Tensor(2157.8582, shape=(), dtype=float32)\n",
      "31 tf.Tensor(2140.2427, shape=(), dtype=float32)\n",
      "32 tf.Tensor(2131.191, shape=(), dtype=float32)\n",
      "33 tf.Tensor(2114.4146, shape=(), dtype=float32)\n",
      "34 tf.Tensor(2103.483, shape=(), dtype=float32)\n",
      "35 tf.Tensor(2094.3813, shape=(), dtype=float32)\n",
      "36 tf.Tensor(2101.67, shape=(), dtype=float32)\n",
      "37 tf.Tensor(2081.4304, shape=(), dtype=float32)\n",
      "38 tf.Tensor(2071.722, shape=(), dtype=float32)\n",
      "39 tf.Tensor(2055.7974, shape=(), dtype=float32)\n",
      "40 tf.Tensor(2064.3652, shape=(), dtype=float32)\n",
      "41 tf.Tensor(2037.7653, shape=(), dtype=float32)\n",
      "42 tf.Tensor(2030.8923, shape=(), dtype=float32)\n",
      "43 tf.Tensor(2025.9199, shape=(), dtype=float32)\n",
      "44 tf.Tensor(2014.2072, shape=(), dtype=float32)\n",
      "45 tf.Tensor(2024.1451, shape=(), dtype=float32)\n",
      "46 tf.Tensor(2018.9109, shape=(), dtype=float32)\n",
      "47 tf.Tensor(2018.2458, shape=(), dtype=float32)\n",
      "48 tf.Tensor(2007.0305, shape=(), dtype=float32)\n",
      "49 tf.Tensor(2000.1405, shape=(), dtype=float32)\n",
      "50 tf.Tensor(1999.5231, shape=(), dtype=float32)\n",
      "51 tf.Tensor(1997.5492, shape=(), dtype=float32)\n",
      "52 tf.Tensor(1999.0573, shape=(), dtype=float32)\n",
      "53 tf.Tensor(1993.4509, shape=(), dtype=float32)\n",
      "54 tf.Tensor(1998.095, shape=(), dtype=float32)\n",
      "55 tf.Tensor(1989.2844, shape=(), dtype=float32)\n",
      "56 tf.Tensor(1970.4308, shape=(), dtype=float32)\n",
      "57 tf.Tensor(1253.5541, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def split_into_random_batches(my_list, N):\n",
    "    random.shuffle(my_list)  # Shuffle the list in-place\n",
    "    return [my_list[i:i + N] for i in range(0, len(my_list), N)]\n",
    "\n",
    "feature_dim = total_graph['node_feats'].shape[1]\n",
    "output_dim = 32\n",
    "K = 2\n",
    "Q = 10\n",
    "fixed_nb_size = 4\n",
    "num_epochs = 1\n",
    "batch_size = 256\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, epsilon=1e-7)\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(tf.keras.Input(shape=(feature_dim,)))\n",
    "for k in range(K):\n",
    "    model.add(Dense(units=output_dim, activation='relu', bias_initializer=\"zeros\"))\n",
    "model(total_graph['node_feats'][0].reshape((1,50)))\n",
    "\n",
    "\n",
    "loss_over_epochs = []\n",
    "for epoch in range(num_epochs):\n",
    "    neighborhoods = compute_neighborhoods(total_graph['edges'], total_graph['N_nodes'], fixed_nb_size, get_degree_count(total_graph['edges'], total_graph['N_nodes']))\n",
    "    batches = split_into_random_batches(total_graph['nodes'], batch_size)\n",
    "    print(len(batches))\n",
    "    for i, batch_nodes in enumerate(batches):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            \n",
    "            Z = batch_forward(total_graph['node_feats'], batch_nodes, K, model, neighborhoods)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = compute_loss(Z, total_graph, batch_nodes, Q, sim_dict)\n",
    "            print(i,loss)\n",
    "\n",
    "        # Calculate gradients\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "     \n",
    "        # Update model weights\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    loss_over_epochs.append(loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NC_5folds = {}\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "nodes = np.array([i for i in range(total_graph['N_nodes'])])\n",
    "for i, (train_index, test_index) in enumerate(kf.split(nodes)):  \n",
    "    NC_5folds[i] = {\"train\":list(nodes[train_index]), \"test\":list(nodes[test_index])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.7793874172185431 0.7793874172185431\n",
      "0.7793874172185431\n",
      "0.7793874172185431\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from  sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "mb = MultiLabelBinarizer(classes=[i for i in range(121)])\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def onehot(y, nclasses):\n",
    "    Y = np.zeros((y.shape[0], nclasses), dtype=int)\n",
    "    for i in range(y.shape[0]):\n",
    "        c = y[i]\n",
    "        Y[i,c-1] =  1\n",
    "    return Y\n",
    "\n",
    "\n",
    "def precision_and_recall(Y_true, Y_pred, nclasses):\n",
    "    # count true positives and false positives and false negatives\n",
    "    #nclasses = len(Y_true[0])\n",
    "    TP_list = [0]*nclasses\n",
    "    FP_list = [0]*nclasses\n",
    "    FN_list = [0]*nclasses\n",
    "    for j in range(nclasses):\n",
    "       for i, pred in enumerate(Y_pred):\n",
    "            if pred[j]==1 and Y_true[i][j]==1:\n",
    "                TP_list[j] += 1\n",
    "            elif pred[j]==1 and  Y_true[i][j]==0:\n",
    "                FP_list[j] += 1\n",
    "            elif pred[j]==0 and Y_true[i][j]==1:\n",
    "                FN_list[j] += 1 \n",
    "\n",
    "    return TP_list, FP_list, FN_list\n",
    "\n",
    "def compute_f1_macro(Y_true, Y_pred, nclasses):\n",
    "    #nclasses = len(Y_true[0])\n",
    "    TP_list, FP_list, FN_list = precision_and_recall(Y_true, Y_pred, nclasses)\n",
    "    f1_scores = [0]*nclasses\n",
    "    for k in range(nclasses):\n",
    "        if TP_list[k]==0:\n",
    "            continue\n",
    "        f1_scores[k] = TP_list[k]/(TP_list[k]+0.5*(FP_list[k]+FN_list[k])) \n",
    "    return np.sum(f1_scores)/nclasses\n",
    "\n",
    "\n",
    "def compute_f1_micro(Y_true, Y_pred, nclasses):\n",
    "    TP_list, FP_list, FN_list = precision_and_recall(Y_true, Y_pred, nclasses)\n",
    "    TP = np.sum(TP_list)\n",
    "    FP = np.sum(FP_list)\n",
    "    FN = np.sum(FN_list)\n",
    "    return TP/(TP + 0.5*(FN+FP))\n",
    "\n",
    "f1_macro_list = []\n",
    "f1_micro_list = []\n",
    "\n",
    "# 5-fold cross validation\n",
    "for i in range(1):\n",
    "    print(i)\n",
    "    training_nodes = NC_5folds[i]['train']\n",
    "    test_nodes = NC_5folds[i]['test']\n",
    "    X_train = scaler.fit_transform(batch_forward(total_graph['node_feats'], training_nodes, K, model, neighborhoods).numpy())\n",
    "    X_test = scaler.fit_transform(batch_forward(total_graph['node_feats'], test_nodes, K, model, neighborhoods))\n",
    "    # For the datasets that only have one one label per node, it gives better results to not use multioutputclassifier\n",
    "    if not total_graph['Multioutput']:\n",
    "        Y_train_sequence = np.array([total_graph['groups'][node][0]  for node in training_nodes],dtype=int)\n",
    "        Y_test_sequence = np.array([total_graph['groups'][node][0] for node in test_nodes], dtype=int)\n",
    "        log_reg = LogisticRegression(multi_class=\"ovr\", max_iter=200)\n",
    "        Y_train = Y_train_sequence\n",
    "        Y_test = Y_test_sequence\n",
    "        log_reg.fit(X_train, Y_train)\n",
    "        Y_pred = log_reg.predict(X_test)\n",
    "        Y_pred = onehot(Y_pred, total_graph['N_classes'])\n",
    "        Y_test = onehot(Y_test, total_graph['N_classes'])\n",
    "    else:\n",
    "        Y_train_sequence = [total_graph['groups'][node]  for node in training_nodes]\n",
    "        Y_test_sequence = [total_graph['groups'][node] for node in test_nodes]\n",
    "        Y_train = mb.fit_transform(Y_train_sequence)\n",
    "        Y_test = mb.fit_transform(Y_test_sequence)\n",
    "        log_reg = MultiOutputClassifier(LogisticRegression(multi_class=\"ovr\", max_iter=200))\n",
    "        log_reg.fit(X_train, Y_train)\n",
    "        Y_pred = log_reg.predict(X_test)\n",
    "        \n",
    "    f1_macro = compute_f1_macro(Y_test, Y_pred, total_graph['N_classes'])\n",
    "    f1_micro = compute_f1_micro(Y_test, Y_pred,total_graph['N_classes'])\n",
    "\n",
    "    f1_macro_list.append(f1_macro)\n",
    "    f1_micro_list.append(f1_micro)\n",
    "    print(f1_macro, f1_micro)\n",
    "    \n",
    "print(np.mean(f1_micro_list))\n",
    "print(np.mean(f1_macro_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
