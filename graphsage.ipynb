{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-03 18:39:35.470664: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from load_data import load_toy, load_blogcatalog\n",
    "from keras.layers import Dense, Dropout\n",
    "from copy import deepcopy\n",
    "from tensorflow.keras.models import Sequential\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_name = \"BlogCatalog\"\n",
    "data_dir = \"../Data/\" + dataset_name\n",
    "\n",
    "total_graph = load_blogcatalog(data_dir)\n",
    "#total_graph = load_toy(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_similarity(total_graph):\n",
    "    \"\"\"Construct a dict of similar nodes, i.e. ones that are within distance 3 of each other\"\"\"\n",
    "    similarity_dict = {i:set() for i in range(total_graph['N_nodes'])}\n",
    "    for i in range(total_graph['N_nodes']):\n",
    "        if i%1000==0:\n",
    "            print(i)\n",
    "        nb = total_graph['edges'][i]\n",
    "        similarity_dict[i].update(nb)\n",
    "        for n in nb:\n",
    "            new_neighbors = total_graph['edges'][n]\n",
    "            similarity_dict[i].update(new_neighbors)\n",
    "            #for j in new_neighbors:\n",
    "            #    similarity_dict[i].update(total_graph['edges'][j])       \n",
    "    return similarity_dict\n",
    "\n",
    "sim_dict = get_similarity(total_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_features(total_graph, mode=\"degree\"):\n",
    "    if mode==\"degree\":\n",
    "        node_features = np.zeros((total_graph['N_nodes'],2))\n",
    "        for i in range(total_graph['N_nodes']):\n",
    "            neighbors = total_graph['edges'][i]\n",
    "            degree = len(neighbors)\n",
    "            node_features[i,0] = degree\n",
    "            second_neighbors_count = 0\n",
    "            for n in neighbors:\n",
    "                second_neighbors_count += len(total_graph['edges'][n])\n",
    "            node_features[i,1] = second_neighbors_count\n",
    "    elif mode==\"degree_dist\":\n",
    "        node_features = total_graph['adj_matrix']  \n",
    "    elif mode==\"node_nr\":\n",
    "        node_features = np.zeros((total_graph['N_nodes'],total_graph['N_nodes']))\n",
    "        for i in range(total_graph['N_nodes']):\n",
    "            node_features[i,i] = 1\n",
    "    return node_features\n",
    "\n",
    "\n",
    "def concat(a, b):\n",
    "    return tf.concat([a, b], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "def get_positive_samples(batch_nodes, edge_dict):\n",
    "    \"\"\"perform a random walk of length 5\"\"\"\n",
    "    pos_samples = [0]*len(batch_nodes)\n",
    "    for i,node in enumerate(batch_nodes):\n",
    "        v = node\n",
    "        for w in range(5):\n",
    "            v = random.choice(edge_dict[v])\n",
    "        pos_samples[i] = v\n",
    "    return pos_samples\n",
    "\n",
    "\n",
    "def get_negative_samples(total_graph, batch_nodes, Q):\n",
    "    batch_size = len(batch_nodes)\n",
    "    neg_ind = np.zeros((batch_size, Q), dtype=int)     \n",
    "    for j in range(Q):\n",
    "        for i, v in enumerate(batch_nodes):\n",
    "            found_negative_edge = False\n",
    "            while not found_negative_edge:\n",
    "                node = random.choice(batch_nodes)\n",
    "                if total_graph['adj_matrix'][node, v]==0 and total_graph['adj_matrix'][v, node]==0:\n",
    "                    neg_ind[i, j] = node\n",
    "                    found_negative_edge = True\n",
    "    return neg_ind\n",
    "\n",
    "\n",
    "def compute_neighborhoods(edge_dict, N_nodes, nb_size):\n",
    "    neighborhoods = [[] for i in range(N_nodes)]\n",
    "    for v, neighbors in edge_dict.items():\n",
    "        nb = len(neighbors)\n",
    "        sample_size = tf.minimum(nb_size, nb)\n",
    "        if sample_size==1:\n",
    "            sample_neighborhood = [edge_dict[v][0]]\n",
    "        else:\n",
    "            neighborhood_ind = tf.random.uniform((sample_size,), maxval=nb-1, dtype=tf.int32)\n",
    "            sample_neighborhood = tf.gather(neighbors, neighborhood_ind).numpy().tolist()\n",
    "        neighborhoods[v] = sample_neighborhood\n",
    "    return neighborhoods\n",
    "\n",
    "\n",
    "def batch_forward(node_features, batch_nodes, K, model, neighborhoods):\n",
    "    eps = 1e-9\n",
    "    B = [[] for k in range(K)]\n",
    "    B[K-1] = batch_nodes[:]\n",
    "    for k in range(K-1, 0, -1):\n",
    "        B[k-1] = B[k][:]\n",
    "        for node in B[k][:]:\n",
    "            B[k-1].extend(neighborhoods[node])\n",
    "\n",
    "    h = node_features \n",
    "    N_nodes = h.shape[0]\n",
    "    for k in range(K):\n",
    "        h_updated = tf.TensorArray(dtype=tf.float32, size=N_nodes)\n",
    "        for v in B[k]:        \n",
    "            neighborhood = neighborhoods[v]\n",
    "            hv = tf.reshape(h[v], (1, -1))\n",
    "            hN = tf.gather(h, neighborhood)\n",
    "            conc = tf.concat([hN, hv], axis=0)\n",
    "            aggregated = tf.reduce_mean(conc, axis=0, keepdims=True) #tf.cast(tf.reduce_mean(conc, axis=0, keepdims=True), dtype=tf.float32)\n",
    "            #layer = model.layers[k]\n",
    "            #hv = layer(aggregated)\n",
    "            hv = model(aggregated)\n",
    "            h_updated = h_updated.write(v, hv)\n",
    "\n",
    "        h = tf.squeeze(h_updated.stack())\n",
    "        h_updated.mark_used()\n",
    "        # Some implementations don't use normalize and relu at final step?\n",
    "        #if k<K-1:\n",
    "        #    h = h / (tf.norm(h, axis=1, keepdims=True)+eps)\n",
    "    # we return entire h, containing 0 for the nodes outside batch\n",
    "    return h\n",
    "\n",
    "\n",
    "def compute_loss(Z_tot, Z_pos, Q, batch_nodes, negative_samples_ind):\n",
    "    Z = tf.gather(Z_tot, batch_nodes)\n",
    "    dot = tf.reduce_sum(tf.math.multiply(Z, Z_pos), axis=1)\n",
    "    term1 = -tf.math.log(tf.math.sigmoid(dot))\n",
    "    term2 = 0\n",
    "    for i in range(Q):\n",
    "        Z_neg = tf.gather(Z_tot, negative_samples_ind[:,i])\n",
    "        term2 -= tf.math.log(tf.math.sigmoid(-tf.reduce_sum(tf.math.multiply(Z, Z_neg), axis=1)))\n",
    "    #loss = term1 + term2\n",
    "    return term1, term2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "0.0 tf.Tensor(127.121994, shape=(), dtype=float32) tf.Tensor(127.121994, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.024390243902439025 tf.Tensor(96.80124, shape=(), dtype=float32) tf.Tensor(96.80124, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.04878048780487805 tf.Tensor(61.3106, shape=(), dtype=float32) tf.Tensor(61.3106, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.07317073170731707 tf.Tensor(37.44089, shape=(), dtype=float32) tf.Tensor(37.44089, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.0975609756097561 tf.Tensor(21.919987, shape=(), dtype=float32) tf.Tensor(21.919987, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.12195121951219512 tf.Tensor(10.892364, shape=(), dtype=float32) tf.Tensor(10.892364, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.14634146341463414 tf.Tensor(10.049408, shape=(), dtype=float32) tf.Tensor(10.049408, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.17073170731707318 tf.Tensor(7.3267937, shape=(), dtype=float32) tf.Tensor(7.3267937, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.1951219512195122 tf.Tensor(2.9206948, shape=(), dtype=float32) tf.Tensor(2.9206948, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.21951219512195122 tf.Tensor(4.2262993, shape=(), dtype=float32) tf.Tensor(4.2262993, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.24390243902439024 tf.Tensor(1.6256586, shape=(), dtype=float32) tf.Tensor(1.6256586, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.2682926829268293 tf.Tensor(2.037414, shape=(), dtype=float32) tf.Tensor(2.037414, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.2926829268292683 tf.Tensor(2.1016235, shape=(), dtype=float32) tf.Tensor(2.1016235, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.3170731707317073 tf.Tensor(1.2405643, shape=(), dtype=float32) tf.Tensor(1.2405643, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.34146341463414637 tf.Tensor(1.025331, shape=(), dtype=float32) tf.Tensor(1.025331, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.36585365853658536 tf.Tensor(1.7902466, shape=(), dtype=float32) tf.Tensor(1.7902466, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.3902439024390244 tf.Tensor(0.8984184, shape=(), dtype=float32) tf.Tensor(0.8984184, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.4146341463414634 tf.Tensor(0.62729615, shape=(), dtype=float32) tf.Tensor(0.62729615, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.43902439024390244 tf.Tensor(0.31523228, shape=(), dtype=float32) tf.Tensor(0.31523228, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.4634146341463415 tf.Tensor(0.8952139, shape=(), dtype=float32) tf.Tensor(0.8952139, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.4878048780487805 tf.Tensor(0.65699, shape=(), dtype=float32) tf.Tensor(0.65699, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.5121951219512195 tf.Tensor(1.2043703, shape=(), dtype=float32) tf.Tensor(1.2043703, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.5365853658536586 tf.Tensor(0.48662668, shape=(), dtype=float32) tf.Tensor(0.48662668, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.5609756097560976 tf.Tensor(1.2786278, shape=(), dtype=float32) tf.Tensor(1.2786278, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.5853658536585366 tf.Tensor(0.7187482, shape=(), dtype=float32) tf.Tensor(0.7187482, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.6097560975609756 tf.Tensor(0.727979, shape=(), dtype=float32) tf.Tensor(0.727979, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.6341463414634146 tf.Tensor(1.2207375, shape=(), dtype=float32) tf.Tensor(1.2207375, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.6585365853658537 tf.Tensor(0.27904618, shape=(), dtype=float32) tf.Tensor(0.27904618, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.6829268292682927 tf.Tensor(0.3547926, shape=(), dtype=float32) tf.Tensor(0.3547926, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.7073170731707317 tf.Tensor(1.0114222, shape=(), dtype=float32) tf.Tensor(1.0114222, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.7317073170731707 tf.Tensor(0.44609964, shape=(), dtype=float32) tf.Tensor(0.44609964, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.7560975609756098 tf.Tensor(0.39723334, shape=(), dtype=float32) tf.Tensor(0.39723334, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.7804878048780488 tf.Tensor(0.27468193, shape=(), dtype=float32) tf.Tensor(0.27468193, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.8048780487804879 tf.Tensor(0.1012043, shape=(), dtype=float32) tf.Tensor(0.1012043, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.8292682926829268 tf.Tensor(1.7751601, shape=(), dtype=float32) tf.Tensor(1.7751601, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.8536585365853658 tf.Tensor(1.4604845, shape=(), dtype=float32) tf.Tensor(1.4604845, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.8780487804878049 tf.Tensor(0.3940119, shape=(), dtype=float32) tf.Tensor(0.3940119, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.9024390243902439 tf.Tensor(0.013458522, shape=(), dtype=float32) tf.Tensor(0.013458522, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.926829268292683 tf.Tensor(0.4373067, shape=(), dtype=float32) tf.Tensor(0.4373067, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.9512195121951219 tf.Tensor(0.5821308, shape=(), dtype=float32) tf.Tensor(0.5821308, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "0.975609756097561 tf.Tensor(0.11014509, shape=(), dtype=float32) tf.Tensor(0.11014509, shape=(), dtype=float32) tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def split_into_random_batches(my_list, N):\n",
    "    random.shuffle(my_list)  # Shuffle the list in-place\n",
    "    return [my_list[i:i + N] for i in range(0, len(my_list), N)]\n",
    "\n",
    "\n",
    "K = 1\n",
    "def train_embedding(total_graph, epochs):\n",
    "    node_features_og = get_node_features(total_graph, \"degree_dist\")    #total_graph['node_feats']\n",
    "    feature_dim = node_features_og.shape[1]\n",
    "    output_dim = 64\n",
    "    fixed_nb_size = 10\n",
    "    num_epochs = epochs\n",
    "    batch_size = 256\n",
    "    Q = 1\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, epsilon=1e-6) \n",
    "\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.Input(shape=(feature_dim,)))\n",
    "    for k in range(K-1):\n",
    "        model.add(Dense(units=output_dim, activation='relu', bias_initializer=\"zeros\"))         \n",
    "    model.add(Dense(units=output_dim, activation='relu', bias_initializer=\"zeros\"))   \n",
    "\n",
    "    # Build the model by passing an input vector\n",
    "    model(node_features_og[0].reshape((1,feature_dim)))\n",
    "\n",
    "    loss_over_epochs = []\n",
    "    neighborhoods = compute_neighborhoods(total_graph['edges'], total_graph['N_nodes'], fixed_nb_size)\n",
    "    for epoch in range(num_epochs):   \n",
    "        batches = split_into_random_batches(total_graph['nodes'], batch_size)\n",
    "        node_features = deepcopy(node_features_og)\n",
    "        print(len(batches))\n",
    "        for i, batch_nodes in enumerate(batches):\n",
    "            pos_samples = get_positive_samples(batch_nodes, total_graph['edges'])  # has shape (N_nodes,)\n",
    "            negative_samples_ind = get_negative_samples(total_graph, batch_nodes, Q)  if Q else None  # has shape (N_nodes, Q)  \n",
    "            Z_pos = tf.gather(batch_forward(node_features, pos_samples, K, model, neighborhoods), pos_samples)\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward pass\n",
    "                Z_tot = batch_forward(node_features, batch_nodes, K, model, neighborhoods)\n",
    "                # Calculate the loss\n",
    "                t1, t2 = compute_loss(Z_tot, Z_pos, Q, batch_nodes, negative_samples_ind)\n",
    "                loss = tf.reduce_sum(t1+t2)\n",
    "                print(i/len(batches), loss, tf.reduce_sum(t1), tf.reduce_sum(t2))\n",
    "\n",
    "            # Calculate gradients\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "            # Update model weights\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        loss_over_epochs.append(loss)    \n",
    "    \n",
    "    return model, loss_over_epochs, neighborhoods\n",
    "\n",
    "\n",
    "model, losses, neighborhoods = train_embedding(total_graph, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NC_5folds = {}\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "nodes = np.array([i for i in range(total_graph['N_nodes'])])\n",
    "for i, (train_index, test_index) in enumerate(kf.split(nodes)):  \n",
    "    NC_5folds[i] = {\"train\":list(nodes[train_index]), \"test\":list(nodes[test_index])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "klar med z train\n",
      "hej\n",
      "59 7 2869\n",
      "0.016345352399220553 0.0394121576486306\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3560: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from  sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def onehot(y, nclasses):\n",
    "    Y = np.zeros((y.shape[0], nclasses), dtype=int)\n",
    "    for i in range(y.shape[0]):\n",
    "        c = y[i]\n",
    "        Y[i,c-1] =  1\n",
    "    return Y\n",
    "\n",
    "\n",
    "def precision_and_recall(Y_true, Y_pred, nclasses):\n",
    "    # count true positives and false positives and false negatives\n",
    "    TP_list = [0]*nclasses\n",
    "    FP_list = [0]*nclasses\n",
    "    FN_list = [0]*nclasses\n",
    "    for j in range(nclasses):\n",
    "       for i, pred in enumerate(Y_pred):\n",
    "            if pred[j]==1 and Y_true[i][j]==1:\n",
    "                TP_list[j] += 1\n",
    "            elif pred[j]==1 and  Y_true[i][j]==0:\n",
    "                FP_list[j] += 1\n",
    "            elif pred[j]==0 and Y_true[i][j]==1:\n",
    "                FN_list[j] += 1 \n",
    "\n",
    "    return TP_list, FP_list, FN_list\n",
    "\n",
    "def compute_f1_macro(Y_true, Y_pred, nclasses):\n",
    "    TP_list, FP_list, FN_list = precision_and_recall(Y_true, Y_pred, nclasses)\n",
    "    f1_scores = [0]*nclasses\n",
    "    for k in range(nclasses):\n",
    "        if TP_list[k]==0:\n",
    "            continue\n",
    "        f1_scores[k] = TP_list[k]/(TP_list[k]+0.5*(FP_list[k]+FN_list[k])) \n",
    "    return np.sum(f1_scores)/nclasses\n",
    "\n",
    "\n",
    "def compute_f1_micro(Y_true, Y_pred, nclasses):\n",
    "    TP_list, FP_list, FN_list = precision_and_recall(Y_true, Y_pred, nclasses)\n",
    "    TP = np.sum(TP_list)\n",
    "    FP = np.sum(FP_list)\n",
    "    FN = np.sum(FN_list)\n",
    "    print(TP, FP, FN)\n",
    "    return TP/(TP + 0.5*(FN+FP))\n",
    "\n",
    "\n",
    "N_classes = total_graph['N_classes']\n",
    "mb = MultiLabelBinarizer(classes=[i for i in range(N_classes)])\n",
    "f1_macro_list = []\n",
    "f1_micro_list = []\n",
    "\n",
    "# 5-fold cross validation\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    node_features = get_node_features(total_graph, \"degree_dist\") \n",
    "    training_nodes = NC_5folds[i]['train']\n",
    "    test_nodes = NC_5folds[i]['test']\n",
    "    Z_train = tf.gather(batch_forward(node_features, training_nodes, K, model, neighborhoods), training_nodes).numpy()\n",
    "    print(\"klar med z train\")\n",
    "    Z_test = tf.gather(batch_forward(node_features, test_nodes, K, model, neighborhoods), test_nodes).numpy()\n",
    "    X_train = Z_train\n",
    "    X_test = Z_test\n",
    "    # For the datasets that only have one one label per node, it gives better results to not use multioutputclassifier\n",
    "    if not total_graph['Multioutput']:\n",
    "        Y_train_sequence = np.array([total_graph['groups'][node][0]  for node in training_nodes],dtype=int)\n",
    "        Y_test_sequence = np.array([total_graph['groups'][node][0] for node in test_nodes], dtype=int)\n",
    "        log_reg = LogisticRegression(multi_class=\"ovr\", max_iter=200)\n",
    "        Y_train = Y_train_sequence\n",
    "        Y_test = Y_test_sequence\n",
    "        log_reg.fit(X_train, Y_train)\n",
    "        Y_pred = log_reg.predict(X_test)\n",
    "        Y_pred = onehot(Y_pred, total_graph['N_classes'])\n",
    "        Y_test = onehot(Y_test, total_graph['N_classes'])\n",
    "    else:\n",
    "        print(\"hej\")\n",
    "        Y_train_sequence = [total_graph['groups'][node]  for node in training_nodes]\n",
    "        Y_test_sequence = [total_graph['groups'][node] for node in test_nodes]\n",
    "        Y_train = mb.fit_transform(Y_train_sequence)\n",
    "        Y_test = mb.fit_transform(Y_test_sequence)\n",
    "        log_reg = MultiOutputClassifier(SGDClassifier(max_iter=200))   #multi_class=\"ovr\",\n",
    "        log_reg.fit(X_train, Y_train)\n",
    "        Y_pred = log_reg.predict(X_test)\n",
    "  \n",
    "    f1_macro = compute_f1_macro(Y_test, Y_pred, N_classes)\n",
    "    f1_micro = compute_f1_micro(Y_test, Y_pred, N_classes)\n",
    "\n",
    "    f1_macro_list.append(f1_macro)\n",
    "    f1_micro_list.append(f1_micro)\n",
    "    print(f1_macro, f1_micro)\n",
    "    sys.exit()\n",
    "    \n",
    "print(np.mean(f1_micro_list))\n",
    "print(np.mean(f1_macro_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
