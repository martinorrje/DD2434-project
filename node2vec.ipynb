{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from sklearn.model_selection import KFold\n",
    "from copy import deepcopy\n",
    "\n",
    "# node2vec paper\n",
    "#https://arxiv.org/pdf/1607.00653.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data.load_data import load_flickr, load_blogcatalog, load_youtube, load_reddit\n",
    "dataset_name = \"Reddit\"\n",
    "data_dir = \"./Data/\" + dataset_name\n",
    "\n",
    "total_graph = load_reddit(data_dir)\n",
    "#total_graph = load_youtube(data_dir)\n",
    "#total_graph = load_flickr(data_dir)\n",
    "#total_graph = load_blogcatalog(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create 5-fold validation set for NC\n",
    "\n",
    "NC_5folds = {}\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "nodes = np.array([i+1 for i in range(total_graph['N_nodes'])])\n",
    "for i, (train_index, test_index) in enumerate(kf.split(nodes)):  \n",
    "    NC_5folds[i] = {\"train\":nodes[train_index], \"test\":nodes[test_index]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create training and test graphs for LP\n",
    "# Select 50% of the edges for training, leave remaining for testing.\n",
    "# Want the remaining graph to still be connected, so we only remove edges if there are several neighbors\n",
    "\n",
    "def create_test_graph(total_graph):\n",
    "    n_test_samples = int(total_graph['N_edges']*0.5)\n",
    "    print(n_test_samples)\n",
    "    training_graph = deepcopy(total_graph[\"edges\"])\n",
    "    LP_test_X = [(1,1)]*n_test_samples*2\n",
    "    LP_test_Y = [0]*n_test_samples*2\n",
    "    counter = 0\n",
    "\n",
    "    high_degree_nodes = []\n",
    "    n_neighbors = {i+1:0 for i in range(total_graph['N_nodes'])}\n",
    "    for i in range(total_graph['N_nodes']):\n",
    "        nb_count = len(total_graph['edges'][i+1]) \n",
    "        n_neighbors[i+1] = nb_count\n",
    "        if nb_count>1:\n",
    "            high_degree_nodes.append(i+1)\n",
    "\n",
    "    high_degree_nodes = np.array(high_degree_nodes)\n",
    "    while counter<n_test_samples:\n",
    "        sampled_nodes = np.random.choice(high_degree_nodes, 1000)\n",
    "        for node1 in sampled_nodes:\n",
    "            if n_neighbors[node1]>1:\n",
    "                node2 = np.random.choice(training_graph[node1])\n",
    "                if n_neighbors[node2]>1:\n",
    "\n",
    "                    # Add to test data\n",
    "                    LP_test_X[counter] = (node1, node2)\n",
    "                    LP_test_Y[counter] = 1\n",
    "\n",
    "                    # remove edge from training graph\n",
    "                    training_graph[node1].remove(node2)\n",
    "                    training_graph[node2].remove(node1)\n",
    "\n",
    "                    # decrease neighbor count\n",
    "                    n_neighbors[node1] -= 1\n",
    "                    n_neighbors[node2] -= 1\n",
    "\n",
    "                    counter += 1          \n",
    "                    if counter%int(n_test_samples/50)==0:\n",
    "                        print(counter/n_test_samples)\n",
    "    \n",
    "\n",
    "    n_negative_edges = 0\n",
    "    while n_negative_edges<n_test_samples-1000:\n",
    "        sampled_nodes = np.random.choice(total_graph['nodes'], 1000)\n",
    "        for i in range(500):\n",
    "            node1 = sampled_nodes[2*i]\n",
    "            node2 = sampled_nodes[2*i+1]\n",
    "            if not node1 in total_graph['edges'][node2]:\n",
    "                try:\n",
    "                    LP_test_X[counter+n_negative_edges] = (node1, node2)\n",
    "                    LP_test_Y[counter+n_negative_edges] = 0\n",
    "                except:\n",
    "                    LP_test_X.append((node1, node2))\n",
    "                    LP_test_Y.append(0)\n",
    "                    print(\"appended edge\")\n",
    "                n_negative_edges += 1\n",
    "        \n",
    "            if n_negative_edges%int(n_test_samples/10)==0:\n",
    "                print(n_negative_edges/n_test_samples)\n",
    "\n",
    "    return LP_test_X, LP_test_Y, training_graph\n",
    "\n",
    "LP_test_X, LP_test_Y, training_graph = create_test_graph(total_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When created the test set, we add remaining edges to the training set\n",
    "# and add negative edges to balance the training data\n",
    "def balance_training_graph(training_graph, total_graph):\n",
    "    n_test_samples = int(total_graph['N_edges']*0.5)\n",
    "    LP_train_X = []\n",
    "    LP_train_Y = []\n",
    "    added_edges = {i+1:{} for i in range(total_graph['N_nodes'])}\n",
    "    for node, neighbors in training_graph.items():\n",
    "        for nb in neighbors:\n",
    "            if not added_edges[node].get(nb, False):\n",
    "                added_edges[node][nb] = True\n",
    "                added_edges[nb][node] = True\n",
    "                LP_train_X.append((node, nb))\n",
    "                LP_train_Y.append(1)\n",
    "\n",
    "    n_negative_edges = 0\n",
    "    while n_negative_edges < n_test_samples-1000:\n",
    "        sampled_nodes = np.random.choice(total_graph['nodes'], 1000)\n",
    "        for i in range(500):\n",
    "            node1 = sampled_nodes[2*i]\n",
    "            node2 = sampled_nodes[2*i+1]\n",
    "            if not node1 in total_graph['edges'][node2]:\n",
    "                LP_train_X.append((node1, node2))\n",
    "                LP_train_Y.append(0)\n",
    "                n_negative_edges += 1\n",
    "\n",
    "            if n_negative_edges%int(n_test_samples/10)==0:\n",
    "                print(n_negative_edges/n_test_samples)\n",
    "        \n",
    "    return LP_train_X, LP_train_Y\n",
    "    \n",
    "LP_train_X, LP_train_Y = balance_training_graph(training_graph, total_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "       \n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "\n",
    "def alias_sample(prev, current, neighbors, p, q):\n",
    "    n_neighbors = len(neighbors)\n",
    "    probability_dist = np.zeros((n_neighbors+2))\n",
    "    available_nodes = np.zeros_like(probability_dist)\n",
    "    probability_dist[0] = 1/p   # returning to the same node we came from\n",
    "    available_nodes[0] = prev\n",
    "    probability_dist[1] = 1     # staying at current node\n",
    "    available_nodes[1] = current\n",
    "    probability_dist[2::] = np.ones_like(neighbors)*1/q\n",
    "    available_nodes[2::] = neighbors\n",
    "    norm = np.sum(probability_dist)\n",
    "    p_normed = probability_dist/norm\n",
    "    sampled_node = np.random.choice(available_nodes,  p=p_normed)\n",
    "    return sampled_node\n",
    "    \n",
    "\n",
    "def learn_features(G, dim, walks_per_node, walk_length, context_size, p, q, SGD_epochs):\n",
    "    walks = [[]]*walks_per_node*G['N_nodes']\n",
    "    c = 0\n",
    "    for i in range(walks_per_node):\n",
    "        print(i)\n",
    "        for node in G[\"nodes\"]:\n",
    "            walk = node2vec_walk(G, node, walk_length, p, q)\n",
    "            walks[c] = walk\n",
    "            c += 1\n",
    "            if node%int(G[\"N_nodes\"]/100)==0:\n",
    "                print(node/G['N_nodes'])\n",
    "                \n",
    "    print(len(walks), len[walks[0]])\n",
    "    f = SDG(walks, context_size, dim, SGD_epochs)\n",
    "    return f\n",
    "\n",
    "\n",
    "def node2vec_walk(G, start_node, walk_length, p, q):\n",
    "    walk = np.zeros((walk_length+1,), dtype=int)\n",
    "    walk[0] = start_node\n",
    "    for i in range(walk_length):\n",
    "        curr = walk[i]\n",
    "        if i==0:\n",
    "            prev = start_node\n",
    "        else:\n",
    "            prev = walk[i-1]\n",
    "\n",
    "        neighbors = G['edges'][curr]\n",
    "        sample = alias_sample(prev, curr, neighbors, p, q)\n",
    "        walk[i+1] = sample\n",
    "    return walk\n",
    "    \n",
    "\n",
    "def SDG(walks, context_size=10, dim=128, n_epochs=5):\n",
    "    \"\"\"Use Word2Vec with SGD to learn embedding based on walks\"\"\"\n",
    "    #sg=1 tells it to use skip-gram algorithm, min_count=0 tells it to not skip \"word\" that occur only 1 time   \n",
    "    model = Word2Vec(sentences=walks, vector_size=dim, window=context_size, min_count=0, sg=1, workers=8, epochs=n_epochs, compute_loss=True, callbacks=[EpochLogger()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters taken from original node2vec paper:\n",
    "dim = 64    # should be 128\n",
    "walks_per_node = 5  #should be 10\n",
    "walk_length = 20    # should be 80\n",
    "context_size = 10\n",
    "# From Khosla et al. these piwere the best performing settings in most cases:\n",
    "p = 0.25\n",
    "q = 4\n",
    "SGD_epochs = 1\n",
    "\n",
    "USE_PRETRAINED = False\n",
    "if USE_PRETRAINED:\n",
    "    embedding_model = Word2Vec.load(\"./Results/{}/{}}.model\".format(dataset_name, dataset_name))\n",
    "else:\n",
    "    embedding_model = learn_features(total_graph, dim, walks_per_node, walk_length, context_size, p, q, SGD_epochs)\n",
    "    if SAVE_BOOL:\n",
    "        embedding_model.save(\"./Results/{}/{}.model\".format(dataset_name, dataset_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for evaluation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_and_recall(Y_true, Y_pred):\n",
    "    # count true positives and false positives and false negatives\n",
    "    nclasses = len(Y_true[0])\n",
    "    TP_list = [0]*nclasses\n",
    "    FP_list = [0]*nclasses\n",
    "    FN_list = [0]*nclasses\n",
    "    for j in range(nclasses):\n",
    "       for i, pred in enumerate(Y_pred):\n",
    "            if pred[j]==1 and Y_true[i][j]==1:\n",
    "                TP_list[j] += 1\n",
    "            elif pred[j]==1 and  Y_true[i][j]==0:\n",
    "                FP_list[j] += 1\n",
    "            elif pred[j]==0 and Y_true[i][j]==1:\n",
    "                FN_list[j] += 1 \n",
    "\n",
    "    return TP_list, FP_list, FN_list\n",
    "\n",
    "def compute_f1_macro(Y_true, Y_pred):\n",
    "    nclasses = len(Y_true[0])\n",
    "    TP_list, FP_list, FN_list = precision_and_recall(Y_true, Y_pred)\n",
    "    f1_scores = [0]*nclasses\n",
    "    for k in range(nclasses):\n",
    "        if TP_list[k]==0:\n",
    "            continue\n",
    "        f1_scores[k] = TP_list[k]/(TP_list[k]+0.5*(FP_list[k]+FN_list[k])) \n",
    "    return np.sum(f1_scores)/nclasses\n",
    "\n",
    "\n",
    "def compute_f1_micro(Y_true, Y_pred):\n",
    "    TP_list, FP_list, FN_list = precision_and_recall(Y_true, Y_pred)\n",
    "    TP = np.sum(TP_list)\n",
    "    FP = np.sum(FP_list)\n",
    "    FN = np.sum(FN_list)\n",
    "    return TP/(TP + 0.5*(FN+FP))\n",
    "\n",
    "\n",
    "def compute_accuracy(Y_true, Y_pred):\n",
    "    n_correct = 0\n",
    "    n_tot = 0\n",
    "    nclasses = len(Y_true[0])\n",
    "    for i, pred in enumerate(Y_pred):\n",
    "        for j in range(nclasses):\n",
    "            n_tot += 1\n",
    "            if pred[j]==Y_true[i][j]:\n",
    "                n_correct += 1\n",
    "    return n_correct/n_tot\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def get_edge_representation(fu,fv):\n",
    "    return sigmoid(np.dot(fu,fv))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate NC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "mb = MultiLabelBinarizer(classes=[i+1 for i in range(total_graph['N_classes'])])\n",
    "\n",
    "f1_macro_list = []\n",
    "f1_micro_list = []\n",
    "accuracy_scores_list = []\n",
    "# 5-fold cross validation\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    training_nodes = NC_5folds[i]['train']\n",
    "    test_nodes = NC_5folds[i]['test']\n",
    "    X_train = np.array([embedding_model.wv[node] for node in training_nodes], dtype=object)\n",
    "    X_test = np.array([embedding_model.wv[node] for node in test_nodes], dtype=object)\n",
    "    Y_train_sequence = np.array([total_graph['groups'][node]  for node in training_nodes], dtype=object)\n",
    "    Y_test_sequence = np.array([total_graph['groups'][node] for node in test_nodes], dtype=object)\n",
    "    Y_train = mb.fit_transform(Y_train_sequence)\n",
    "    Y_test = mb.fit_transform(Y_test_sequence)\n",
    "    log_reg = MultiOutputClassifier(LogisticRegression(multi_class=\"ovr\"))\n",
    "    log_reg.fit(X_train, Y_train)\n",
    "    Y_pred = log_reg.predict(X_test)\n",
    "    acc = compute_accuracy(Y_test, Y_pred)\n",
    "    f1_macro = compute_f1_macro(Y_test, Y_pred)\n",
    "    f1_micro = compute_f1_micro(Y_test, Y_pred)\n",
    "    accuracy_scores_list.append(acc)\n",
    "    f1_macro_list.append(f1_macro)\n",
    "    f1_micro_list.append(f1_micro)\n",
    "    \n",
    "print(np.mean(f1_micro_list))\n",
    "print(np.mean(f1_macro_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(\"./Results/BlogCatalog/metrics.csv\", \"w\") as file:\n",
    "    settings_str = \"Results for Node2vec embedding generated with p={}, q={}, walk length={}, walks per node={}, sgd_epochs={}\\n\".format(p,q,\n",
    "    walk_length, walks_per_node, SGD_epochs)\n",
    "    file.write(settings_str)\n",
    "    header = \"Dataset; F1 macro; F1 micro; ROC-AUC \\n\"\n",
    "    file.write(header)\n",
    "    data_row = \"{dataset};{f1mac};{f1mic};{roc}\".format(dataset=dataset_name, f1mac=np.mean(f1_macro_list), f1mic=np.mean(f1_micro_list), roc=roc_auc)\n",
    "    file.write(data_row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate LP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = LP_train_Y\n",
    "Y_test = LP_test_Y\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# build representation of edge datasets using inner product of the representation of the two nodes\n",
    "X_train = np.zeros((len(LP_train_X), 1))\n",
    "for i, edge in enumerate(LP_train_X):\n",
    "    u = edge[0]\n",
    "    v = edge[1]\n",
    "    X_train[i] = get_edge_representation(embedding_model.wv[u], embedding_model.wv[v])\n",
    "X_test = np.zeros((len(LP_test_X), 1))\n",
    "for i, edge in enumerate(LP_test_X):\n",
    "    u = edge[0]\n",
    "    v = edge[1]\n",
    "    X_test[i] = get_edge_representation(embedding_model.wv[u], embedding_model.wv[v])\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, Y_train)\n",
    "Y_probs = classifier.predict_proba(X_test)[:,1]\n",
    "roc_auc = roc_auc_score(Y_test, Y_probs)\n",
    "print(roc_auc)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2f5f294937e1f47dd6e010afb2ca0c96836afcb29d9a31a278c78890f03e991"
  },
  "kernelspec": {
   "display_name": "Python 3.7.16 64-bit ('venv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
