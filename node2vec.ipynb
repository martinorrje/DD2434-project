{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from random import sample \n",
    "import sys\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from sklearn.model_selection import KFold\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "10042\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"BlogCatalog\"\n",
    "data_dir = \"./Data/BlogCatalog-dataset/data\"\n",
    "with open(data_dir+\"/groups.csv\", \"r\") as file:\n",
    "    N_groups = len(file.readlines())\n",
    "with open(data_dir+\"/nodes.csv\", \"r\") as file:\n",
    "    N_nodes = len(file.readlines())\n",
    "\n",
    "total_graph = {\"edges\":{i+1:[] for i in range(N_nodes)}, \"nodes\":[i+1 for i in range(N_nodes)], \n",
    "               \"groups\":{i+1:[] for i in range(N_nodes)},  \"N_nodes\":N_nodes}\n",
    "\n",
    "\n",
    "N_classes = 0\n",
    "with open(data_dir+'/groups.csv', \"r\") as file:\n",
    "    N_classes = len(file.readlines())\n",
    "print(N_classes)\n",
    "\n",
    "\n",
    "with open(data_dir+\"/edges.csv\", \"r\") as file:\n",
    "    N_edges = 0\n",
    "    for line in file.readlines():\n",
    "        node1  = int(line.split(\",\")[0])\n",
    "        node2 = int(line.split(\",\")[1])\n",
    "        total_graph['edges'][node1].append(node2)\n",
    "        total_graph['edges'][node2].append(node1)\n",
    "        N_edges += 1\n",
    "\n",
    "with open(data_dir+\"/group-edges.csv\", \"r\") as file:\n",
    "    for line in file.readlines():\n",
    "        node  = int(line.split(\",\")[0])\n",
    "        group = int(line.split(\",\")[1])\n",
    "        total_graph['groups'][node].append(group)\n",
    "\n",
    "good_nodes = []\n",
    "for i in range(N_nodes):\n",
    "    node = i+1\n",
    "    if len(total_graph['edges'][node])>1:\n",
    "        good_nodes.append(node)\n",
    "\n",
    "print(len(good_nodes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create 5-fold validation set\n",
    "\n",
    "NC_5folds = {}\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "nodes = np.array([i+1 for i in range(N_nodes)])\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(nodes)):\n",
    "    \n",
    "    NC_5folds[i] = {\"train\":nodes[train_index], \"test\":nodes[test_index]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 50% of the edges for training, leave remaining for testing.\n",
    "# Want the remaining graph to still be connected, so we only remove edges if there are several neighbors\n",
    "n_test_samples = int(N_edges*0.5)\n",
    "LP_data = {\"train_edges\":deepcopy(total_graph[\"edges\"]), \"test_edges\":{}}\n",
    "counter = 0\n",
    "while counter<n_test_samples:\n",
    "    node1 = np.random.choice(good_nodes)\n",
    "    node1_neighbors = LP_data['train_edges'][node1]\n",
    "    if len(node1_neighbors)>1:\n",
    "        node2 = np.random.choice(node1_neighbors)\n",
    "        node2_neighbors = LP_data['train_edges'][node2]\n",
    "        if len(node2_neighbors)>1:\n",
    "            node1_neighbors.remove(node2)\n",
    "            node2_neighbors.remove(node1)\n",
    "            if not LP_data['test_edges'].get(node1):\n",
    "                LP_data['test_edges'][node1] = []\n",
    "            if not LP_data['test_edges'].get(node2):\n",
    "                LP_data['test_edges'][node2] = []\n",
    "            LP_data[\"test_edges\"][node2].append(node1)\n",
    "            LP_data[\"test_edges\"][node1].append(node2)\n",
    "            counter += 1\n",
    "            \n",
    "    if counter%int(n_test_samples/10)==0:\n",
    "        print(counter/n_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "       \n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "\n",
    "\n",
    "def alias_sample(prev, current, neighbors, p, q):\n",
    "    n_neighbors = len(neighbors)\n",
    "    probability_dist = np.zeros((n_neighbors+2))\n",
    "    available_nodes = [0]*(len(neighbors)+2)\n",
    "    probability_dist[0] = 1/p   # returning to the same node we came from\n",
    "    available_nodes[0] = prev\n",
    "    probability_dist[1] = 1     # staying at current node\n",
    "    available_nodes[1] = current\n",
    "    probability_dist[2::] = np.ones_like(neighbors)*1/q\n",
    "    available_nodes[2::] = neighbors\n",
    "    norm = np.sum(probability_dist)\n",
    "    p_normed = probability_dist/norm\n",
    "    sampled_node = np.random.choice(available_nodes,  p=p_normed)\n",
    "    return sampled_node\n",
    "    \n",
    "\n",
    "def learn_features(G, dim, walks_per_node, walk_length, context_size, p, q, SGD_epochs):\n",
    "    walks = [[] for i in range(walks_per_node)]\n",
    "    for i in range(walks_per_node):\n",
    "        print(i)\n",
    "        for node in G[\"nodes\"]:\n",
    "            walk = node2vec_walk(G, node, walk_length, p, q)\n",
    "            walks.append(walk)\n",
    "    f = SDG(walks, context_size, dim, SGD_epochs)\n",
    "    return f\n",
    "\n",
    "\n",
    "def node2vec_walk(G, start_node, walk_length, p, q):\n",
    "    walk = [start_node]\n",
    "    for i in range(walk_length):\n",
    "        curr = walk[-1]\n",
    "        if i==0:\n",
    "            prev = start_node\n",
    "        else:\n",
    "            prev = walk[-2]\n",
    "        neighbors = G['edges'][curr]\n",
    "        sample = alias_sample(prev, curr, neighbors, p, q)\n",
    "        walk.append(sample)\n",
    "    return walk\n",
    "    \n",
    "\n",
    "def SDG(walks, context_size=10, dim=128, n_epochs=5):\n",
    "    \"\"\"Use Word2Vec with SGD to learn embedding based on walks\"\"\"\n",
    "    #sg=1 tells it to use skip-gram algorithm, min_count=0 tells it to not skip \"word\" that occur only 1 time   \n",
    "    model = Word2Vec(sentences=walks, vector_size=dim, window=context_size, min_count=0, sg=1, workers=8, epochs=n_epochs, compute_loss=True, callbacks=[EpochLogger()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Epoch #0 start\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parameters taken from original node2vec paper:\n",
    "dim = 128    # should be 128\n",
    "walks_per_node = 10\n",
    "walk_length = 80    # should be 80\n",
    "context_size = 10\n",
    "# From Khosla et al. these were the best performing settings in most cases:\n",
    "p = 0.25\n",
    "q = 4\n",
    "SGD_epochs = 1\n",
    "\n",
    "SAVE_BOOL = True\n",
    "\n",
    "embedding_model = learn_features(total_graph, dim, walks_per_node, walk_length, context_size, p, q, SGD_epochs)\n",
    "\n",
    "if SAVE_BOOL:\n",
    "    embedding_model.save(\"./Results/BlogCatalog/blogcatalog.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_and_recall(Y_true, Y_pred):\n",
    "    # count true positives and false positives and false negatives\n",
    "    nclasses = len(Y_true[0])\n",
    "    TP_list = [0]*nclasses\n",
    "    FP_list = [0]*nclasses\n",
    "    FN_list = [0]*nclasses\n",
    "    for j in range(nclasses):\n",
    "       for i, pred in enumerate(Y_pred):\n",
    "            if pred[j]==1 and Y_true[i][j]==1:\n",
    "                TP_list[j] += 1\n",
    "            elif pred[j]==1 and  Y_true[i][j]==0:\n",
    "                FP_list[j] += 1\n",
    "            elif pred[j]==0 and Y_true[i][j]==1:\n",
    "                FN_list[j] += 1 \n",
    "\n",
    "    return TP_list, FP_list, FN_list\n",
    "\n",
    "def compute_f1_macro(Y_true, Y_pred):\n",
    "    nclasses = len(Y_true[0])\n",
    "    TP_list, FP_list, FN_list = precision_and_recall(Y_true, Y_pred)\n",
    "    f1_scores = [0]*nclasses\n",
    "    for k in range(nclasses):\n",
    "        if TP_list[k]==0:\n",
    "            continue\n",
    "        f1_scores[k] = TP_list[k]/(TP_list[k]+0.5*(FP_list[k]+FN_list[k])) \n",
    "    return np.sum(f1_scores)/nclasses\n",
    "\n",
    "\n",
    "def compute_f1_micro(Y_true, Y_pred):\n",
    "    TP_list, FP_list, FN_list = precision_and_recall(Y_true, Y_pred)\n",
    "    TP = np.sum(TP_list)\n",
    "    FP = np.sum(FP_list)\n",
    "    FN = np.sum(FN_list)\n",
    "    return TP/(TP + 0.5*(FN+FP))\n",
    "\n",
    "\n",
    "def compute_accuracy(Y_true, Y_pred):\n",
    "    n_correct = 0\n",
    "    n_tot = 0\n",
    "    nclasses = len(Y_true[0])\n",
    "    for i, pred in enumerate(Y_pred):\n",
    "        for j in range(nclasses):\n",
    "            n_tot += 1\n",
    "            if pred[j]==Y_true[i][j]:\n",
    "                n_correct += 1\n",
    "    return n_correct/n_tot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance of embedding on node prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.2933387375465004\n",
      "0.16219503330439097\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "mb = MultiLabelBinarizer(classes=[i+1 for i in range(N_classes)])\n",
    "\n",
    "f1_macro_list = []\n",
    "f1_micro_list = []\n",
    "accuracy_scores_list = []\n",
    "# 5-fold cross validation\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    training_nodes = NC_5folds[i]['train']\n",
    "    test_nodes = NC_5folds[i]['test']\n",
    "    X_train = np.array([embedding_model.wv[node] for node in training_nodes], dtype=object)\n",
    "    X_test = np.array([embedding_model.wv[node] for node in test_nodes], dtype=object)\n",
    "    Y_train_sequence = np.array([total_graph['groups'][node]  for node in training_nodes], dtype=object)\n",
    "    Y_test_sequence = np.array([total_graph['groups'][node] for node in test_nodes], dtype=object)\n",
    "    Y_train = mb.fit_transform(Y_train_sequence)\n",
    "    Y_test = mb.fit_transform(Y_test_sequence)\n",
    "    log_reg = MultiOutputClassifier(LogisticRegression(multi_class=\"ovr\"))\n",
    "    log_reg.fit(X_train, Y_train)\n",
    "    Y_pred = log_reg.predict(X_test)\n",
    "    acc = compute_accuracy(Y_test, Y_pred)\n",
    "    f1_macro = compute_f1_macro(Y_test, Y_pred)\n",
    "    f1_micro = compute_f1_micro(Y_test, Y_pred)\n",
    "    accuracy_scores_list.append(zcore)\n",
    "    f1_macro_list.append(f1_macro)\n",
    "    f1_micro_list.append(f1_micro)\n",
    "    \n",
    "print(np.mean(f1_micro_list))\n",
    "print(np.mean(f1_macro_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_BOOL:\n",
    "    with open(\"./Results/BlogCatalog/metrics.csv\", \"w\") as file:\n",
    "        settings_str = \"Node2vec embedding generated with p={}, q={}, walk length={}, walks per node={}, sgd_epochs={}\\n\".format(p,q,\n",
    "        walk_length, walks_per_node, SGD_epochs)\n",
    "        file.write(settings_str)\n",
    "        header = \"Dataset; Total Accuracy; F1 macro; F1 micro\\n\"\n",
    "        file.write(header)\n",
    "        data_row = \"{dataset};{acc};{f1mac};{f1mic}\".format(dataset=dataset_name, acc=np.mean(accuracy_scores_list), \n",
    "        f1mac=np.mean(f1_macro_list), f1mic=np.mean(f1_micro_list))\n",
    "        file.write(data_row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2f5f294937e1f47dd6e010afb2ca0c96836afcb29d9a31a278c78890f03e991"
  },
  "kernelspec": {
   "display_name": "Python 3.7.16 64-bit ('venv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}