{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from sklearn.model_selection import KFold\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "# node2vec paper\n",
    "#https://arxiv.org/pdf/1607.00653.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'/Artificial_Intelligence/Vision_and_Pattern_Recognition/': 1, '/Operating_Systems/Distributed/': 2, '/Data_Structures__Algorithms_and_Theory/Randomized/': 3, '/Data_Structures__Algorithms_and_Theory/Parallel/': 4, '/Programming/Compiler_Design/': 5, '/Networking/Protocols/': 6, '/Encryption_and_Compression/Compression/': 7, '/Human_Computer_Interaction/Graphics_and_Virtual_Reality/': 8, '/Artificial_Intelligence/Machine_Learning/Probabilistic_Methods/': 9, '/Artificial_Intelligence/Knowledge_Representation/': 10, '/Artificial_Intelligence/Planning/': 11, '/Databases/Deductive/': 12, '/Data_Structures__Algorithms_and_Theory/Sorting/': 13, '/Human_Computer_Interaction/Multimedia/': 14, '/Databases/Query_Evaluation/': 15, '/Databases/Performance/': 16, '/Artificial_Intelligence/Games_and_Search/': 17, '/Artificial_Intelligence/Machine_Learning/Case-Based/': 18, '/Artificial_Intelligence/Data_Mining/': 19, '/Artificial_Intelligence/Machine_Learning/Theory/': 20, '/Programming/Logic/': 21, '/Programming/Object_Oriented/': 22, '/Networking/Wireless/': 23, '/Networking/Routing/': 24, '/Data_Structures__Algorithms_and_Theory/Quantum_Computing/': 25, '/Operating_Systems/Memory_Management/': 26, '/Databases/Object_Oriented/': 27, '/Artificial_Intelligence/Robotics/': 28, '/Data_Structures__Algorithms_and_Theory/Logic/': 29, '/Artificial_Intelligence/Speech/': 30, '/Artificial_Intelligence/Agents/': 31, '/Artificial_Intelligence/Machine_Learning/Reinforcement_Learning/': 32, '/Programming/Debugging/': 33, '/Operating_Systems/Fault_Tolerance/': 34, '/Artificial_Intelligence/Machine_Learning/Neural_Networks/': 35, '/Encryption_and_Compression/Encryption/': 36, '/Encryption_and_Compression/Security/': 37, '/Data_Structures__Algorithms_and_Theory/Formal_Languages/': 38, '/Hardware_and_Architecture/Logic_Design/': 39, '/Programming/Functional/': 40, '/Information_Retrieval/Filtering/': 41, '/Artificial_Intelligence/Theorem_Proving/': 42, '/Hardware_and_Architecture/VLSI/': 43, '/Databases/Relational/': 44, '/Hardware_and_Architecture/Microprogramming/': 45, '/Operating_Systems/Realtime/': 46, '/Data_Structures__Algorithms_and_Theory/Computational_Complexity/': 47, '/Data_Structures__Algorithms_and_Theory/Computational_Geometry/': 48, '/Artificial_Intelligence/NLP/': 49, '/Programming/Software_Development/': 50, '/Human_Computer_Interaction/Cooperative/': 51, '/Artificial_Intelligence/Machine_Learning/Rule_Learning/': 52, '/Artificial_Intelligence/Machine_Learning/Genetic_Algorithms/': 53, '/Programming/Semantics/': 54, '/Programming/Garbage_Collection/': 55, '/Hardware_and_Architecture/Input_Output_and_Storage/': 56, '/Information_Retrieval/Retrieval/': 57, '/Hardware_and_Architecture/Memory_Structures/': 58, '/Artificial_Intelligence/Expert_Systems/': 59, '/Databases/Temporal/': 60, '/Human_Computer_Interaction/Interface_Design/': 61, '/Networking/Internet/': 62, '/Information_Retrieval/Digital_Library/': 63, '/Programming/Java/': 64, '/Hardware_and_Architecture/High_Performance_Computing/': 65, '/Human_Computer_Interaction/Wearable_Computers/': 66, '/Data_Structures__Algorithms_and_Theory/Hashing/': 67, '/Databases/Concurrency/': 68, '/Hardware_and_Architecture/Distributed_Architectures/': 69, '/Information_Retrieval/Extraction/': 70}\n"
     ]
    }
   ],
   "source": [
    "from load_data import load_flickr, load_blogcatalog, load_youtube, load_reddit, load_cora\n",
    "dataset_name = \"Cora\"\n",
    "data_dir = \"../Data/\" + dataset_name\n",
    "\n",
    "#total_graph = load_reddit(data_dir)\n",
    "#total_graph = load_youtube(data_dir)\n",
    "#total_graph = load_flickr(data_dir)\n",
    "#total_graph = load_blogcatalog(data_dir)\n",
    "total_graph = load_cora(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create 5-fold validation set for NC\n",
    "\n",
    "\n",
    "NC_5folds = {}\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "nodes = np.array([i+1 for i in range(total_graph['N_nodes'])])\n",
    "for i, (train_index, test_index) in enumerate(kf.split(nodes)):  \n",
    "    NC_5folds[i] = {\"train\":nodes[train_index], \"test\":nodes[test_index]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training and testing graphs for LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting graphs\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "balancing test graph\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "balancing training graph\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select 50% of the edges for training, leave remaining for testing.\n",
    "# Want the remaining graph to still be connected, so we only remove edges if there are several neighbors\n",
    "\n",
    "def split_graphs(total_graph, directed=False):\n",
    "    print(\"splitting graphs\")\n",
    "    n_test_samples = int(total_graph['N_edges']*0.5)\n",
    "    training_graph_unbalanced = deepcopy(total_graph[\"edges\"])\n",
    "    test_graph_unbalanced = {i+1:[] for i in range(total_graph['N_nodes'])}\n",
    "    LP_test_X = [(1,1)]*n_test_samples*2\n",
    "    LP_test_Y = [0]*n_test_samples*2\n",
    "    counter = 0\n",
    "\n",
    "    high_degree_nodes = []\n",
    "    n_neighbors = {i+1:0 for i in range(total_graph['N_nodes'])}\n",
    "    for i in range(total_graph['N_nodes']):\n",
    "        nb_count = len(total_graph['edges'][i+1]) \n",
    "        n_neighbors[i+1] = nb_count\n",
    "        if nb_count>1:\n",
    "            high_degree_nodes.append(i+1)\n",
    "\n",
    "    while counter<n_test_samples:\n",
    "        node1 = random.choice(high_degree_nodes)\n",
    "        if n_neighbors[node1]>1:\n",
    "            node2 = random.choice(training_graph_unbalanced[node1])\n",
    "            if n_neighbors[node2]>1:\n",
    "                # Add to test data\n",
    "                LP_test_X[counter] = (node1, node2)\n",
    "                LP_test_Y[counter] = 1\n",
    "                test_graph_unbalanced[node1].append(node2)\n",
    "\n",
    "                # remove edge from training graph\n",
    "                training_graph_unbalanced[node1].remove(node2)\n",
    "\n",
    "                # add/remove reverse edge in case of undirected graphs                 \n",
    "                if not directed:\n",
    "                    test_graph_unbalanced[node2].append(node1)\n",
    "                    training_graph_unbalanced[node2].remove(node1)\n",
    "                    n_neighbors[node2] -= 1\n",
    "    \n",
    "                # decrease neighbor count\n",
    "                n_neighbors[node1] -= 1\n",
    "                counter += 1          \n",
    "                if counter%int(n_test_samples/5)==0:\n",
    "                    print(counter/n_test_samples)\n",
    "\n",
    "    return LP_test_X, LP_test_Y, training_graph_unbalanced, test_graph_unbalanced\n",
    "\n",
    "\n",
    "def balance_test_graph(LP_test_X, LP_test_Y, test_graph_unbalanced, directed=False, reverse_fraction=0.5):\n",
    "    print(\"balancing test graph\")\n",
    "    counter = 0\n",
    "    n_test_samples = int(total_graph['N_edges']*0.5)\n",
    "    # in case of directed graphs, a fraction of the negative edges are added by reversing true edges\n",
    "    if directed:\n",
    "        true_edges = LP_test_X[0:n_test_samples]\n",
    "        while counter<int(n_test_samples*reverse_fraction):\n",
    "            true_edge = random.choice(true_edges)\n",
    "            src = true_edge[0]\n",
    "            target = true_edge[1]\n",
    "            if not src in test_graph_unbalanced.get(target):\n",
    "                LP_test_X[n_test_samples+counter] = (target, src)\n",
    "                counter += 1\n",
    "            \n",
    "            if counter%int(n_test_samples/5)==0:\n",
    "                print(counter/n_test_samples)\n",
    "\n",
    "    while counter<n_test_samples:\n",
    "        node1, node2 = random.sample(total_graph['nodes'], 2)\n",
    "        if not node1 in test_graph_unbalanced[node2]:\n",
    "            try:\n",
    "                LP_test_X[n_test_samples+counter] = (node1, node2)\n",
    "                LP_test_Y[n_test_samples+counter] = 0\n",
    "            except:\n",
    "                LP_test_X.append((node1, node2))\n",
    "                LP_test_Y.append(0)\n",
    "                print(\"appended edge\")\n",
    "            counter += 1\n",
    "    \n",
    "        if counter%int(n_test_samples/5)==0:\n",
    "            print(counter/n_test_samples)\n",
    "    return LP_test_X, LP_test_Y\n",
    "\n",
    "# When created the test set, we add remaining edges to the training set\n",
    "# and add negative edges to balance the training data\n",
    "def balance_training_graph(training_graph_unbalanced, total_graph, directed=False):\n",
    "    print(\"balancing training graph\")\n",
    "    n_test_samples = int(total_graph['N_edges']*0.5)\n",
    "    LP_train_X = []\n",
    "    LP_train_Y = []\n",
    "    added_edges = {i+1:{} for i in range(total_graph['N_nodes'])}\n",
    "    for node, neighbors in training_graph_unbalanced.items():\n",
    "        for nb in neighbors:\n",
    "            if not added_edges[node].get(nb, False):\n",
    "                added_edges[node][nb] = True\n",
    "                if not directed:\n",
    "                    added_edges[nb][node] = True\n",
    "                LP_train_X.append((node, nb))\n",
    "                LP_train_Y.append(1)\n",
    "\n",
    "    n_negative_edges = 0\n",
    "    while n_negative_edges < n_test_samples:\n",
    "        node1, node2 = random.sample(total_graph['nodes'], 2)\n",
    "        if not node1 in training_graph_unbalanced[node2]:\n",
    "            LP_train_X.append((node1, node2))\n",
    "            LP_train_Y.append(0)\n",
    "            n_negative_edges += 1\n",
    "\n",
    "        if n_negative_edges%int(n_test_samples/5)==0:\n",
    "            print(n_negative_edges/n_test_samples)\n",
    "        \n",
    "    return LP_train_X, LP_train_Y\n",
    "\n",
    "\n",
    "LP_test_X_unb, LP_test_Y_unb, training_graph_unbalanced, test_graph_unbalanced = split_graphs(total_graph, directed=True)\n",
    "LP_test_X, LP_test_Y = balance_test_graph(LP_test_X_unb, LP_test_Y_unb, test_graph_unbalanced, directed=True, reverse_fraction=1)\n",
    "LP_train_X, LP_train_Y = balance_training_graph(training_graph_unbalanced, total_graph, directed=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "       \n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "\n",
    "def compute_pi(total_graph, p, q):\n",
    "    pi_dict = {i+1:[] for i in range(total_graph['N_nodes'])}\n",
    "    for i in range(total_graph['N_nodes']):\n",
    "        neighbors = total_graph['edges'].get(i+1, [])\n",
    "        n_neighbors = len(neighbors)\n",
    "        probability_dist = np.ones((n_neighbors+2))\n",
    "        probability_dist[0:n_neighbors] *= 1/q\n",
    "        #probability_dist[n_neighbors] = 1     # staying at current node\n",
    "        probability_dist[n_neighbors+1] = 1/p   # returning to the same node we came from\n",
    "        norm = 1/p + 1 + n_neighbors/q\n",
    "        p_normed = probability_dist/norm\n",
    "        pi_dict[i+1] = p_normed\n",
    "    return pi_dict \n",
    "\n",
    "\n",
    "def alias_sample(prev, current, neighbors, pi_dict):\n",
    "    n_neighbors = len(neighbors)\n",
    "    p_normed = pi_dict[current]\n",
    "    sampled_indx = np.random.choice(n_neighbors+2,  p=p_normed)\n",
    "    if sampled_indx==n_neighbors:\n",
    "        return current\n",
    "    elif sampled_indx==n_neighbors+1:\n",
    "        return prev\n",
    "    else:\n",
    "        return neighbors[sampled_indx]\n",
    "\n",
    "    \n",
    "\n",
    "def learn_features(G, dim, walks_per_node, walk_length, context_size, p, q, SGD_epochs):\n",
    "    pi = compute_pi(G, p, q)\n",
    "    walks = [[]]*walks_per_node*G['N_nodes']\n",
    "    c = 0\n",
    "    for i in range(walks_per_node):\n",
    "        print(i)\n",
    "        for node in G[\"nodes\"]:\n",
    "            walk = node2vec_walk(G, node, walk_length, p, q, pi)\n",
    "            walks[c] = walk\n",
    "            c += 1\n",
    "            #if node%int(G[\"N_nodes\"]/10)==0:\n",
    "            #    print(node/G['N_nodes'])\n",
    " \n",
    "    f = SDG(walks, context_size, dim, SGD_epochs)\n",
    "    return f\n",
    "\n",
    "\n",
    "def node2vec_walk(G, start_node, walk_length, p, q, pi):\n",
    "    walk = [0]*(walk_length+1)\n",
    "    walk[0] = start_node\n",
    "    for i in range(walk_length):\n",
    "        curr = walk[i]\n",
    "        if i==0:\n",
    "            prev = start_node\n",
    "        else:\n",
    "            prev = walk[i-1]\n",
    "\n",
    "        neighbors = G['edges'][curr]\n",
    "        sample = alias_sample(prev, curr, neighbors, pi)\n",
    "        walk[i+1] = sample\n",
    "    return walk\n",
    "    \n",
    "\n",
    "def SDG(walks, context_size=10, dim=128, n_epochs=5):\n",
    "    \"\"\"Use Word2Vec with SGD to learn embedding based on walks\"\"\"\n",
    "    #sg=1 tells it to use skip-gram algorithm, min_count=0 tells it to not skip \"word\" that occur only 1 time   \n",
    "    model = Word2Vec(sentences=walks, vector_size=dim, window=context_size, min_count=0, sg=1, workers=8, epochs=n_epochs, compute_loss=True, callbacks=[EpochLogger()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters taken from original node2vec paper:\n",
    "dim = 64    # should be 128\n",
    "walks_per_node = 10  #should be 10\n",
    "walk_length = 80    # should be 80\n",
    "context_size = 10\n",
    "# From Khosla et al. these piwere the best performing settings in most cases:\n",
    "p = 0.25\n",
    "q = 4\n",
    "SGD_epochs = 1\n",
    "\n",
    "USE_PRETRAINED = True\n",
    "if USE_PRETRAINED:\n",
    "    embedding_model = Word2Vec.load(\"../Results/Node2Vec/{}.model\".format(dataset_name))\n",
    "else:\n",
    "    embedding_model = learn_features(total_graph, dim, walks_per_node, walk_length, context_size, p, q, SGD_epochs)\n",
    "\n",
    "    embedding_model.save(\"../Results/Node2Vec/{}.model\".format(dataset_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for evaluation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_and_recall(Y_true, Y_pred):\n",
    "    # count true positives and false positives and false negatives\n",
    "    nclasses = len(Y_true[0])\n",
    "    TP_list = [0]*nclasses\n",
    "    FP_list = [0]*nclasses\n",
    "    FN_list = [0]*nclasses\n",
    "    for j in range(nclasses):\n",
    "       for i, pred in enumerate(Y_pred):\n",
    "            if pred[j]==1 and Y_true[i][j]==1:\n",
    "                TP_list[j] += 1\n",
    "            elif pred[j]==1 and  Y_true[i][j]==0:\n",
    "                FP_list[j] += 1\n",
    "            elif pred[j]==0 and Y_true[i][j]==1:\n",
    "                FN_list[j] += 1 \n",
    "\n",
    "    return TP_list, FP_list, FN_list\n",
    "\n",
    "def compute_f1_macro(Y_true, Y_pred):\n",
    "    nclasses = len(Y_true[0])\n",
    "    TP_list, FP_list, FN_list = precision_and_recall(Y_true, Y_pred)\n",
    "    f1_scores = [0]*nclasses\n",
    "    for k in range(nclasses):\n",
    "        if TP_list[k]==0:\n",
    "            continue\n",
    "        f1_scores[k] = TP_list[k]/(TP_list[k]+0.5*(FP_list[k]+FN_list[k])) \n",
    "    return np.sum(f1_scores)/nclasses\n",
    "\n",
    "\n",
    "def compute_f1_micro(Y_true, Y_pred):\n",
    "    TP_list, FP_list, FN_list = precision_and_recall(Y_true, Y_pred)\n",
    "    TP = np.sum(TP_list)\n",
    "    FP = np.sum(FP_list)\n",
    "    FN = np.sum(FN_list)\n",
    "    return TP/(TP + 0.5*(FN+FP))\n",
    "\n",
    "\n",
    "def compute_accuracy(Y_true, Y_pred):\n",
    "    n_correct = 0\n",
    "    n_tot = 0\n",
    "    nclasses = len(Y_true[0])\n",
    "    for i, pred in enumerate(Y_pred):\n",
    "        for j in range(nclasses):\n",
    "            n_tot += 1\n",
    "            if pred[j]==Y_true[i][j]:\n",
    "                n_correct += 1\n",
    "    return n_correct/n_tot\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def get_edge_representation(fu,fv):\n",
    "    return sigmoid(np.dot(fu,fv))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate NC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.3990627363600241\n",
      "0.3185315552504907\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "mb = MultiLabelBinarizer(classes=[i+1 for i in range(total_graph['N_classes'])])\n",
    "\n",
    "f1_macro_list = []\n",
    "f1_micro_list = []\n",
    "accuracy_scores_list = []\n",
    "# 5-fold cross validation\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    training_nodes = NC_5folds[i]['train']\n",
    "    test_nodes = NC_5folds[i]['test']\n",
    "    X_train = np.array([embedding_model.wv[node] for node in training_nodes], dtype=object)\n",
    "    X_test = np.array([embedding_model.wv[node] for node in test_nodes], dtype=object)\n",
    "    Y_train_sequence = np.array([total_graph['groups'][node]  for node in training_nodes], dtype=object)\n",
    "    Y_test_sequence = np.array([total_graph['groups'][node] for node in test_nodes], dtype=object)\n",
    "    Y_train = mb.fit_transform(Y_train_sequence)\n",
    "    Y_test = mb.fit_transform(Y_test_sequence)\n",
    "    log_reg = MultiOutputClassifier(LogisticRegression(multi_class=\"ovr\"))\n",
    "    log_reg.fit(X_train, Y_train)\n",
    "    Y_pred = log_reg.predict(X_test)\n",
    "    acc = compute_accuracy(Y_test, Y_pred)\n",
    "    f1_macro = compute_f1_macro(Y_test, Y_pred)\n",
    "    f1_micro = compute_f1_micro(Y_test, Y_pred)\n",
    "    accuracy_scores_list.append(acc)\n",
    "    f1_macro_list.append(f1_macro)\n",
    "    f1_micro_list.append(f1_micro)\n",
    "    \n",
    "print(np.mean(f1_micro_list))\n",
    "print(np.mean(f1_macro_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5123165029711249\n"
     ]
    }
   ],
   "source": [
    "Y_train = LP_train_Y\n",
    "Y_test = LP_test_Y\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# build representation of edge datasets using inner product of the representation of the two nodes\n",
    "X_train = np.zeros((len(LP_train_X), 1))\n",
    "for i, edge in enumerate(LP_train_X):\n",
    "    u = edge[0]\n",
    "    v = edge[1]\n",
    "    X_train[i] = get_edge_representation(embedding_model.wv[u], embedding_model.wv[v])\n",
    "X_test = np.zeros((len(LP_test_X), 1))\n",
    "for i, edge in enumerate(LP_test_X):\n",
    "    u = edge[0]\n",
    "    v = edge[1]\n",
    "    X_test[i] = get_edge_representation(embedding_model.wv[u], embedding_model.wv[v])\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, Y_train)\n",
    "Y_probs = classifier.predict_proba(X_test)[:,1]\n",
    "roc_auc = roc_auc_score(Y_test, Y_probs)\n",
    "print(roc_auc)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(\"../Results/node2vec/{}_metrics1.csv\".format(dataset_name), \"w\") as file:\n",
    "    settings_str = \"Results for Node2vec embedding generated with p={}, q={}, walk length={}, walks per node={}, sgd_epochs={}\\n\".format(p,q,\n",
    "    walk_length, walks_per_node, SGD_epochs)\n",
    "    file.write(settings_str)\n",
    "    #header = \"Dataset; F1 macro; F1 micro; ROC-AUC \\n\"\n",
    "    header = \"Dataset; F1 macro; F1 micro; ROC-AUC_0 \\n\"\n",
    "    file.write(header)\n",
    "    data_row = \"{dataset};{f1mac};{f1mic};{roc}\".format(dataset=dataset_name, f1mac=np.mean(f1_macro_list), f1mic=np.mean(f1_micro_list), roc=roc_auc)\n",
    "    file.write(data_row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2f5f294937e1f47dd6e010afb2ca0c96836afcb29d9a31a278c78890f03e991"
  },
  "kernelspec": {
   "display_name": "Python 3.7.16 64-bit ('venv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
