{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from sklearn.model_selection import KFold\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.datasets import Reddit\n",
    "\n",
    "# node2vec paper\n",
    "#https://arxiv.org/pdf/1607.00653.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.09999992246004301\n",
      "0.19999984492008602\n",
      "0.29999976738012907\n",
      "0.39999968984017203\n",
      "0.49999961230021506\n",
      "0.5999995347602581\n",
      "0.6999994572203011\n",
      "0.7999993796803441\n",
      "0.8999993021403871\n",
      "0.9999992246004301\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from load_data import *\n",
    "dataset_name = \"Reddit\"\n",
    "data_dir = \"../Data/\" + dataset_name\n",
    "\n",
    "\n",
    "total_graph = load_reddit(data_dir)\n",
    "#total_graph = load_youtube(data_dir)\n",
    "#total_graph = load_flickr(data_dir)\n",
    "#total_graph = load_blogcatalog(data_dir)\n",
    "#total_graph = load_cora(data_dir)\n",
    "#total_graph = load_pubmed(data_dir)\n",
    "print(total_graph['Multioutput'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create 5-fold validation set for NC\n",
    "\n",
    "NC_5folds = {}\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "nodes = np.array([i+1 for i in range(total_graph['N_nodes'])])\n",
    "for i, (train_index, test_index) in enumerate(kf.split(nodes)):  \n",
    "    NC_5folds[i] = {\"train\":nodes[train_index], \"test\":nodes[test_index]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training and testing graphs for LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting graphs\n",
      "1\n",
      "0.09999984492007266\n",
      "0.19999968984014532\n",
      "0.299999534760218\n",
      "0.39999937968029065\n",
      "0.49999922460036333\n",
      "0.599999069520436\n",
      "0.6999989144405087\n",
      "0.7999987593605813\n",
      "0.899998604280654\n",
      "0.9999984492007267\n",
      "balancing test graph\n",
      "0.1999998621511757\n",
      "0.3999997243023514\n",
      "0.5999995864535271\n",
      "0.7999994486047028\n",
      "0.9999993107558786\n",
      "balancing training graph\n",
      "0.09999984492007266\n",
      "0.19999968984014532\n",
      "0.299999534760218\n",
      "0.39999937968029065\n",
      "0.49999922460036333\n",
      "0.599999069520436\n",
      "0.6999989144405087\n",
      "0.7999987593605813\n",
      "0.899998604280654\n",
      "0.9999984492007267\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select 50% of the edges for training, leave remaining for testing.\n",
    "# Want the remaining graph to still be connected, so we only remove edges if there are several neighbors\n",
    "\n",
    "def split_graphs(total_graph, directed=False):\n",
    "    print(\"splitting graphs\")\n",
    "    n_test_samples = int(total_graph['N_edges']*0.5)\n",
    "    training_graph_unbalanced = deepcopy(total_graph[\"edges\"])\n",
    "    test_graph_unbalanced = {i+1:[] for i in range(total_graph['N_nodes'])}\n",
    "    LP_test_X = [(1,1)]*n_test_samples*2\n",
    "    LP_test_Y = [0]*n_test_samples*2\n",
    "    counter = 0\n",
    "\n",
    "    high_degree_nodes = []\n",
    "    n_neighbors = {i+1:0 for i in range(total_graph['N_nodes'])}\n",
    "    for i in range(total_graph['N_nodes']):\n",
    "        nb_count = len(total_graph['edges'][i+1]) \n",
    "        n_neighbors[i+1] = nb_count\n",
    "        #if nb_count>1:\n",
    "        #    high_degree_nodes.append(i+1)\n",
    "    #print(high_degree_nodes[0:10])\n",
    "\n",
    "    while counter<n_test_samples:\n",
    "        node1 = random.choice(total_graph['nodes'])\n",
    "        if n_neighbors[node1]>1:\n",
    "            node2 = random.choice(training_graph_unbalanced[node1])\n",
    "            if n_neighbors[node2]>1:\n",
    "                # Add to test data\n",
    "                LP_test_X[counter] = (node1, node2)\n",
    "                LP_test_Y[counter] = 1\n",
    "                test_graph_unbalanced[node1].append(node2)\n",
    "\n",
    "                # remove edge from training graph\n",
    "                training_graph_unbalanced[node1].remove(node2)\n",
    "\n",
    "                # add/remove reverse edge in case of undirected graphs                 \n",
    "                if not directed:\n",
    "                    test_graph_unbalanced[node2].append(node1)\n",
    "                    training_graph_unbalanced[node2].remove(node1)\n",
    "                    n_neighbors[node2] -= 1\n",
    "    \n",
    "                # decrease neighbor count\n",
    "                n_neighbors[node1] -= 1\n",
    "                counter += 1          \n",
    "                if counter%int(n_test_samples/10)==0:\n",
    "                    print(counter/n_test_samples)\n",
    "\n",
    "    return LP_test_X, LP_test_Y, training_graph_unbalanced, test_graph_unbalanced\n",
    "\n",
    "\n",
    "def balance_test_graph(LP_test_X, LP_test_Y, test_graph_unbalanced, directed=False, reverse_fraction=0.5):\n",
    "    print(\"balancing test graph\")\n",
    "    counter = 0\n",
    "    n_test_samples = int(total_graph['N_edges']*0.5)\n",
    "    # in case of directed graphs, a fraction of the negative edges are added by reversing true edges\n",
    "    if directed:\n",
    "        true_edges = LP_test_X[0:n_test_samples]\n",
    "        while counter<int(n_test_samples*reverse_fraction):\n",
    "            true_edge = random.choice(true_edges)\n",
    "            src = true_edge[0]\n",
    "            target = true_edge[1]\n",
    "            if not src in test_graph_unbalanced.get(target):\n",
    "                LP_test_X[n_test_samples+counter] = (target, src)\n",
    "                counter += 1\n",
    "            \n",
    "            if counter%int(n_test_samples/10)==0:\n",
    "                print(counter/n_test_samples)\n",
    "\n",
    "    while counter<n_test_samples:\n",
    "        node1, node2 = random.sample(total_graph['nodes'], 2)\n",
    "        if not node1 in test_graph_unbalanced[node2]:\n",
    "            try:\n",
    "                LP_test_X[n_test_samples+counter] = (node1, node2)\n",
    "                LP_test_Y[n_test_samples+counter] = 0\n",
    "            except:\n",
    "                LP_test_X.append((node1, node2))\n",
    "                LP_test_Y.append(0)\n",
    "                print(\"appended edge\")\n",
    "            counter += 1\n",
    "    \n",
    "        if counter%int(n_test_samples/5)==0:\n",
    "            print(counter/n_test_samples)\n",
    "    return LP_test_X, LP_test_Y\n",
    "\n",
    "# When created the test set, we add remaining edges to the training set\n",
    "# and add negative edges to balance the training data\n",
    "def balance_training_graph(training_graph_unbalanced, total_graph, directed=False):\n",
    "    print(\"balancing training graph\")\n",
    "    n_test_samples = int(total_graph['N_edges']*0.5)\n",
    "    LP_train_X = []\n",
    "    LP_train_Y = []\n",
    "    added_edges = {i+1:{} for i in range(total_graph['N_nodes'])}\n",
    "    for node, neighbors in training_graph_unbalanced.items():\n",
    "        for nb in neighbors:\n",
    "            if not added_edges[node].get(nb, False):\n",
    "                added_edges[node][nb] = True\n",
    "                if not directed:\n",
    "                    added_edges[nb][node] = True\n",
    "                LP_train_X.append((node, nb))\n",
    "                LP_train_Y.append(1)\n",
    "\n",
    "    n_negative_edges = 0\n",
    "    while n_negative_edges < n_test_samples:\n",
    "        node1, node2 = random.sample(total_graph['nodes'], 2)\n",
    "        if not node1 in training_graph_unbalanced[node2]:\n",
    "            LP_train_X.append((node1, node2))\n",
    "            LP_train_Y.append(0)\n",
    "            n_negative_edges += 1\n",
    "\n",
    "        if n_negative_edges%int(n_test_samples/10)==0:\n",
    "            print(n_negative_edges/n_test_samples)\n",
    "        \n",
    "    return LP_train_X, LP_train_Y\n",
    "\n",
    "\n",
    "reverse_fraction = 0\n",
    "LP_test_X_unb, LP_test_Y_unb, training_graph_unbalanced, test_graph_unbalanced = split_graphs(total_graph, directed=True)\n",
    "LP_test_X, LP_test_Y = balance_test_graph(LP_test_X_unb, LP_test_Y_unb, test_graph_unbalanced, directed=True, reverse_fraction=reverse_fraction)\n",
    "LP_train_X, LP_train_Y = balance_training_graph(training_graph_unbalanced, total_graph, directed=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "       \n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "\n",
    "def compute_pi(total_graph, p, q):\n",
    "    pi_dict = {i+1:[] for i in range(total_graph['N_nodes'])}\n",
    "    for i in range(total_graph['N_nodes']):\n",
    "        neighbors = total_graph['edges'].get(i+1, [])\n",
    "        n_neighbors = len(neighbors)\n",
    "        probability_dist = np.ones((n_neighbors+2))\n",
    "        probability_dist[0:n_neighbors] *= 1/q\n",
    "        #probability_dist[n_neighbors] = 1     # staying at current node\n",
    "        probability_dist[n_neighbors+1] = 1/p   # returning to the same node we came from\n",
    "        norm = 1/p + 1 + n_neighbors/q\n",
    "        p_normed = probability_dist/norm\n",
    "        pi_dict[i+1] = p_normed\n",
    "    return pi_dict \n",
    "\n",
    "\n",
    "def alias_sample(prev, current, neighbors, pi_dict):\n",
    "    n_neighbors = len(neighbors)\n",
    "    p_normed = pi_dict[current]\n",
    "    sampled_indx = np.random.choice(n_neighbors+2,  p=p_normed)\n",
    "    if sampled_indx==n_neighbors:\n",
    "        return current\n",
    "    elif sampled_indx==n_neighbors+1:\n",
    "        return prev\n",
    "    else:\n",
    "        return neighbors[sampled_indx]\n",
    "\n",
    "    \n",
    "\n",
    "def learn_features(G, dim, walks_per_node, walk_length, context_size, p, q, SGD_epochs):\n",
    "    pi = compute_pi(G, p, q)\n",
    "    walks = [[]]*walks_per_node*G['N_nodes']\n",
    "    c = 0\n",
    "    for i in range(walks_per_node):\n",
    "        print(i)\n",
    "        for node in G[\"nodes\"]:\n",
    "            walk = node2vec_walk(G, node, walk_length, p, q, pi)\n",
    "            walks[c] = walk\n",
    "            c += 1\n",
    "            if node%int(G[\"N_nodes\"]/10)==0:\n",
    "                print(node/G['N_nodes'])\n",
    " \n",
    "    f = SDG(walks, context_size, dim, SGD_epochs)\n",
    "    return f\n",
    "\n",
    "\n",
    "def node2vec_walk(G, start_node, walk_length, p, q, pi):\n",
    "    walk = [0]*(walk_length+1)\n",
    "    walk[0] = start_node\n",
    "    for i in range(walk_length):\n",
    "        curr = walk[i]\n",
    "        if i==0:\n",
    "            prev = start_node\n",
    "        else:\n",
    "            prev = walk[i-1]\n",
    "\n",
    "        neighbors = G['edges'][curr]\n",
    "        sample = alias_sample(prev, curr, neighbors, pi)\n",
    "        walk[i+1] = sample\n",
    "    return walk\n",
    "    \n",
    "\n",
    "def SDG(walks, context_size=10, dim=128, n_epochs=5):\n",
    "    \"\"\"Use Word2Vec with SGD to learn embedding based on walks\"\"\"\n",
    "    #sg=1 tells it to use skip-gram algorithm, min_count=0 tells it to not skip \"word\" that occur only 1 time   \n",
    "    model = Word2Vec(sentences=walks, vector_size=dim, window=context_size, min_count=0, sg=1, workers=8, epochs=n_epochs, compute_loss=True, callbacks=[EpochLogger()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.09999785375485588\n",
      "0.19999570750971177\n",
      "0.29999356126456767\n",
      "0.39999141501942354\n",
      "0.4999892687742794\n",
      "0.5999871225291353\n",
      "0.6999849762839911\n",
      "0.7999828300388471\n",
      "0.8999806837937029\n",
      "0.9999785375485588\n",
      "1\n",
      "0.09999785375485588\n",
      "0.19999570750971177\n",
      "0.29999356126456767\n",
      "0.39999141501942354\n",
      "0.4999892687742794\n",
      "0.5999871225291353\n",
      "0.6999849762839911\n",
      "0.7999828300388471\n",
      "0.8999806837937029\n",
      "0.9999785375485588\n",
      "2\n",
      "0.09999785375485588\n",
      "0.19999570750971177\n",
      "0.29999356126456767\n",
      "0.39999141501942354\n",
      "0.4999892687742794\n",
      "0.5999871225291353\n",
      "0.6999849762839911\n",
      "0.7999828300388471\n",
      "0.8999806837937029\n",
      "0.9999785375485588\n",
      "3\n",
      "0.09999785375485588\n",
      "0.19999570750971177\n",
      "0.29999356126456767\n",
      "0.39999141501942354\n",
      "0.4999892687742794\n",
      "0.5999871225291353\n",
      "0.6999849762839911\n",
      "0.7999828300388471\n",
      "0.8999806837937029\n",
      "0.9999785375485588\n",
      "4\n",
      "0.09999785375485588\n",
      "0.19999570750971177\n",
      "0.29999356126456767\n",
      "0.39999141501942354\n",
      "0.4999892687742794\n",
      "0.5999871225291353\n",
      "0.6999849762839911\n",
      "0.7999828300388471\n",
      "0.8999806837937029\n",
      "0.9999785375485588\n",
      "Epoch #0 start\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parameters taken from original node2vec paper:\n",
    "dim = 128    # should be 128\n",
    "walks_per_node = 5  #should be 10\n",
    "walk_length = 40    # should be 80\n",
    "context_size = 10\n",
    "# From Khosla et al. these piwere the best performing settings in most cases:\n",
    "p = 0.25\n",
    "q = 4\n",
    "SGD_epochs = 1\n",
    "\n",
    "USE_PRETRAINED = False\n",
    "if USE_PRETRAINED:\n",
    "    embedding_model = Word2Vec.load(\"../Results/node2vec/{}.model\".format(dataset_name))\n",
    "   #embedding_model = Word2Vec.load(\"../Results/node2vec/blogcatalog.model\")\n",
    "else:\n",
    "    embedding_model = learn_features(total_graph, dim, walks_per_node, walk_length, context_size, p, q, SGD_epochs)\n",
    "\n",
    "    embedding_model.save(\"../Results/node2vec/{}_longer.model\".format(dataset_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for evaluation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_and_recall(Y_true, Y_pred, nclasses):\n",
    "    # count true positives and false positives and false negatives\n",
    "    #nclasses = len(Y_true[0])\n",
    "    TP_list = [0]*nclasses\n",
    "    FP_list = [0]*nclasses\n",
    "    FN_list = [0]*nclasses\n",
    "    for j in range(nclasses):\n",
    "       for i, pred in enumerate(Y_pred):\n",
    "            if pred[j]==1 and Y_true[i][j]==1:\n",
    "                TP_list[j] += 1\n",
    "            elif pred[j]==1 and  Y_true[i][j]==0:\n",
    "                FP_list[j] += 1\n",
    "            elif pred[j]==0 and Y_true[i][j]==1:\n",
    "                FN_list[j] += 1 \n",
    "\n",
    "    return TP_list, FP_list, FN_list\n",
    "\n",
    "def compute_f1_macro(Y_true, Y_pred, nclasses):\n",
    "    #nclasses = len(Y_true[0])\n",
    "    TP_list, FP_list, FN_list = precision_and_recall(Y_true, Y_pred, nclasses)\n",
    "    f1_scores = [0]*nclasses\n",
    "    for k in range(nclasses):\n",
    "        if TP_list[k]==0:\n",
    "            continue\n",
    "        f1_scores[k] = TP_list[k]/(TP_list[k]+0.5*(FP_list[k]+FN_list[k])) \n",
    "    return np.sum(f1_scores)/nclasses\n",
    "\n",
    "\n",
    "def compute_f1_micro(Y_true, Y_pred, nclasses):\n",
    "    TP_list, FP_list, FN_list = precision_and_recall(Y_true, Y_pred, nclasses)\n",
    "    TP = np.sum(TP_list)\n",
    "    FP = np.sum(FP_list)\n",
    "    FN = np.sum(FN_list)\n",
    "    return TP/(TP + 0.5*(FN+FP))\n",
    "\n",
    "\n",
    "def compute_accuracy(Y_true, Y_pred, nclasses):\n",
    "    n_correct = 0\n",
    "    n_tot = 0\n",
    "    #nclasses = len(Y_true[0])\n",
    "    for i, pred in enumerate(Y_pred):\n",
    "        for j in range(nclasses):\n",
    "            n_tot += 1\n",
    "            if pred[j]==Y_true[i][j]:\n",
    "                n_correct += 1\n",
    "    return n_correct/n_tot\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def get_edge_representation(fu,fv):\n",
    "    return sigmoid(np.dot(fu,fv))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate NC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0059193137467269475 0.12091945141974117\n",
      "1\n",
      "0.005506062599148393 0.11986779129912219\n",
      "2\n",
      "0.005631555105599174 0.11950292962462172\n",
      "3\n",
      "0.005710089912445558 0.12209988624900736\n",
      "4\n",
      "0.005572340452583576 0.12089798896829997\n",
      "0.1206576095121585\n",
      "0.00566787236330073\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from  sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "mb = MultiLabelBinarizer(classes=[i+1 for i in range(total_graph['N_classes'])])\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def onehot(y, nclasses):\n",
    "    Y = np.zeros((y.shape[0], nclasses), dtype=int)\n",
    "    for i in range(y.shape[0]):\n",
    "        c = y[i]\n",
    "        Y[i,c-1] =  1\n",
    "    return Y\n",
    "\n",
    "f1_macro_list = []\n",
    "f1_micro_list = []\n",
    "accuracy_scores_list = []\n",
    "# 5-fold cross validation\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    training_nodes = NC_5folds[i]['train']\n",
    "    test_nodes = NC_5folds[i]['test']\n",
    "    X_train = scaler.fit_transform(np.array([embedding_model.wv[node] for node in training_nodes], dtype=object))\n",
    "    X_test = scaler.fit_transform(np.array([embedding_model.wv[node] for node in test_nodes], dtype=object))\n",
    "    # For the datasets that only have one one label per node, it gives better results to not use multioutputclassifier\n",
    "    if not total_graph['Multioutput']:\n",
    "        Y_train_sequence = np.array([total_graph['groups'][node][0]  for node in training_nodes],dtype=int)\n",
    "        Y_test_sequence = np.array([total_graph['groups'][node][0] for node in test_nodes], dtype=int)\n",
    "        log_reg = LogisticRegression(multi_class=\"ovr\", max_iter=200)\n",
    "        Y_train = Y_train_sequence\n",
    "        Y_test = Y_test_sequence\n",
    "        log_reg.fit(X_train, Y_train)\n",
    "        Y_pred = log_reg.predict(X_test)\n",
    "        Y_pred = onehot(Y_pred, total_graph['N_classes'])\n",
    "        Y_test = onehot(Y_test, total_graph['N_classes'])\n",
    "    else:\n",
    "        Y_train_sequence = np.array([total_graph['groups'][node]  for node in training_nodes],dtype=int)\n",
    "        Y_test_sequence = np.array([total_graph['groups'][node] for node in test_nodes], dtype=int)\n",
    "        Y_train = mb.fit_transform(Y_train_sequence)\n",
    "        Y_test = mb.fit_transform(Y_test_sequence)\n",
    "        log_reg = MultiOutputClassifier(LogisticRegression(multi_class=\"ovr\"))\n",
    "        log_reg.fit(X_train, Y_train)\n",
    "        Y_pred = log_reg.predict(X_test)\n",
    "        \n",
    "    acc = compute_accuracy(Y_test, Y_pred, total_graph['N_classes'])\n",
    "    f1_macro = compute_f1_macro(Y_test, Y_pred, total_graph['N_classes'])\n",
    "    f1_micro = compute_f1_micro(Y_test, Y_pred,total_graph['N_classes'])\n",
    "    accuracy_scores_list.append(acc)\n",
    "    f1_macro_list.append(f1_macro)\n",
    "    f1_micro_list.append(f1_micro)\n",
    "    print(f1_macro, f1_micro)\n",
    "    \n",
    "print(np.mean(f1_micro_list))\n",
    "print(np.mean(f1_macro_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit model\n",
      "0.9382635986742481\n"
     ]
    }
   ],
   "source": [
    "Y_train = LP_train_Y\n",
    "Y_test = LP_test_Y\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# build representation of edge datasets using inner product of the representation of the two nodes\n",
    "X_train = np.zeros((len(LP_train_X), 1))\n",
    "for i, edge in enumerate(LP_train_X):\n",
    "    u = edge[0]\n",
    "    v = edge[1]\n",
    "    X_train[i] = get_edge_representation(embedding_model.wv[u], embedding_model.wv[v])\n",
    "X_test = np.zeros((len(LP_test_X), 1))\n",
    "for i, edge in enumerate(LP_test_X):\n",
    "    u = edge[0]\n",
    "    v = edge[1]\n",
    "    X_test[i] = get_edge_representation(embedding_model.wv[u], embedding_model.wv[v])\n",
    "    \n",
    "print(\"fit model\")\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, Y_train)\n",
    "Y_probs = classifier.predict_proba(X_test)[:,1]\n",
    "roc_auc = roc_auc_score(Y_test, Y_probs)\n",
    "print(roc_auc)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(\"../Results/node2vec/{}_metrics{}.csv\".format(dataset_name, reverse_fraction), \"w\") as file:\n",
    "    settings_str = \"Results for Node2vec embedding generated with p={}, q={}, walk length={}, walks per node={}, sgd_epochs={}\\n\".format(p,q,\n",
    "    walk_length, walks_per_node, SGD_epochs)\n",
    "    file.write(settings_str)\n",
    "    #header = \"Dataset; F1 macro; F1 micro; ROC-AUC \\n\"\n",
    "    header = \"Dataset; F1 macro; F1 micro; ROC-AUC_{} \\n\".format(reverse_fraction)\n",
    "    file.write(header)\n",
    "    data_row = \"{dataset};{f1mac};{f1mic};{roc}\".format(dataset=dataset_name, f1mac=np.mean(f1_macro_list), f1mic=np.mean(f1_micro_list), roc=None)\n",
    "    file.write(data_row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2f5f294937e1f47dd6e010afb2ca0c96836afcb29d9a31a278c78890f03e991"
  },
  "kernelspec": {
   "display_name": "Python 3.7.16 64-bit ('venv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
