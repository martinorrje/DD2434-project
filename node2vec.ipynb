{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from sklearn.model_selection import KFold\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "\n",
    "# node2vec paper\n",
    "#https://arxiv.org/pdf/1607.00653.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/venv/lib/python3.7/site-packages/torch_geometric/typing.py:18: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: dlopen(/opt/anaconda3/envs/venv/lib/python3.7/site-packages/libpyg.so, 0x0006): Library not loaded: '/usr/local/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/Python'\n",
      "  Referenced from: '/opt/anaconda3/envs/venv/lib/python3.7/site-packages/libpyg.so'\n",
      "  Reason: tried: '/usr/local/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/Python' (no such file), '/Library/Frameworks/Python.framework/Versions/3.10/Python' (no such file), '/System/Library/Frameworks/Python.framework/Versions/3.10/Python' (no such file)\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
      "/opt/anaconda3/envs/venv/lib/python3.7/site-packages/torch_geometric/typing.py:42: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: dlopen(/opt/anaconda3/envs/venv/lib/python3.7/site-packages/libpyg.so, 0x0006): Library not loaded: '/usr/local/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/Python'\n",
      "  Referenced from: '/opt/anaconda3/envs/venv/lib/python3.7/site-packages/libpyg.so'\n",
      "  Reason: tried: '/usr/local/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/Python' (no such file), '/Library/Frameworks/Python.framework/Versions/3.10/Python' (no such file), '/System/Library/Frameworks/Python.framework/Versions/3.10/Python' (no such file)\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nr of self-loop edges: 93\n",
      "False\n",
      "7600 26659\n"
     ]
    }
   ],
   "source": [
    "from load_data import *\n",
    "dataset_name = \"Actor\"\n",
    "data_dir = \"../Data/\" + dataset_name\n",
    "\n",
    "total_graph = load_geometric_dataset(dataset_name)\n",
    "#total_graph = load_reddit(data_dir)\n",
    "#total_graph = load_youtube(data_dir)\n",
    "#total_graph = load_flickr(data_dir)\n",
    "#total_graph = load_blogcatalog(data_dir)\n",
    "#total_graph = load_cora(data_dir)\n",
    "#total_graph = load_pubmed(data_dir)\n",
    "print(total_graph['Multioutput'])\n",
    "print(total_graph['N_nodes'], total_graph['N_edges'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "       \n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "\n",
    "def compute_pi(total_graph, p, q):\n",
    "    pi_dict = {i:[] for i in range(total_graph['N_nodes'])}\n",
    "    for i in range(total_graph['N_nodes']):\n",
    "        neighbors = total_graph['edges'].get(i, [])\n",
    "        n_neighbors = len(neighbors)\n",
    "        probability_dist = np.ones((n_neighbors+2))\n",
    "        probability_dist[0:n_neighbors] *= 1/q\n",
    "        #probability_dist[n_neighbors] = 1     # staying at current node\n",
    "        probability_dist[n_neighbors+1] = 1/p   # returning to the same node we came from\n",
    "        norm = 1/p + 1 + n_neighbors/q\n",
    "        p_normed = probability_dist/norm\n",
    "        pi_dict[i] = p_normed\n",
    "    return pi_dict \n",
    "\n",
    "\n",
    "def alias_sample(prev, current, neighbors, pi_dict):\n",
    "    n_neighbors = len(neighbors)\n",
    "    p_normed = pi_dict[current]\n",
    "    sampled_indx = np.random.choice(n_neighbors+2,  p=p_normed)\n",
    "    if sampled_indx==n_neighbors:\n",
    "        return current\n",
    "    elif sampled_indx==n_neighbors+1:\n",
    "        return prev\n",
    "    else:\n",
    "        return neighbors[sampled_indx]\n",
    "\n",
    "    \n",
    "\n",
    "def learn_features(G, dim, walks_per_node, walk_length, context_size, p, q, SGD_epochs):\n",
    "    pi = compute_pi(G, p, q)\n",
    "    walks = [[]]*walks_per_node*G['N_nodes']\n",
    "    c = 0\n",
    "    for i in range(walks_per_node):\n",
    "        print(i)\n",
    "        for node in G[\"nodes\"]:\n",
    "            walk = node2vec_walk(G, node, walk_length, pi)\n",
    "            walks[c] = walk\n",
    "            c += 1\n",
    "            if node%int(G[\"N_nodes\"]/10)==0:\n",
    "                print(node/G['N_nodes'])\n",
    " \n",
    "    f = SDG(walks, context_size, dim, SGD_epochs)\n",
    "    return f\n",
    "\n",
    "\n",
    "def node2vec_walk(G, start_node, walk_length, pi):\n",
    "    walk = [0]*(walk_length+1)\n",
    "    walk[0] = start_node\n",
    "    for i in range(walk_length):\n",
    "        curr = walk[i]\n",
    "        if i==0:\n",
    "            prev = start_node\n",
    "        else:\n",
    "            prev = walk[i-1]\n",
    "\n",
    "        neighbors = G['edges'][curr]\n",
    "        sample = alias_sample(prev, curr, neighbors, pi)\n",
    "        walk[i+1] = sample\n",
    "    return walk\n",
    "    \n",
    "\n",
    "def SDG(walks, context_size=10, dim=128, n_epochs=5):\n",
    "    \"\"\"Use Word2Vec with SGD to learn embedding based on walks\"\"\"\n",
    "    #sg=1 tells it to use skip-gram algorithm, min_count=0 tells it to not skip \"word\" that occur only 1 time   \n",
    "    model = Word2Vec(sentences=walks, vector_size=dim, window=context_size, min_count=0, sg=1, workers=8, epochs=n_epochs, compute_loss=True, callbacks=[EpochLogger()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters taken from original node2vec paper:\n",
    "dim = 128    # should be 128\n",
    "walks_per_node = 10  #should be 10\n",
    "walk_length = 80    # should be 80\n",
    "context_size = 10\n",
    "# From Khosla et al. these piwere the best performing settings in most cases:\n",
    "p = 0.25\n",
    "q = 4\n",
    "SGD_epochs = 1\n",
    "\n",
    "USE_PRETRAINED = True\n",
    "if USE_PRETRAINED:\n",
    "    embedding_model = Word2Vec.load(\"../Results/node2vec/{}.model\".format(dataset_name))\n",
    "   #embedding_model = Word2Vec.load(\"../Results/node2vec/blogcatalog.model\")\n",
    "else:\n",
    "    embedding_model = learn_features(total_graph, dim, walks_per_node, walk_length, context_size, p, q, SGD_epochs)\n",
    "    embedding_model.save(\"../Results/node2vec/{}.model\".format(dataset_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting graphs\n",
      "0.099932478055368\n",
      "0.199864956110736\n",
      "0.299797434166104\n",
      "0.399729912221472\n",
      "0.49966239027683995\n",
      "0.599594868332208\n",
      "0.699527346387576\n",
      "0.799459824442944\n",
      "0.8993923024983119\n",
      "0.9993247805536799\n",
      "balancing test graph\n",
      "0.19993998049366044\n",
      "0.39987996098732087\n",
      "0.5998199414809813\n",
      "0.7997599219746417\n",
      "0.9996999024683022\n",
      "balancing training graph\n",
      "0.099932478055368\n",
      "0.199864956110736\n",
      "0.299797434166104\n",
      "0.399729912221472\n",
      "0.49966239027683995\n",
      "0.599594868332208\n",
      "0.699527346387576\n",
      "0.799459824442944\n",
      "0.8993923024983119\n",
      "0.9993247805536799\n"
     ]
    }
   ],
   "source": [
    "## Create 5-fold validation set for NC\n",
    "\n",
    "import utils\n",
    "\n",
    "NC_5folds = {}\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "nodes = np.array([i for i in range(total_graph['N_nodes'])])\n",
    "for i, (train_index, test_index) in enumerate(kf.split(nodes)):  \n",
    "    NC_5folds[i] = {\"train\":nodes[train_index], \"test\":nodes[test_index]}\n",
    "\n",
    "\n",
    "reverse_fraction = 0\n",
    "LP_test_X_unb, LP_test_Y_unb, training_graph_unbalanced, test_graph_unbalanced = utils.split_graphs(total_graph, directed=True)\n",
    "LP_test_X, LP_test_Y = utils.balance_test_graph(total_graph, LP_test_X_unb, LP_test_Y_unb, test_graph_unbalanced, directed=True, reverse_fraction=reverse_fraction)\n",
    "LP_train_X, LP_train_Y = utils.balance_training_graph(training_graph_unbalanced, total_graph, directed=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate NC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.18332972717212054 0.23223684210526316\n",
      "1\n",
      "0.19528937541194544 0.25263157894736843\n",
      "2\n",
      "0.17675910029959113 0.22894736842105262\n",
      "3\n",
      "0.1708518712697923 0.23026315789473684\n",
      "4\n",
      "0.16841645376564962 0.22236842105263158\n",
      "0.23328947368421055\n",
      "0.1789293055838198\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from  sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "mb = MultiLabelBinarizer(classes=[i for i in range(total_graph['N_classes'])])\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "f1_macro_list = []\n",
    "f1_micro_list = []\n",
    "# 5-fold cross validation\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    training_nodes = NC_5folds[i]['train']\n",
    "    test_nodes = NC_5folds[i]['test']\n",
    "    X_train = scaler.fit_transform(np.array([embedding_model.wv[node] for node in training_nodes], dtype=object))\n",
    "    X_test = scaler.fit_transform(np.array([embedding_model.wv[node] for node in test_nodes], dtype=object))\n",
    "    # For the datasets that only have one one label per node, it gives better results to not use multioutputclassifier\n",
    "    if not total_graph['Multioutput']:\n",
    "        Y_train_sequence = np.array([total_graph['groups'][node][0]  for node in training_nodes],dtype=int)\n",
    "        Y_test_sequence = np.array([total_graph['groups'][node][0] for node in test_nodes], dtype=int)\n",
    "        log_reg = LogisticRegression(multi_class=\"ovr\", max_iter=200)\n",
    "        Y_train = Y_train_sequence\n",
    "        Y_test = Y_test_sequence\n",
    "        log_reg.fit(X_train, Y_train)\n",
    "        Y_pred = log_reg.predict(X_test)\n",
    "        Y_pred = utils.onehot(Y_pred, total_graph['N_classes'])\n",
    "        Y_test = utils.onehot(Y_test, total_graph['N_classes'])\n",
    "    else:\n",
    "      \n",
    "        Y_train_sequence = np.array([total_graph['groups'][node]  for node in training_nodes], dtype=object)\n",
    "        Y_test_sequence = np.array([total_graph['groups'][node] for node in test_nodes], dtype=object)\n",
    "        Y_train = mb.fit_transform(Y_train_sequence)\n",
    "        Y_test = mb.fit_transform(Y_test_sequence)\n",
    "        log_reg = MultiOutputClassifier(LogisticRegression(multi_class=\"ovr\"))\n",
    "        log_reg.fit(X_train, Y_train)\n",
    "        Y_pred = log_reg.predict(X_test)\n",
    "        \n",
    "    f1_macro = utils.compute_f1_macro(Y_test, Y_pred, total_graph['N_classes'])\n",
    "    f1_micro = utils.compute_f1_micro(Y_test, Y_pred,total_graph['N_classes'])\n",
    "    f1_macro_list.append(f1_macro)\n",
    "    f1_micro_list.append(f1_micro)\n",
    "    print(f1_macro, f1_micro)\n",
    "    \n",
    "print(np.mean(f1_micro_list))\n",
    "print(np.mean(f1_macro_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit model\n",
      "0.9601912175587157\n"
     ]
    }
   ],
   "source": [
    "Y_train = LP_train_Y\n",
    "Y_test = LP_test_Y\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# build representation of edge datasets using inner product of the representation of the two nodes\n",
    "X_train = np.zeros((len(LP_train_X), 1))\n",
    "for i, edge in enumerate(LP_train_X):\n",
    "    u = edge[0]\n",
    "    v = edge[1]\n",
    "    X_train[i] = utils.get_edge_representation(embedding_model.wv[u], embedding_model.wv[v])\n",
    "X_test = np.zeros((len(LP_test_X), 1))\n",
    "for i, edge in enumerate(LP_test_X):\n",
    "    u = edge[0]\n",
    "    v = edge[1]\n",
    "    X_test[i] = utils.get_edge_representation(embedding_model.wv[u], embedding_model.wv[v])\n",
    "    \n",
    "print(\"fit model\")\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, Y_train)\n",
    "Y_probs = classifier.predict_proba(X_test)[:,1]\n",
    "roc_auc = roc_auc_score(Y_test, Y_probs)\n",
    "print(roc_auc)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(\"../Results/node2vec/{}_metrics{}.csv\".format(dataset_name, reverse_fraction), \"w\") as file:\n",
    "    settings_str = \"Results for Node2vec embedding generated with p={}, q={}, walk length={}, walks per node={}, sgd_epochs={}\\n\".format(p,q,\n",
    "    walk_length, walks_per_node, SGD_epochs)\n",
    "    file.write(settings_str)\n",
    "    #header = \"Dataset; F1 macro; F1 micro; ROC-AUC \\n\"\n",
    "    header = \"Dataset; F1 macro; F1 micro; ROC-AUC_{} \\n\".format(reverse_fraction)\n",
    "    file.write(header)\n",
    "    data_row = \"{dataset};{f1mac};{f1mic};{roc}\".format(dataset=dataset_name, f1mac=np.mean(f1_macro_list), f1mic=np.mean(f1_micro_list), roc=roc_auc)\n",
    "    file.write(data_row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2f5f294937e1f47dd6e010afb2ca0c96836afcb29d9a31a278c78890f03e991"
  },
  "kernelspec": {
   "display_name": "Python 3.7.16 64-bit ('venv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
