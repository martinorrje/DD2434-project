{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from load_data import *\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nr of self-loop edges: 93\n",
      "7600 26659\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_name = \"Actor\"\n",
    "data_dir = \"../Data/\" + dataset_name\n",
    "\n",
    "\n",
    "#total_graph = load_cora(data_dir)\n",
    "from_torch_geometric = True\n",
    "total_graph = load_geometric_dataset(dataset_name)\n",
    "#total_graph = load_blogcatalog(data_dir)\n",
    "#total_graph = load_toy(data_dir)\n",
    "#total_graph = load_pubmed(data_dir)\n",
    "#total_graph = load_flickr(data_dir)\n",
    "#total_graph = load_reddit(data_dir)\n",
    "\n",
    "def get_node_features(total_graph, mode=\"degree\"):\n",
    "    if mode==\"degree\":\n",
    "        node_features = np.zeros((total_graph['N_nodes'],2))\n",
    "        for i in range(total_graph['N_nodes']):\n",
    "            neighbors = total_graph['edges'][i]\n",
    "            degree = len(neighbors)\n",
    "            node_features[i,0] = degree\n",
    "            second_neighbors_count = 0\n",
    "            for n in neighbors:\n",
    "                second_neighbors_count += len(total_graph['edges'][n])\n",
    "            node_features[i,1] = second_neighbors_count\n",
    "    elif mode==\"degree_dist\":\n",
    "        node_features = total_graph['adj_matrix']/(np.linalg.norm(total_graph['adj_matrix'], axis=1, keepdims=True)+1e-9)\n",
    "    elif mode==\"node_nr\":\n",
    "        node_features = np.zeros((total_graph['N_nodes'],total_graph['N_nodes']))\n",
    "        for i in range(total_graph['N_nodes']):\n",
    "            node_features[i,i] = 1\n",
    "    return node_features\n",
    "\n",
    "print(total_graph['N_nodes'], total_graph['N_edges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GraphSage(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, K):\n",
    "        super(GraphSage, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_dim, output_dim))\n",
    "        for i in range(K-1):\n",
    "            self.layers.append(nn.Linear(output_dim, output_dim))\n",
    "        for i in range(K):\n",
    "            nn.init.zeros_(self.layers[i].bias)\n",
    "        self.K = K\n",
    "    \n",
    "    def infer(self, node_features, neighborhood_dict):\n",
    "        eps = 1e-9\n",
    "        h = node_features\n",
    "        N_nodes = h.shape[0]\n",
    "        for k in range(self.K):\n",
    "            layer = self.layers[k]\n",
    "            h_updated = torch.zeros((N_nodes, layer.out_features))\n",
    "            for v in range(N_nodes):        \n",
    "                neighborhood = neighborhood_dict[v]\n",
    "                \n",
    "                hv = h[v].view(1, -1)\n",
    "                hN = h[neighborhood]\n",
    "                conc = torch.cat([hN, hv], dim=0)\n",
    "                aggregated = torch.mean(conc, dim=0, keepdim=True)\n",
    "                output = layer(aggregated)\n",
    "                hv = F.relu(output)\n",
    "                h_updated[v] =  hv    \n",
    "            h = h_updated\n",
    "            h = h / (torch.norm(h, dim=1, keepdim=True)+eps)\n",
    "        return h\n",
    "\n",
    "    \n",
    "    def aggregate_neighbors(self, batch):\n",
    "        batch_nodes = batch.src_index\n",
    "        subgraph_edge_indices = batch.edge_index\n",
    "\n",
    "        neighborhoods = {i.item():[] for i in batch_nodes}\n",
    "        for src, target in zip(subgraph_edge_indices[0], subgraph_edge_indices[1]):\n",
    "            src = src.item()\n",
    "            target = target.item()\n",
    "            if not neighborhoods.get(src):\n",
    "                neighborhoods[src] = []\n",
    "            neighborhoods[src].append(target)\n",
    "        B = [[] for k in range(self.K+1)]\n",
    "        B[-1] = batch_nodes[:].tolist()\n",
    "        for k in range(self.K, 0, -1):\n",
    "            B[k-1] = B[k][:]\n",
    "            for node in B[k]:  \n",
    "                B[k-1].extend(neighborhoods[node])\n",
    "        return B, neighborhoods\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        eps = 1e-9\n",
    "        B, neighborhood_dict = self.aggregate_neighbors(batch)\n",
    "        h = batch.x\n",
    "        N_nodes = h.shape[0]\n",
    "        for k in range(self.K):\n",
    "            layer = self.layers[k]\n",
    "            h_updated = torch.zeros((N_nodes, layer.out_features))\n",
    "            for i,v in enumerate(B[k+1]):      # this is because B[0] is base case, B[1] are nodes corresponding to layer 1 etc  \n",
    "                neighborhood = neighborhood_dict[v]\n",
    "                hv = h[v].view(1, -1)\n",
    "                hN = h[neighborhood]\n",
    "                conc = torch.cat([hN, hv], dim=0)\n",
    "                aggregated = torch.mean(conc, dim=0, keepdim=True)\n",
    "                output = layer(aggregated)\n",
    "                hv = F.relu(output)\n",
    "                h_updated[v] =  hv    \n",
    "            h = h_updated\n",
    "            h = h / (torch.norm(h, dim=1, keepdim=True)+eps)\n",
    "        return h\n",
    "\n",
    "\n",
    "\n",
    "def compute_loss(Z, Z_pos, Z_neg):\n",
    "    eps = 1e-9\n",
    "    dot = torch.sum(Z * Z_pos, dim=1)\n",
    "    term1 = -torch.log(torch.sigmoid(dot)+eps)\n",
    "    term2 = 0\n",
    "    for q in range(Z_neg.shape[0]):\n",
    "        term2 = -torch.log(torch.sigmoid(-torch.sum(Z * Z_neg[q,:,:], dim=1))+eps)\n",
    "    return torch.mean(term1+term2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7600 26659\n",
      "starting training\n",
      "0.0 tensor(1.3794, grad_fn=<MeanBackward0>)\n",
      "0.00847457627118644 tensor(1.3778, grad_fn=<MeanBackward0>)\n",
      "0.01694915254237288 tensor(1.3776, grad_fn=<MeanBackward0>)\n",
      "0.025423728813559324 tensor(1.3701, grad_fn=<MeanBackward0>)\n",
      "0.03389830508474576 tensor(1.3737, grad_fn=<MeanBackward0>)\n",
      "0.0423728813559322 tensor(1.3698, grad_fn=<MeanBackward0>)\n",
      "0.05084745762711865 tensor(1.3892, grad_fn=<MeanBackward0>)\n",
      "0.059322033898305086 tensor(1.3760, grad_fn=<MeanBackward0>)\n",
      "0.06779661016949153 tensor(1.3841, grad_fn=<MeanBackward0>)\n",
      "0.07627118644067797 tensor(1.3771, grad_fn=<MeanBackward0>)\n",
      "0.0847457627118644 tensor(1.3747, grad_fn=<MeanBackward0>)\n",
      "0.09322033898305085 tensor(1.3874, grad_fn=<MeanBackward0>)\n",
      "0.1016949152542373 tensor(1.3797, grad_fn=<MeanBackward0>)\n",
      "0.11016949152542373 tensor(1.3835, grad_fn=<MeanBackward0>)\n",
      "0.11864406779661017 tensor(1.3760, grad_fn=<MeanBackward0>)\n",
      "0.1271186440677966 tensor(1.3779, grad_fn=<MeanBackward0>)\n",
      "0.13559322033898305 tensor(1.3538, grad_fn=<MeanBackward0>)\n",
      "0.1440677966101695 tensor(1.3812, grad_fn=<MeanBackward0>)\n",
      "0.15254237288135594 tensor(1.4024, grad_fn=<MeanBackward0>)\n",
      "0.16101694915254236 tensor(1.3794, grad_fn=<MeanBackward0>)\n",
      "0.1694915254237288 tensor(1.3942, grad_fn=<MeanBackward0>)\n",
      "0.17796610169491525 tensor(1.3841, grad_fn=<MeanBackward0>)\n",
      "0.1864406779661017 tensor(1.3560, grad_fn=<MeanBackward0>)\n",
      "0.19491525423728814 tensor(1.3819, grad_fn=<MeanBackward0>)\n",
      "0.2033898305084746 tensor(1.3725, grad_fn=<MeanBackward0>)\n",
      "0.211864406779661 tensor(1.3736, grad_fn=<MeanBackward0>)\n",
      "0.22033898305084745 tensor(1.3728, grad_fn=<MeanBackward0>)\n",
      "0.2288135593220339 tensor(1.3836, grad_fn=<MeanBackward0>)\n",
      "0.23728813559322035 tensor(1.3902, grad_fn=<MeanBackward0>)\n",
      "0.2457627118644068 tensor(1.3746, grad_fn=<MeanBackward0>)\n",
      "0.2542372881355932 tensor(1.3769, grad_fn=<MeanBackward0>)\n",
      "0.2627118644067797 tensor(1.3693, grad_fn=<MeanBackward0>)\n",
      "0.2711864406779661 tensor(1.3788, grad_fn=<MeanBackward0>)\n",
      "0.2796610169491525 tensor(1.3707, grad_fn=<MeanBackward0>)\n",
      "0.288135593220339 tensor(1.3781, grad_fn=<MeanBackward0>)\n",
      "0.2966101694915254 tensor(1.3642, grad_fn=<MeanBackward0>)\n",
      "0.3050847457627119 tensor(1.3753, grad_fn=<MeanBackward0>)\n",
      "0.3135593220338983 tensor(1.3651, grad_fn=<MeanBackward0>)\n",
      "0.3220338983050847 tensor(1.3601, grad_fn=<MeanBackward0>)\n",
      "0.3305084745762712 tensor(1.3851, grad_fn=<MeanBackward0>)\n",
      "0.3389830508474576 tensor(1.3926, grad_fn=<MeanBackward0>)\n",
      "0.3474576271186441 tensor(1.3922, grad_fn=<MeanBackward0>)\n",
      "0.3559322033898305 tensor(1.3644, grad_fn=<MeanBackward0>)\n",
      "0.3644067796610169 tensor(1.3667, grad_fn=<MeanBackward0>)\n",
      "0.3728813559322034 tensor(1.3738, grad_fn=<MeanBackward0>)\n",
      "0.3813559322033898 tensor(1.3849, grad_fn=<MeanBackward0>)\n",
      "0.3898305084745763 tensor(1.3828, grad_fn=<MeanBackward0>)\n",
      "0.3983050847457627 tensor(1.3697, grad_fn=<MeanBackward0>)\n",
      "0.4067796610169492 tensor(1.3763, grad_fn=<MeanBackward0>)\n",
      "0.4152542372881356 tensor(1.4019, grad_fn=<MeanBackward0>)\n",
      "0.423728813559322 tensor(1.3876, grad_fn=<MeanBackward0>)\n",
      "0.4322033898305085 tensor(1.3929, grad_fn=<MeanBackward0>)\n",
      "0.4406779661016949 tensor(1.3855, grad_fn=<MeanBackward0>)\n",
      "0.4491525423728814 tensor(1.3893, grad_fn=<MeanBackward0>)\n",
      "0.4576271186440678 tensor(1.3556, grad_fn=<MeanBackward0>)\n",
      "0.4661016949152542 tensor(1.3699, grad_fn=<MeanBackward0>)\n",
      "0.4745762711864407 tensor(1.3737, grad_fn=<MeanBackward0>)\n",
      "0.4830508474576271 tensor(1.3799, grad_fn=<MeanBackward0>)\n",
      "0.4915254237288136 tensor(1.3856, grad_fn=<MeanBackward0>)\n",
      "0.5 tensor(1.3828, grad_fn=<MeanBackward0>)\n",
      "0.5084745762711864 tensor(1.3797, grad_fn=<MeanBackward0>)\n",
      "0.5169491525423728 tensor(1.3720, grad_fn=<MeanBackward0>)\n",
      "0.5254237288135594 tensor(1.3693, grad_fn=<MeanBackward0>)\n",
      "0.5338983050847458 tensor(1.3499, grad_fn=<MeanBackward0>)\n",
      "0.5423728813559322 tensor(1.3770, grad_fn=<MeanBackward0>)\n",
      "0.5508474576271186 tensor(1.3731, grad_fn=<MeanBackward0>)\n",
      "0.559322033898305 tensor(1.3764, grad_fn=<MeanBackward0>)\n",
      "0.5677966101694916 tensor(1.3776, grad_fn=<MeanBackward0>)\n",
      "0.576271186440678 tensor(1.3826, grad_fn=<MeanBackward0>)\n",
      "0.5847457627118644 tensor(1.3868, grad_fn=<MeanBackward0>)\n",
      "0.5932203389830508 tensor(1.3730, grad_fn=<MeanBackward0>)\n",
      "0.6016949152542372 tensor(1.3792, grad_fn=<MeanBackward0>)\n",
      "0.6101694915254238 tensor(1.3822, grad_fn=<MeanBackward0>)\n",
      "0.6186440677966102 tensor(1.3829, grad_fn=<MeanBackward0>)\n",
      "0.6271186440677966 tensor(1.3836, grad_fn=<MeanBackward0>)\n",
      "0.635593220338983 tensor(1.3770, grad_fn=<MeanBackward0>)\n",
      "0.6440677966101694 tensor(1.3556, grad_fn=<MeanBackward0>)\n",
      "0.652542372881356 tensor(1.3789, grad_fn=<MeanBackward0>)\n",
      "0.6610169491525424 tensor(1.3832, grad_fn=<MeanBackward0>)\n",
      "0.6694915254237288 tensor(1.3904, grad_fn=<MeanBackward0>)\n",
      "0.6779661016949152 tensor(1.3798, grad_fn=<MeanBackward0>)\n",
      "0.6864406779661016 tensor(1.3855, grad_fn=<MeanBackward0>)\n",
      "0.6949152542372882 tensor(1.3726, grad_fn=<MeanBackward0>)\n",
      "0.7033898305084746 tensor(1.3811, grad_fn=<MeanBackward0>)\n",
      "0.711864406779661 tensor(1.3696, grad_fn=<MeanBackward0>)\n",
      "0.7203389830508474 tensor(1.3865, grad_fn=<MeanBackward0>)\n",
      "0.7288135593220338 tensor(1.3775, grad_fn=<MeanBackward0>)\n",
      "0.7372881355932204 tensor(1.3777, grad_fn=<MeanBackward0>)\n",
      "0.7457627118644068 tensor(1.3894, grad_fn=<MeanBackward0>)\n",
      "0.7542372881355932 tensor(1.3977, grad_fn=<MeanBackward0>)\n",
      "0.7627118644067796 tensor(1.3670, grad_fn=<MeanBackward0>)\n",
      "0.7711864406779662 tensor(1.3612, grad_fn=<MeanBackward0>)\n",
      "0.7796610169491526 tensor(1.3893, grad_fn=<MeanBackward0>)\n",
      "0.788135593220339 tensor(1.3734, grad_fn=<MeanBackward0>)\n",
      "0.7966101694915254 tensor(1.3868, grad_fn=<MeanBackward0>)\n",
      "0.8050847457627118 tensor(1.3733, grad_fn=<MeanBackward0>)\n",
      "0.8135593220338984 tensor(1.3842, grad_fn=<MeanBackward0>)\n",
      "0.8220338983050848 tensor(1.3653, grad_fn=<MeanBackward0>)\n",
      "0.8305084745762712 tensor(1.3763, grad_fn=<MeanBackward0>)\n",
      "0.8389830508474576 tensor(1.3875, grad_fn=<MeanBackward0>)\n",
      "0.847457627118644 tensor(1.3902, grad_fn=<MeanBackward0>)\n",
      "0.8559322033898306 tensor(1.3832, grad_fn=<MeanBackward0>)\n",
      "0.864406779661017 tensor(1.3714, grad_fn=<MeanBackward0>)\n",
      "0.8728813559322034 tensor(1.3793, grad_fn=<MeanBackward0>)\n",
      "0.8813559322033898 tensor(1.3860, grad_fn=<MeanBackward0>)\n",
      "0.8898305084745762 tensor(1.3704, grad_fn=<MeanBackward0>)\n",
      "0.8983050847457628 tensor(1.3695, grad_fn=<MeanBackward0>)\n",
      "0.9067796610169492 tensor(1.3746, grad_fn=<MeanBackward0>)\n",
      "0.9152542372881356 tensor(1.3699, grad_fn=<MeanBackward0>)\n",
      "0.923728813559322 tensor(1.3742, grad_fn=<MeanBackward0>)\n",
      "0.9322033898305084 tensor(1.3823, grad_fn=<MeanBackward0>)\n",
      "0.940677966101695 tensor(1.3816, grad_fn=<MeanBackward0>)\n",
      "0.9491525423728814 tensor(1.3670, grad_fn=<MeanBackward0>)\n",
      "0.9576271186440678 tensor(1.3853, grad_fn=<MeanBackward0>)\n",
      "0.9661016949152542 tensor(1.3886, grad_fn=<MeanBackward0>)\n",
      "0.9745762711864406 tensor(1.3894, grad_fn=<MeanBackward0>)\n",
      "0.9830508474576272 tensor(1.3789, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import LinkNeighborLoader, NeighborLoader\n",
    "from torch_geometric.sampler import NegativeSampling\n",
    "\n",
    "# Define node features\n",
    "feature_type = \"node_nr\"\n",
    "node_features_np = get_node_features(total_graph, mode=feature_type) \n",
    "node_features_torch = torch.tensor(node_features_np).float()\n",
    "#node_features_torch = total_graph['node_feats'] # for datasets coming from torch_geometric, there exist precomputed features\n",
    "\n",
    "if from_torch_geometric:\n",
    "    node_links_torch = total_graph['edges_list']\n",
    "else:\n",
    "    node_links_torch = torch.tensor(total_graph['edges_list']).T\n",
    "\n",
    "print(total_graph['N_nodes'], total_graph['N_edges'])\n",
    "    \n",
    "graph_data = torch_geometric.data.Data(node_features_torch, node_links_torch)\n",
    "# parameters\n",
    "num_epochs = 1\n",
    "K = 1   # number of iterations\n",
    "Q = 10\n",
    "batch_size = 256\n",
    "input_size = node_features_torch.shape[1]\n",
    "output_dim = 128\n",
    "nb_size = 10\n",
    "# Create the model\n",
    "model = GraphSage(input_size, output_dim, K)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "ns = NegativeSampling(mode=\"triplet\", amount=Q)\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    loader = LinkNeighborLoader(graph_data, batch_size=batch_size, num_neighbors=[nb_size]*K, neg_sampling=ns, shuffle=True, replace=True)\n",
    "\n",
    "    print(\"starting training\")\n",
    "    for i,batch in enumerate(loader):\n",
    "        if batch.src_index.shape[0] != batch_size:\n",
    "            break \n",
    "        positive_samples = batch.dst_pos_index.tolist()\n",
    "        negative_samples = batch.dst_neg_index.numpy()\n",
    "        Z_tot = model(batch)\n",
    "        Z = Z_tot[batch.src_index.tolist()]\n",
    "        Z_pos = Z_tot[positive_samples]\n",
    "        Z_neg = torch.zeros((Q, batch_size, output_dim))\n",
    "        for q in range(Q):\n",
    "            Z_neg[q] = Z_tot[negative_samples[:,q]]\n",
    "       \n",
    "        loss = compute_loss(Z, Z_pos, Z_neg)\n",
    "        print(i/len(loader), loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate NC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.16256483118598392 0.25526315789473686\n",
      "1\n",
      "0.15975803489517645 0.2546052631578947\n",
      "2\n",
      "0.15669936390073452 0.2355263157894737\n",
      "3\n",
      "0.15770013944734246 0.25723684210526315\n",
      "4\n",
      "0.15112560169702616 0.2388157894736842\n",
      "0.24828947368421056\n",
      "0.15756959422525268\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from  sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# helper functions\n",
    "import utils\n",
    "\n",
    "# Create 5fold train/test data\n",
    "NC_5folds = {}\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "nodes = np.array([i for i in range(total_graph['N_nodes'])])\n",
    "#labels = np.array([total_graph['grops'][n] for n in nodes])\n",
    "for i, (train_index, test_index) in enumerate(kf.split(nodes)):  \n",
    "    NC_5folds[i] = {\"train\":list(nodes[train_index]), \"test\":list(nodes[test_index])}\n",
    "\n",
    "\n",
    "N_classes = total_graph['N_classes']\n",
    "mb = MultiLabelBinarizer(classes=[i for i in range(N_classes)])\n",
    "f1_macro_list = []\n",
    "f1_micro_list = []\n",
    "\n",
    "\n",
    "# 5-fold cross validation\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        print(i)\n",
    "        training_nodes = torch.tensor(NC_5folds[i]['train'])\n",
    "        test_nodes = torch.tensor(NC_5folds[i]['test'])\n",
    "        training_features = node_features_torch[training_nodes]\n",
    "        test_features = node_features_torch[test_nodes]\n",
    "\n",
    "        neighborhoods_train = utils.compute_neighborhoods_subgraph(total_graph['edges'], NC_5folds[i]['train'], 25)\n",
    "        neighborhoods_test = utils.compute_neighborhoods_subgraph(total_graph['edges'], NC_5folds[i]['test'], 25)\n",
    "\n",
    "        X_train = model.infer(training_features, neighborhoods_train)\n",
    "        X_test = model.infer(test_features, neighborhoods_test)\n",
    "        # For the datasets that only have one one label per node, it gives better results to not use multioutputclassifier\n",
    "        if not total_graph['Multioutput']:\n",
    "            yt = []\n",
    "            for n in NC_5folds[i]['train']:\n",
    "                if len(total_graph['groups'][n]):\n",
    "                    yt.append(total_graph['groups'][n][0])\n",
    "                else:\n",
    "                    yt.append(0)\n",
    "            #Y_train_sequence = np.array([total_graph['groups'][node][0]  for node in  NC_5folds[i]['train']],dtype=int)\n",
    "            Y_train_sequence = np.array(yt,dtype=int)\n",
    "            yt = []\n",
    "            for n in NC_5folds[i]['test']:\n",
    "                if len(total_graph['groups'][n]):\n",
    "                    yt.append(total_graph['groups'][n][0])\n",
    "                else:\n",
    "                    yt.append(0)\n",
    "            #Y_test_sequence = np.array([total_graph['groups'][node][0] for node in  NC_5folds[i]['test'] if len(total_graph['groups'][node])], dtype=int)\n",
    "            Y_test_sequence = np.array(yt, dtype=int)\n",
    "            log_reg = LogisticRegression(multi_class=\"ovr\", max_iter=200)\n",
    "            Y_train = Y_train_sequence\n",
    "            Y_test = Y_test_sequence\n",
    "            log_reg.fit(X_train, Y_train)\n",
    "            Y_pred = log_reg.predict(X_test)\n",
    "            Y_pred = utils.onehot(Y_pred, N_classes)\n",
    "            Y_test = utils.onehot(Y_test, N_classes)\n",
    "        else:\n",
    "            print(\"fitting model\")\n",
    "            Y_train_sequence = [total_graph['groups'][node]  for node in NC_5folds[i]['train']]\n",
    "            Y_test_sequence = [total_graph['groups'][node] for node in NC_5folds[i]['test']]\n",
    "            Y_train = mb.fit_transform(Y_train_sequence)\n",
    "            Y_test = mb.fit_transform(Y_test_sequence)\n",
    "            log_reg = MultiOutputClassifier(LogisticRegression(multi_class=\"ovr\", max_iter=200))   #\n",
    "            log_reg.fit(X_train, Y_train)\n",
    "            Y_pred = log_reg.predict(X_test)\n",
    "    \n",
    "        f1_macro = utils.compute_f1_macro(Y_test, Y_pred, N_classes)\n",
    "        f1_micro = utils.compute_f1_micro(Y_test, Y_pred, N_classes)\n",
    "\n",
    "        f1_macro_list.append(f1_macro)\n",
    "        f1_micro_list.append(f1_micro)\n",
    "        print(f1_macro, f1_micro)\n",
    "\n",
    "        \n",
    "    print(np.mean(f1_micro_list))\n",
    "    print(np.mean(f1_macro_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting graphs\n",
      "0.099932478055368\n",
      "0.199864956110736\n",
      "0.299797434166104\n",
      "0.399729912221472\n",
      "0.49966239027683995\n",
      "0.599594868332208\n",
      "0.699527346387576\n",
      "0.799459824442944\n",
      "0.8993923024983119\n",
      "0.9993247805536799\n",
      "balancing test graph\n",
      "0.19993998049366044\n",
      "0.39987996098732087\n",
      "0.5998199414809813\n",
      "0.7997599219746417\n",
      "0.9996999024683022\n",
      "balancing training graph\n",
      "0.099932478055368\n",
      "0.199864956110736\n",
      "0.299797434166104\n",
      "0.399729912221472\n",
      "0.49966239027683995\n",
      "0.599594868332208\n",
      "0.699527346387576\n",
      "0.799459824442944\n",
      "0.8993923024983119\n",
      "0.9993247805536799\n",
      "fit model\n",
      "0.6760356974220538\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "directed_graph = False\n",
    "reverse_fraction = 0\n",
    "LP_test_X_unb, LP_test_Y_unb, training_graph_unbalanced, test_graph_unbalanced = utils.split_graphs(total_graph, directed=directed_graph)\n",
    "LP_test_X, LP_test_Y = utils.balance_test_graph(total_graph, LP_test_X_unb, LP_test_Y_unb, test_graph_unbalanced, directed=directed_graph, reverse_fraction=reverse_fraction)\n",
    "LP_train_X, LP_train_Y = utils.balance_training_graph(training_graph_unbalanced, total_graph, directed=directed_graph)\n",
    "\n",
    "\n",
    "neighborhoods_train = utils.compute_neighborhoods_subgraph(training_graph_unbalanced, total_graph['nodes'], nb_size)\n",
    "neighborhoods_test = utils.compute_neighborhoods_subgraph(test_graph_unbalanced, total_graph['nodes'], nb_size)\n",
    "\n",
    "node_features_np = get_node_features(total_graph, mode=feature_type)\n",
    "node_features_torch = torch.tensor(node_features_np).float()\n",
    "\n",
    "Y_train = LP_train_Y\n",
    "Y_test = LP_test_Y\n",
    "with torch.no_grad():\n",
    "    # build representation of edge datasets using inner product of the representation of the two nodes\n",
    "    X_train = np.zeros((len(LP_train_X), 1))\n",
    "    Z_train = model.infer(node_features_torch, neighborhoods_train)\n",
    "    for i, edge in enumerate(LP_train_X):\n",
    "        u = edge[0]\n",
    "        v = edge[1]\n",
    "        X_train[i] = utils.get_edge_representation(Z_train[u], Z_train[v])\n",
    "    X_test = np.zeros((len(LP_test_X), 1))\n",
    "    Z_test = model.infer(node_features_torch, neighborhoods_test)\n",
    "    for i, edge in enumerate(LP_test_X):\n",
    "        u = edge[0]\n",
    "        v = edge[1]\n",
    "        X_test[i] = utils.get_edge_representation(Z_test[u], Z_test[v])\n",
    "        \n",
    "    print(\"fit model\")\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    Y_probs = classifier.predict_proba(X_test)[:,1]\n",
    "    roc_auc = roc_auc_score(Y_test, Y_probs)\n",
    "    print(roc_auc)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"../Results/graphsage/{}_metrics{}.csv\".format(dataset_name, reverse_fraction), \"w\") as file:\n",
    "    settings_str = \"Results for graphsage embedding generated with {} epochs, K={}, Q={}, nb_size={}\\n\".format(num_epochs, K, Q, nb_size)\n",
    "    file.write(settings_str)\n",
    "    header = \"Dataset; F1 macro; F1 micro; ROC-AUC \\n\"\n",
    "    file.write(header)\n",
    "    data_row = \"{dataset};{f1mac};{f1mic};{roc}\".format(dataset=dataset_name, f1mac=np.mean(f1_macro_list), f1mic=np.mean(f1_micro_list), roc=roc_auc)\n",
    "    file.write(data_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
