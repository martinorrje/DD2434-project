{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from load_data import load_toy, load_blogcatalog\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_name = \"BlogCatalog\"\n",
    "data_dir = \"../Data/\" + dataset_name\n",
    "\n",
    "total_graph = load_blogcatalog(data_dir)\n",
    "#total_graph = load_toy(data_dir)\n",
    "\n",
    "\n",
    "\n",
    "def get_node_features(total_graph, mode=\"degree\"):\n",
    "    if mode==\"degree\":\n",
    "        node_features = np.zeros((total_graph['N_nodes'],2))\n",
    "        for i in range(total_graph['N_nodes']):\n",
    "            neighbors = total_graph['edges'][i]\n",
    "            degree = len(neighbors)\n",
    "            node_features[i,0] = degree\n",
    "            second_neighbors_count = 0\n",
    "            for n in neighbors:\n",
    "                second_neighbors_count += len(total_graph['edges'][n])\n",
    "            node_features[i,1] = second_neighbors_count\n",
    "    elif mode==\"degree_dist\":\n",
    "        node_features = total_graph['adj_matrix']  \n",
    "    elif mode==\"node_nr\":\n",
    "        node_features = np.zeros((total_graph['N_nodes'],total_graph['N_nodes']))\n",
    "        for i in range(total_graph['N_nodes']):\n",
    "            node_features[i,i] = 1\n",
    "    return node_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GraphSage(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, K):\n",
    "        super(GraphSage, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_dim, output_dim))\n",
    "        for i in range(K-1):\n",
    "            self.layers.append(nn.Linear(output_dim, output_dim))\n",
    "        self.K = K\n",
    "    \n",
    "    \n",
    "    def aggregate_neighbors(self, batch):\n",
    "        batch_nodes = batch.src_index\n",
    "        subgraph_edge_indices = batch.edge_index\n",
    "\n",
    "        neighborhoods = {}\n",
    "        for key, value in zip(subgraph_edge_indices[0], subgraph_edge_indices[1]):\n",
    "            if key.item() not in neighborhoods:\n",
    "                neighborhoods[key.item()] = []\n",
    "            neighborhoods[key.item()].append(value.item())\n",
    "        \n",
    "        B = [[] for k in range(self.K+1)]\n",
    "        B[-1] = batch_nodes[:].tolist()\n",
    "        for k in range(self.K, 0, -1):\n",
    "            B[k-1] = B[k][:]\n",
    "            for node in B[k]:  \n",
    "                B[k-1].extend(neighborhoods[node])\n",
    "        return B, neighborhoods\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        #need node_features, batch_nodes\n",
    "        eps = 1e-9\n",
    "        \n",
    "        B, neighborhood_dict = self.aggregate_neighbors(batch)\n",
    "        #print(B)\n",
    "        #print(neighborhood_dict)\n",
    "        h = batch.x\n",
    "        N_nodes = h.shape[0]\n",
    "        for k in range(self.K):\n",
    "            layer = self.layers[k]\n",
    "            h_updated = torch.zeros((N_nodes, layer.out_features))\n",
    "            for i,v in enumerate(B[k]):        \n",
    "                neighborhood = neighborhood_dict[v]\n",
    "                hv = h[v].view(1, -1)\n",
    "                hN = h[neighborhood]\n",
    "                conc = torch.cat([hN, hv], dim=0)\n",
    "                aggregated = torch.mean(conc, dim=0, keepdim=True)\n",
    "                output = layer(aggregated)\n",
    "                hv = F.relu(output)\n",
    "                h_updated[v] =  hv    \n",
    "            h = h_updated\n",
    "            h = h / (torch.norm(h, dim=1, keepdim=True)+eps)\n",
    "        return h\n",
    "\n",
    "\n",
    "def compute_neighborhoods(edge_dict, N_nodes, nb_size):\n",
    "    neighborhoods = [[] for _ in range(N_nodes)]\n",
    "    for v, neighbors in edge_dict.items():\n",
    "        nb = len(neighbors)\n",
    "        sample_size = min(nb_size, nb)\n",
    "        if sample_size == 1:\n",
    "            sample_neighborhood = [neighbors[0]]*nb_size\n",
    "        else:\n",
    "            neighborhood_ind = torch.randint(0, nb, (sample_size,))\n",
    "            sample_neighborhood = [neighbors[i] for i in neighborhood_ind.tolist()]\n",
    "        neighborhoods[v] = sample_neighborhood\n",
    "    return neighborhoods\n",
    "\n",
    "\n",
    "def compute_loss(Z, Z_pos, Z_neg):\n",
    "    dot = torch.sum(Z * Z_pos, dim=1)\n",
    "    term1 = -torch.log(torch.sigmoid(dot))\n",
    "    term2 = 0\n",
    "    for q in range(Z_neg.shape[0]):\n",
    "        term2 = -torch.log(torch.sigmoid(-torch.sum(Z * Z_neg[q,:,:], dim=1)))\n",
    "    return torch.mean(term1+term2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 tensor(1.4868, grad_fn=<MeanBackward0>)\n",
      "0.0002994011976047904 tensor(1.5142, grad_fn=<MeanBackward0>)\n",
      "0.0005988023952095808 tensor(1.5283, grad_fn=<MeanBackward0>)\n",
      "0.0008982035928143712 tensor(1.4996, grad_fn=<MeanBackward0>)\n",
      "0.0011976047904191617 tensor(1.4718, grad_fn=<MeanBackward0>)\n",
      "0.0014970059880239522 tensor(1.4837, grad_fn=<MeanBackward0>)\n",
      "0.0017964071856287425 tensor(1.4346, grad_fn=<MeanBackward0>)\n",
      "0.002095808383233533 tensor(1.3952, grad_fn=<MeanBackward0>)\n",
      "0.0023952095808383233 tensor(1.3987, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m positive_samples \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mdst_pos_index\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     30\u001b[0m negative_samples \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mdst_neg_index\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 31\u001b[0m Z_tot \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m Z \u001b[38;5;241m=\u001b[39m Z_tot[batch\u001b[38;5;241m.\u001b[39msrc_index\u001b[38;5;241m.\u001b[39mtolist()]\n\u001b[1;32m     33\u001b[0m Z_pos \u001b[38;5;241m=\u001b[39m Z_tot[positive_samples]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python310/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python310/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[105], line 49\u001b[0m, in \u001b[0;36mGraphSage.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     47\u001b[0m     aggregated \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(conc, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     48\u001b[0m     output \u001b[38;5;241m=\u001b[39m layer(aggregated)\n\u001b[0;32m---> 49\u001b[0m     hv \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     h_updated[v] \u001b[38;5;241m=\u001b[39m  hv    \n\u001b[1;32m     51\u001b[0m h \u001b[38;5;241m=\u001b[39m h_updated\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python310/lib/python3.10/site-packages/torch/nn/functional.py:1471\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1471\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import LinkNeighborLoader, NeighborLoader\n",
    "from torch_geometric.sampler import NegativeSampling\n",
    "\n",
    "\n",
    "node_features_np = get_node_features(total_graph, mode=\"node_nr\")\n",
    "node_features_torch = torch.tensor(node_features_np).float()\n",
    "\n",
    "node_links_torch = torch.tensor(total_graph['edges_list']).T\n",
    "graph_data = torch_geometric.data.Data(node_features_torch, node_links_torch)\n",
    "# parameters\n",
    "num_epochs = 1\n",
    "K = 1   # number of iterations\n",
    "Q = 2\n",
    "batch_size = 128\n",
    "input_size = node_features_np.shape[1]\n",
    "output_dim = 64\n",
    "nb_size = 10\n",
    "\n",
    "# Create the model\n",
    "model = GraphSage(input_size, output_dim, K)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "ns = NegativeSampling(mode=\"triplet\", amount=Q)\n",
    "for epoch in range(num_epochs):\n",
    "    #loader = NeighborLoader(graph_data, input_nodes=torch.arange(total_graph['N_nodes']), batch_size=batch_size, num_neighbors=[10]*K, replace=True, subgraph_type=\"bidirectional\")\n",
    "    loader = LinkNeighborLoader(graph_data, batch_size=batch_size, num_neighbors=[nb_size]*K, neg_sampling=ns, shuffle=True, replace=True, subgraph_type=\"bidirectional\")  \n",
    "    for i,batch in enumerate(loader):\n",
    " \n",
    "        positive_samples = batch.dst_pos_index.tolist()\n",
    "        negative_samples = batch.dst_neg_index.numpy()\n",
    "        Z_tot = model(batch)\n",
    "        Z = Z_tot[batch.src_index.tolist()]\n",
    "        Z_pos = Z_tot[positive_samples]\n",
    "        Z_neg = torch.zeros((Q, batch_size, output_dim))\n",
    "        for q in range(Q):\n",
    "            Z_neg[q] = Z_tot[negative_samples[:,q]]\n",
    "       \n",
    "        loss = compute_loss(Z, Z_pos, Z_neg)\n",
    "        print(i/len(loader), loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    node_features_np = get_node_features(total_graph, mode=\"node_nr\")\n",
    "    node_features_torch = torch.tensor(node_features_np).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NC_5folds = {}\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "nodes = np.array([i for i in range(total_graph['N_nodes'])])\n",
    "for i, (train_index, test_index) in enumerate(kf.split(nodes)):  \n",
    "    NC_5folds[i] = {\"train\":list(nodes[train_index]), \"test\":list(nodes[test_index])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "klar med z train\n",
      "hej\n",
      "15 1 2887\n",
      "0.005827505827505827 0.01028101439342015\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python310/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from  sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def onehot(y, nclasses):\n",
    "    Y = np.zeros((y.shape[0], nclasses), dtype=int)\n",
    "    for i in range(y.shape[0]):\n",
    "        c = y[i]\n",
    "        Y[i,c-1] =  1\n",
    "    return Y\n",
    "\n",
    "\n",
    "def precision_and_recall(Y_true, Y_pred, nclasses):\n",
    "    # count true positives and false positives and false negatives\n",
    "    TP_list = [0]*nclasses\n",
    "    FP_list = [0]*nclasses\n",
    "    FN_list = [0]*nclasses\n",
    "    for j in range(nclasses):\n",
    "       for i, pred in enumerate(Y_pred):\n",
    "            if pred[j]==1 and Y_true[i][j]==1:\n",
    "                TP_list[j] += 1\n",
    "            elif pred[j]==1 and  Y_true[i][j]==0:\n",
    "                FP_list[j] += 1\n",
    "            elif pred[j]==0 and Y_true[i][j]==1:\n",
    "                FN_list[j] += 1 \n",
    "\n",
    "    return TP_list, FP_list, FN_list\n",
    "\n",
    "def compute_f1_macro(Y_true, Y_pred, nclasses):\n",
    "    TP_list, FP_list, FN_list = precision_and_recall(Y_true, Y_pred, nclasses)\n",
    "    f1_scores = [0]*nclasses\n",
    "    for k in range(nclasses):\n",
    "        if TP_list[k]==0:\n",
    "            continue\n",
    "        f1_scores[k] = TP_list[k]/(TP_list[k]+0.5*(FP_list[k]+FN_list[k])) \n",
    "    return np.sum(f1_scores)/nclasses\n",
    "\n",
    "\n",
    "def compute_f1_micro(Y_true, Y_pred, nclasses):\n",
    "    TP_list, FP_list, FN_list = precision_and_recall(Y_true, Y_pred, nclasses)\n",
    "    TP = np.sum(TP_list)\n",
    "    FP = np.sum(FP_list)\n",
    "    FN = np.sum(FN_list)\n",
    "    print(TP, FP, FN)\n",
    "    return TP/(TP + 0.5*(FN+FP))\n",
    "\n",
    "\n",
    "N_classes = total_graph['N_classes']\n",
    "mb = MultiLabelBinarizer(classes=[i for i in range(N_classes)])\n",
    "f1_macro_list = []\n",
    "f1_micro_list = []\n",
    "\n",
    "\n",
    "# 5-fold cross validation\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        print(i)\n",
    "        node_features_np = deepcopy(get_node_features(total_graph, mode=\"node_nr\"))\n",
    "        node_features_torch = torch.tensor(node_features_np).float()    \n",
    "        training_nodes = NC_5folds[i]['train']\n",
    "        test_nodes = NC_5folds[i]['test']\n",
    "        Z_train = model(training_nodes, node_features_torch, neighborhoods)\n",
    "        print(\"klar med z train\")\n",
    "        Z_test = model(test_nodes, node_features_torch, neighborhoods)\n",
    "        X_train = Z_train\n",
    "        X_test = Z_test\n",
    "        # For the datasets that only have one one label per node, it gives better results to not use multioutputclassifier\n",
    "        if not total_graph['Multioutput']:\n",
    "            Y_train_sequence = np.array([total_graph['groups'][node][0]  for node in training_nodes],dtype=int)\n",
    "            Y_test_sequence = np.array([total_graph['groups'][node][0] for node in test_nodes], dtype=int)\n",
    "            log_reg = LogisticRegression(multi_class=\"ovr\", max_iter=200)\n",
    "            Y_train = Y_train_sequence\n",
    "            Y_test = Y_test_sequence\n",
    "            log_reg.fit(X_train, Y_train)\n",
    "            Y_pred = log_reg.predict(X_test)\n",
    "            Y_pred = onehot(Y_pred, total_graph['N_classes'])\n",
    "            Y_test = onehot(Y_test, total_graph['N_classes'])\n",
    "        else:\n",
    "            print(\"hej\")\n",
    "            Y_train_sequence = [total_graph['groups'][node]  for node in training_nodes]\n",
    "            Y_test_sequence = [total_graph['groups'][node] for node in test_nodes]\n",
    "            Y_train = mb.fit_transform(Y_train_sequence)\n",
    "            Y_test = mb.fit_transform(Y_test_sequence)\n",
    "            log_reg = MultiOutputClassifier(SGDClassifier(max_iter=200))   #multi_class=\"ovr\",\n",
    "            log_reg.fit(X_train, Y_train)\n",
    "            Y_pred = log_reg.predict(X_test)\n",
    "    \n",
    "        f1_macro = compute_f1_macro(Y_test, Y_pred, N_classes)\n",
    "        f1_micro = compute_f1_micro(Y_test, Y_pred, N_classes)\n",
    "\n",
    "        f1_macro_list.append(f1_macro)\n",
    "        f1_micro_list.append(f1_micro)\n",
    "        print(f1_macro, f1_micro)\n",
    "        sys.exit()\n",
    "        \n",
    "    print(np.mean(f1_micro_list))\n",
    "    print(np.mean(f1_macro_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
