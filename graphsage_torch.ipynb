{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from load_data import *\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18333 163788\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_name = \"Coauthor\"\n",
    "data_dir = \"../Data/\" + dataset_name\n",
    "\n",
    "total_graph = load_geometric_dataset(dataset_name)\n",
    "#total_graph = load_blogcatalog(data_dir)\n",
    "#total_graph = load_toy(data_dir)\n",
    "#total_graph = load_pubmed(data_dir)\n",
    "#total_graph = load_flickr(data_dir)\n",
    "#total_graph = load_reddit(data_dir)\n",
    "\n",
    "def get_node_features(total_graph, mode=\"degree\"):\n",
    "    if mode==\"degree\":\n",
    "        node_features = np.zeros((total_graph['N_nodes'],2))\n",
    "        for i in range(total_graph['N_nodes']):\n",
    "            neighbors = total_graph['edges'][i]\n",
    "            degree = len(neighbors)\n",
    "            node_features[i,0] = degree\n",
    "            second_neighbors_count = 0\n",
    "            for n in neighbors:\n",
    "                second_neighbors_count += len(total_graph['edges'][n])\n",
    "            node_features[i,1] = second_neighbors_count\n",
    "    elif mode==\"degree_dist\":\n",
    "        node_features = total_graph['adj_matrix']/(np.linalg.norm(total_graph['adj_matrix'], axis=1, keepdims=True)+1e-9)\n",
    "    elif mode==\"node_nr\":\n",
    "        node_features = np.zeros((total_graph['N_nodes'],total_graph['N_nodes']))\n",
    "        for i in range(total_graph['N_nodes']):\n",
    "            node_features[i,i] = 1\n",
    "    return node_features\n",
    "\n",
    "print(total_graph['N_nodes'], total_graph['N_edges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GraphSage(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, K):\n",
    "        super(GraphSage, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_dim, output_dim))\n",
    "        for i in range(K-1):\n",
    "            self.layers.append(nn.Linear(output_dim, output_dim))\n",
    "        for i in range(K):\n",
    "            nn.init.zeros_(self.layers[i].bias)\n",
    "        self.K = K\n",
    "    \n",
    "    def infer(self, node_features, neighborhood_dict):\n",
    "        eps = 1e-9\n",
    "        h = node_features\n",
    "        N_nodes = h.shape[0]\n",
    "        for k in range(self.K):\n",
    "            layer = self.layers[k]\n",
    "            h_updated = torch.zeros((N_nodes, layer.out_features))\n",
    "            for v in range(N_nodes):        \n",
    "                neighborhood = neighborhood_dict[v]\n",
    "                \n",
    "                hv = h[v].view(1, -1)\n",
    "                hN = h[neighborhood]\n",
    "                conc = torch.cat([hN, hv], dim=0)\n",
    "                aggregated = torch.mean(conc, dim=0, keepdim=True)\n",
    "                output = layer(aggregated)\n",
    "                hv = F.relu(output)\n",
    "                h_updated[v] =  hv    \n",
    "            h = h_updated\n",
    "            h = h / (torch.norm(h, dim=1, keepdim=True)+eps)\n",
    "        return h\n",
    "\n",
    "    \n",
    "    def aggregate_neighbors(self, batch):\n",
    "        batch_nodes = batch.src_index\n",
    "        subgraph_edge_indices = batch.edge_index\n",
    "\n",
    "        neighborhoods = {i.item():[] for i in batch_nodes}\n",
    "        for src, target in zip(subgraph_edge_indices[0], subgraph_edge_indices[1]):\n",
    "            src = src.item()\n",
    "            target = target.item()\n",
    "            if not neighborhoods.get(src):\n",
    "                neighborhoods[src] = []\n",
    "            neighborhoods[src].append(target)\n",
    "        B = [[] for k in range(self.K+1)]\n",
    "        B[-1] = batch_nodes[:].tolist()\n",
    "        for k in range(self.K, 0, -1):\n",
    "            B[k-1] = B[k][:]\n",
    "            for node in B[k]:  \n",
    "                B[k-1].extend(neighborhoods[node])\n",
    "        return B, neighborhoods\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        eps = 1e-9\n",
    "        B, neighborhood_dict = self.aggregate_neighbors(batch)\n",
    "        h = batch.x\n",
    "        N_nodes = h.shape[0]\n",
    "        for k in range(self.K):\n",
    "            layer = self.layers[k]\n",
    "            h_updated = torch.zeros((N_nodes, layer.out_features))\n",
    "            for i,v in enumerate(B[k+1]):      # this is because B[0] is base case, B[1] are nodes corresponding to layer 1 etc  \n",
    "                neighborhood = neighborhood_dict[v]\n",
    "                hv = h[v].view(1, -1)\n",
    "                hN = h[neighborhood]\n",
    "                conc = torch.cat([hN, hv], dim=0)\n",
    "                aggregated = torch.mean(conc, dim=0, keepdim=True)\n",
    "                output = layer(aggregated)\n",
    "                hv = F.relu(output)\n",
    "                h_updated[v] =  hv    \n",
    "            h = h_updated\n",
    "            h = h / (torch.norm(h, dim=1, keepdim=True)+eps)\n",
    "        return h\n",
    "\n",
    "\n",
    "\n",
    "def compute_loss(Z, Z_pos, Z_neg):\n",
    "    eps = 1e-9\n",
    "    dot = torch.sum(Z * Z_pos, dim=1)\n",
    "    term1 = -torch.log(torch.sigmoid(dot)+eps)\n",
    "    term2 = 0\n",
    "    for q in range(Z_neg.shape[0]):\n",
    "        term2 = -torch.log(torch.sigmoid(-torch.sum(Z * Z_neg[q,:,:], dim=1))+eps)\n",
    "    return torch.mean(term1+term2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18333 163788\n",
      "starting training\n",
      "0.0 tensor(1.3852, grad_fn=<MeanBackward0>)\n",
      "0.0015625 tensor(1.3801, grad_fn=<MeanBackward0>)\n",
      "0.003125 tensor(1.3776, grad_fn=<MeanBackward0>)\n",
      "0.0046875 tensor(1.3835, grad_fn=<MeanBackward0>)\n",
      "0.00625 tensor(1.3830, grad_fn=<MeanBackward0>)\n",
      "0.0078125 tensor(1.3840, grad_fn=<MeanBackward0>)\n",
      "0.009375 tensor(1.3856, grad_fn=<MeanBackward0>)\n",
      "0.0109375 tensor(1.3755, grad_fn=<MeanBackward0>)\n",
      "0.0125 tensor(1.3819, grad_fn=<MeanBackward0>)\n",
      "0.0140625 tensor(1.3848, grad_fn=<MeanBackward0>)\n",
      "0.015625 tensor(1.3842, grad_fn=<MeanBackward0>)\n",
      "0.0171875 tensor(1.3851, grad_fn=<MeanBackward0>)\n",
      "0.01875 tensor(1.3838, grad_fn=<MeanBackward0>)\n",
      "0.0203125 tensor(1.3887, grad_fn=<MeanBackward0>)\n",
      "0.021875 tensor(1.3894, grad_fn=<MeanBackward0>)\n",
      "0.0234375 tensor(1.3944, grad_fn=<MeanBackward0>)\n",
      "0.025 tensor(1.3810, grad_fn=<MeanBackward0>)\n",
      "0.0265625 tensor(1.3953, grad_fn=<MeanBackward0>)\n",
      "0.028125 tensor(1.3847, grad_fn=<MeanBackward0>)\n",
      "0.0296875 tensor(1.3875, grad_fn=<MeanBackward0>)\n",
      "0.03125 tensor(1.3846, grad_fn=<MeanBackward0>)\n",
      "0.0328125 tensor(1.3847, grad_fn=<MeanBackward0>)\n",
      "0.034375 tensor(1.3755, grad_fn=<MeanBackward0>)\n",
      "0.0359375 tensor(1.3857, grad_fn=<MeanBackward0>)\n",
      "0.0375 tensor(1.3804, grad_fn=<MeanBackward0>)\n",
      "0.0390625 tensor(1.3726, grad_fn=<MeanBackward0>)\n",
      "0.040625 tensor(1.3874, grad_fn=<MeanBackward0>)\n",
      "0.0421875 tensor(1.3919, grad_fn=<MeanBackward0>)\n",
      "0.04375 tensor(1.3856, grad_fn=<MeanBackward0>)\n",
      "0.0453125 tensor(1.3866, grad_fn=<MeanBackward0>)\n",
      "0.046875 tensor(1.3884, grad_fn=<MeanBackward0>)\n",
      "0.0484375 tensor(1.3849, grad_fn=<MeanBackward0>)\n",
      "0.05 tensor(1.3853, grad_fn=<MeanBackward0>)\n",
      "0.0515625 tensor(1.3724, grad_fn=<MeanBackward0>)\n",
      "0.053125 tensor(1.3825, grad_fn=<MeanBackward0>)\n",
      "0.0546875 tensor(1.3855, grad_fn=<MeanBackward0>)\n",
      "0.05625 tensor(1.3879, grad_fn=<MeanBackward0>)\n",
      "0.0578125 tensor(1.3854, grad_fn=<MeanBackward0>)\n",
      "0.059375 tensor(1.3700, grad_fn=<MeanBackward0>)\n",
      "0.0609375 tensor(1.3880, grad_fn=<MeanBackward0>)\n",
      "0.0625 tensor(1.3919, grad_fn=<MeanBackward0>)\n",
      "0.0640625 tensor(1.3922, grad_fn=<MeanBackward0>)\n",
      "0.065625 tensor(1.3904, grad_fn=<MeanBackward0>)\n",
      "0.0671875 tensor(1.3880, grad_fn=<MeanBackward0>)\n",
      "0.06875 tensor(1.3855, grad_fn=<MeanBackward0>)\n",
      "0.0703125 tensor(1.3882, grad_fn=<MeanBackward0>)\n",
      "0.071875 tensor(1.3728, grad_fn=<MeanBackward0>)\n",
      "0.0734375 tensor(1.3732, grad_fn=<MeanBackward0>)\n",
      "0.075 tensor(1.3853, grad_fn=<MeanBackward0>)\n",
      "0.0765625 tensor(1.3887, grad_fn=<MeanBackward0>)\n",
      "0.078125 tensor(1.3772, grad_fn=<MeanBackward0>)\n",
      "0.0796875 tensor(1.3833, grad_fn=<MeanBackward0>)\n",
      "0.08125 tensor(1.3788, grad_fn=<MeanBackward0>)\n",
      "0.0828125 tensor(1.3985, grad_fn=<MeanBackward0>)\n",
      "0.084375 tensor(1.3851, grad_fn=<MeanBackward0>)\n",
      "0.0859375 tensor(1.3853, grad_fn=<MeanBackward0>)\n",
      "0.0875 tensor(1.3841, grad_fn=<MeanBackward0>)\n",
      "0.0890625 tensor(1.3770, grad_fn=<MeanBackward0>)\n",
      "0.090625 tensor(1.3863, grad_fn=<MeanBackward0>)\n",
      "0.0921875 tensor(1.3906, grad_fn=<MeanBackward0>)\n",
      "0.09375 tensor(1.3853, grad_fn=<MeanBackward0>)\n",
      "0.0953125 tensor(1.3851, grad_fn=<MeanBackward0>)\n",
      "0.096875 tensor(1.3829, grad_fn=<MeanBackward0>)\n",
      "0.0984375 tensor(1.3878, grad_fn=<MeanBackward0>)\n",
      "0.1 tensor(1.3922, grad_fn=<MeanBackward0>)\n",
      "0.1015625 tensor(1.3787, grad_fn=<MeanBackward0>)\n",
      "0.103125 tensor(1.3759, grad_fn=<MeanBackward0>)\n",
      "0.1046875 tensor(1.3785, grad_fn=<MeanBackward0>)\n",
      "0.10625 tensor(1.3823, grad_fn=<MeanBackward0>)\n",
      "0.1078125 tensor(1.3905, grad_fn=<MeanBackward0>)\n",
      "0.109375 tensor(1.3871, grad_fn=<MeanBackward0>)\n",
      "0.1109375 tensor(1.3852, grad_fn=<MeanBackward0>)\n",
      "0.1125 tensor(1.3869, grad_fn=<MeanBackward0>)\n",
      "0.1140625 tensor(1.3806, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import LinkNeighborLoader, NeighborLoader\n",
    "from torch_geometric.sampler import NegativeSampling\n",
    "\n",
    "\n",
    "feature_type = \"node_nr\"\n",
    "node_features_np = get_node_features(total_graph, mode=feature_type)  # to be used for normal datasets\n",
    "if dataset_name==\"toy\":\n",
    "    node_features_np = total_graph['node_feats']    # this is for toy dataset\n",
    "\n",
    "node_features_torch = torch.tensor(node_features_np).float()\n",
    "if dataset_name in [\"Actor\", \"Coauthor\", \"Reddit\", \"Citeseer\"]:\n",
    "    node_links_torch = total_graph['edges_list']\n",
    "else:\n",
    "    node_links_torch = torch.tensor(total_graph['edges_list']).T\n",
    "\n",
    "print(total_graph['N_nodes'], total_graph['N_edges'])\n",
    "    \n",
    "graph_data = torch_geometric.data.Data(node_features_torch, node_links_torch)\n",
    "# parameters\n",
    "num_epochs = 1\n",
    "K = 1   # number of iterations\n",
    "Q = 10\n",
    "batch_size = 256\n",
    "input_size = node_features_np.shape[1]\n",
    "output_dim = 128\n",
    "nb_size = 10\n",
    "directed_graph = False  # used later for LP\n",
    "# Create the model\n",
    "model = GraphSage(input_size, output_dim, K)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "ns = NegativeSampling(mode=\"triplet\", amount=Q)\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    loader = LinkNeighborLoader(graph_data, batch_size=batch_size, num_neighbors=[nb_size]*K, neg_sampling=ns, shuffle=True, replace=True)\n",
    "\n",
    "    print(\"starting training\")\n",
    "    for i,batch in enumerate(loader):\n",
    "        if batch.src_index.shape[0] != batch_size:\n",
    "            break \n",
    "        positive_samples = batch.dst_pos_index.tolist()\n",
    "        negative_samples = batch.dst_neg_index.numpy()\n",
    "        Z_tot = model(batch)\n",
    "        Z = Z_tot[batch.src_index.tolist()]\n",
    "        Z_pos = Z_tot[positive_samples]\n",
    "        Z_neg = torch.zeros((Q, batch_size, output_dim))\n",
    "        for q in range(Q):\n",
    "            Z_neg[q] = Z_tot[negative_samples[:,q]]\n",
    "       \n",
    "        loss = compute_loss(Z, Z_pos, Z_neg)\n",
    "        print(i/len(loader), loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NC_5folds = {}\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "nodes = np.array([i for i in range(total_graph['N_nodes'])])\n",
    "#labels = np.array([total_graph['grops'][n] for n in nodes])\n",
    "for i, (train_index, test_index) in enumerate(kf.split(nodes)):  \n",
    "    NC_5folds[i] = {\"train\":list(nodes[train_index]), \"test\":list(nodes[test_index])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "372 1148 1148\n",
      "0.04974966353337549 0.24473684210526317\n",
      "1\n",
      "384 1136 1136\n",
      "0.05052142120651409 0.25263157894736843\n",
      "2\n",
      "383 1137 1137\n",
      "0.055295957959609426 0.2519736842105263\n",
      "3\n",
      "358 1162 1162\n",
      "0.04846327365007754 0.2355263157894737\n",
      "4\n",
      "379 1141 1141\n",
      "0.055493585644607685 0.2493421052631579\n",
      "0.2468421052631579\n",
      "0.05190478039883685\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from  sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def onehot(y, nclasses):\n",
    "    Y = np.zeros((y.shape[0], nclasses), dtype=int)\n",
    "    for i in range(y.shape[0]):\n",
    "        c = y[i]\n",
    "        Y[i,c-1] =  1\n",
    "    return Y\n",
    "\n",
    "\n",
    "def precision_and_recall(Y_true, Y_pred, nclasses):\n",
    "    # count true positives and false positives and false negatives\n",
    "    TP_list = [0]*nclasses\n",
    "    FP_list = [0]*nclasses\n",
    "    FN_list = [0]*nclasses\n",
    "    for j in range(nclasses):\n",
    "       for i, pred in enumerate(Y_pred):\n",
    "            if pred[j]==1 and Y_true[i][j]==1:\n",
    "                TP_list[j] += 1\n",
    "            elif pred[j]==1 and  Y_true[i][j]==0:\n",
    "                FP_list[j] += 1\n",
    "            elif pred[j]==0 and Y_true[i][j]==1:\n",
    "                FN_list[j] += 1 \n",
    "\n",
    "    return TP_list, FP_list, FN_list\n",
    "\n",
    "def compute_f1_macro(Y_true, Y_pred, nclasses):\n",
    "    TP_list, FP_list, FN_list = precision_and_recall(Y_true, Y_pred, nclasses)\n",
    "    f1_scores = [0]*nclasses\n",
    "    for k in range(nclasses):\n",
    "        if TP_list[k]==0:\n",
    "            continue\n",
    "        f1_scores[k] = TP_list[k]/(TP_list[k]+0.5*(FP_list[k]+FN_list[k])) \n",
    "    return np.sum(f1_scores)/nclasses\n",
    "\n",
    "\n",
    "def compute_f1_micro(Y_true, Y_pred, nclasses):\n",
    "    TP_list, FP_list, FN_list = precision_and_recall(Y_true, Y_pred, nclasses)\n",
    "    TP = np.sum(TP_list)\n",
    "    FP = np.sum(FP_list)\n",
    "    FN = np.sum(FN_list)\n",
    "    print(TP, FP, FN)\n",
    "    return TP/(TP + 0.5*(FN+FP))\n",
    "\n",
    "def ind_mapping(nodes):\n",
    "    mapping = {}\n",
    "    c = 0\n",
    "    for node in nodes:\n",
    "        mapping[node] = c\n",
    "        c += 1\n",
    "    return mapping\n",
    "\n",
    "def compute_neighborhoods_subgraph(edge_dict, nodes, nb_size):\n",
    "    unique_nodes = set(nodes)\n",
    "    neighborhoods = {}  \n",
    "    c = 0\n",
    "    original_to_new = ind_mapping(nodes)\n",
    "    for node in nodes:\n",
    "        neighbors = edge_dict[node]\n",
    "        # filter out neighbors that are not part of the subgraph   \n",
    "        neighbors = [original_to_new[n] for n in neighbors if n in unique_nodes] \n",
    "        nb = len(neighbors)\n",
    "        sample_size = min(nb_size, nb)\n",
    "        if sample_size==0:\n",
    "            neighborhoods[original_to_new[node]] = []\n",
    "            continue\n",
    "        if sample_size == 1:\n",
    "            sample_neighborhood = [neighbors[0]]*nb_size\n",
    "        else:\n",
    "            neighborhood_ind = torch.randint(0, nb, (sample_size,))\n",
    "            sample_neighborhood = [neighbors[i] for i in neighborhood_ind.tolist()]\n",
    "\n",
    "        neighborhoods[original_to_new[node]] = sample_neighborhood\n",
    "        c += 1\n",
    "    return neighborhoods\n",
    "\n",
    "\n",
    "N_classes = 15#total_graph['N_classes']\n",
    "mb = MultiLabelBinarizer(classes=[i for i in range(N_classes)])\n",
    "f1_macro_list = []\n",
    "f1_micro_list = []\n",
    "\n",
    "\n",
    "# 5-fold cross validation\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        print(i)\n",
    "        training_nodes = torch.tensor(NC_5folds[i]['train'])\n",
    "        test_nodes = torch.tensor(NC_5folds[i]['test'])\n",
    "        training_features = node_features_torch[training_nodes]\n",
    "        test_features = node_features_torch[test_nodes]\n",
    "\n",
    "        neighborhoods_train = compute_neighborhoods_subgraph(total_graph['edges'], NC_5folds[i]['train'], 25)\n",
    "        neighborhoods_test = compute_neighborhoods_subgraph(total_graph['edges'], NC_5folds[i]['test'], 25)\n",
    "\n",
    "        X_train = model.infer(training_features, neighborhoods_train)\n",
    "        X_test = model.infer(test_features, neighborhoods_test)\n",
    "        # For the datasets that only have one one label per node, it gives better results to not use multioutputclassifier\n",
    "        if not total_graph['Multioutput']:\n",
    "            yt = []\n",
    "            for n in NC_5folds[i]['train']:\n",
    "                if len(total_graph['groups'][n]):\n",
    "                    yt.append(total_graph['groups'][n][0])\n",
    "                else:\n",
    "                    yt.append(0)\n",
    "            #Y_train_sequence = np.array([total_graph['groups'][node][0]  for node in  NC_5folds[i]['train']],dtype=int)\n",
    "            Y_train_sequence = np.array(yt,dtype=int)\n",
    "            yt = []\n",
    "            for n in NC_5folds[i]['test']:\n",
    "                if len(total_graph['groups'][n]):\n",
    "                    yt.append(total_graph['groups'][n][0])\n",
    "                else:\n",
    "                    yt.append(0)\n",
    "            #Y_test_sequence = np.array([total_graph['groups'][node][0] for node in  NC_5folds[i]['test'] if len(total_graph['groups'][node])], dtype=int)\n",
    "            Y_test_sequence = np.array(yt, dtype=int)\n",
    "            log_reg = LogisticRegression(multi_class=\"ovr\", max_iter=200)\n",
    "            Y_train = Y_train_sequence\n",
    "            Y_test = Y_test_sequence\n",
    "            log_reg.fit(X_train, Y_train)\n",
    "            Y_pred = log_reg.predict(X_test)\n",
    "            Y_pred = onehot(Y_pred, N_classes)\n",
    "            Y_test = onehot(Y_test, N_classes)\n",
    "        else:\n",
    "            print(\"fitting model\")\n",
    "            Y_train_sequence = [total_graph['groups'][node]  for node in NC_5folds[i]['train']]\n",
    "            Y_test_sequence = [total_graph['groups'][node] for node in NC_5folds[i]['test']]\n",
    "            Y_train = mb.fit_transform(Y_train_sequence)\n",
    "            Y_test = mb.fit_transform(Y_test_sequence)\n",
    "            log_reg = MultiOutputClassifier(SGDClassifier(max_iter=1000))   #multi_class=\"ovr\",\n",
    "            log_reg.fit(X_train, Y_train)\n",
    "            Y_pred = log_reg.predict(X_test)\n",
    "    \n",
    "        f1_macro = compute_f1_macro(Y_test, Y_pred, N_classes)\n",
    "        f1_micro = compute_f1_micro(Y_test, Y_pred, N_classes)\n",
    "\n",
    "        f1_macro_list.append(f1_macro)\n",
    "        f1_micro_list.append(f1_micro)\n",
    "        print(f1_macro, f1_micro)\n",
    "\n",
    "        \n",
    "    print(np.mean(f1_micro_list))\n",
    "    print(np.mean(f1_macro_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting graphs\n",
      "0.09994003597841296\n",
      "0.19988007195682592\n",
      "0.29982010793523883\n",
      "0.39976014391365183\n",
      "0.4997001798920648\n",
      "0.5996402158704777\n",
      "0.6995802518488906\n",
      "0.7995202878273037\n",
      "0.8994603238057166\n",
      "0.9994003597841296\n",
      "balancing test graph\n",
      "0.1999466986474782\n",
      "0.3998933972949564\n",
      "0.5998400959424346\n",
      "0.7997867945899128\n",
      "0.9997334932373909\n",
      "balancing training graph\n",
      "0.09994003597841296\n",
      "0.19988007195682592\n",
      "0.29982010793523883\n",
      "0.39976014391365183\n",
      "0.4997001798920648\n",
      "0.5996402158704777\n",
      "0.6995802518488906\n",
      "0.7995202878273037\n",
      "0.8994603238057166\n",
      "0.9994003597841296\n"
     ]
    }
   ],
   "source": [
    "# Select 50% of the edges for training, leave remaining for testing.\n",
    "# Want the remaining graph to still be connected, so we only remove edges if there are several neighbors\n",
    "\n",
    "def split_graphs(total_graph, directed=False):\n",
    "    print(\"splitting graphs\")\n",
    "    n_test_samples = int(total_graph['N_edges']*0.5)\n",
    "    training_graph_unbalanced = deepcopy(total_graph[\"edges\"])\n",
    "    test_graph_unbalanced = {i:[] for i in range(total_graph['N_nodes'])}\n",
    "    LP_test_X = [(1,1)]*n_test_samples*2\n",
    "    LP_test_Y = [0]*n_test_samples*2\n",
    "    counter = 0\n",
    "\n",
    "    high_degree_nodes = []\n",
    "    n_neighbors = {i:0 for i in range(total_graph['N_nodes'])}\n",
    "    for i in range(total_graph['N_nodes']):\n",
    "        nb_count = len(total_graph['edges'][i]) \n",
    "        n_neighbors[i] = nb_count\n",
    "        if nb_count>1:\n",
    "            high_degree_nodes.append(i)\n",
    "\n",
    "\n",
    "    #print(high_degree_nodes[0:10])\n",
    "    while counter<n_test_samples:\n",
    "        node1 = random.choice(high_degree_nodes)\n",
    "        if n_neighbors[node1]>1:\n",
    "            for node2 in training_graph_unbalanced[node1]:\n",
    "                if n_neighbors[node2]>1:\n",
    "                    # Add to test data\n",
    "                    LP_test_X[counter] = (node1, node2)\n",
    "                    LP_test_Y[counter] = 1\n",
    "                    test_graph_unbalanced[node1].append(node2)\n",
    "\n",
    "                    # remove edge from training graph\n",
    "                    training_graph_unbalanced[node1].remove(node2)\n",
    "                       \n",
    "                    # decrease neighbor count\n",
    "                    n_neighbors[node1] -= 1\n",
    "\n",
    "                    # add/remove reverse edge in case of undirected graphs                 \n",
    "                    if not directed:\n",
    "                        test_graph_unbalanced[node2].append(node1)\n",
    "                        try:\n",
    "                            training_graph_unbalanced[node2].remove(node1)\n",
    "                        except:\n",
    "                            print(training_graph_unbalanced[node2], node1, training_graph_unbalanced[node1])\n",
    "                            raise\n",
    "                        n_neighbors[node2] -= 1\n",
    "     \n",
    "\n",
    "                    counter += 1          \n",
    "                    if counter%int(n_test_samples/10)==0:\n",
    "                        print(counter/n_test_samples)\n",
    "                    break\n",
    "\n",
    "    return LP_test_X, LP_test_Y, training_graph_unbalanced, test_graph_unbalanced\n",
    "\n",
    "\n",
    "def balance_test_graph(LP_test_X, LP_test_Y, test_graph_unbalanced, directed=False, reverse_fraction=0.5):\n",
    "    print(\"balancing test graph\")\n",
    "    counter = 0\n",
    "    n_test_samples = int(total_graph['N_edges']*0.5)\n",
    "    # in case of directed graphs, a fraction of the negative edges are added by reversing true edges\n",
    "    if directed and reverse_fraction:\n",
    "        true_edges = LP_test_X[0:n_test_samples]\n",
    "        while counter<int(n_test_samples*reverse_fraction):\n",
    "            true_edge = random.choice(true_edges)\n",
    "            src = true_edge[0]\n",
    "            target = true_edge[1]\n",
    "            if not src in test_graph_unbalanced.get(target):\n",
    "                LP_test_X[n_test_samples+counter] = (target, src)\n",
    "                counter += 1\n",
    "            \n",
    "            if counter%int(n_test_samples/10)==0:\n",
    "                print(counter/n_test_samples)\n",
    "\n",
    "    while counter<n_test_samples:\n",
    "        node1, node2 = random.sample(total_graph['nodes'], 2)\n",
    "        if not node1 in test_graph_unbalanced[node2]:\n",
    "            try:\n",
    "                LP_test_X[n_test_samples+counter] = (node1, node2)\n",
    "                LP_test_Y[n_test_samples+counter] = 0\n",
    "            except:\n",
    "                LP_test_X.append((node1, node2))\n",
    "                LP_test_Y.append(0)\n",
    "                print(\"appended edge\")\n",
    "            counter += 1\n",
    "    \n",
    "        if counter%int(n_test_samples/5)==0:\n",
    "            print(counter/n_test_samples)\n",
    "    return LP_test_X, LP_test_Y\n",
    "\n",
    "# When created the test set, we add remaining edges to the training set\n",
    "# and add negative edges to balance the training data\n",
    "def balance_training_graph(training_graph_unbalanced, total_graph, directed=False):\n",
    "    print(\"balancing training graph\")\n",
    "    n_test_samples = int(total_graph['N_edges']*0.5)\n",
    "    LP_train_X = []\n",
    "    LP_train_Y = []\n",
    "    added_edges = {i:{} for i in range(total_graph['N_nodes'])}\n",
    "    for node, neighbors in training_graph_unbalanced.items():\n",
    "        for nb in neighbors:\n",
    "            if not added_edges[node].get(nb, False):\n",
    "                added_edges[node][nb] = True\n",
    "                if not directed:\n",
    "                    added_edges[nb][node] = True\n",
    "                LP_train_X.append((node, nb))\n",
    "                LP_train_Y.append(1)\n",
    "\n",
    "    n_negative_edges = 0\n",
    "    while n_negative_edges < n_test_samples:\n",
    "        node1, node2 = random.sample(total_graph['nodes'], 2)\n",
    "        if not node1 in training_graph_unbalanced[node2]:\n",
    "            LP_train_X.append((node1, node2))\n",
    "            LP_train_Y.append(0)\n",
    "            n_negative_edges += 1\n",
    "\n",
    "        if n_negative_edges%int(n_test_samples/10)==0:\n",
    "            print(n_negative_edges/n_test_samples)\n",
    "        \n",
    "    return LP_train_X, LP_train_Y\n",
    "\n",
    "\n",
    "reverse_fraction = 0\n",
    "LP_test_X_unb, LP_test_Y_unb, training_graph_unbalanced, test_graph_unbalanced = split_graphs(total_graph, directed=directed_graph)\n",
    "LP_test_X, LP_test_Y = balance_test_graph(LP_test_X_unb, LP_test_Y_unb, test_graph_unbalanced, directed=directed_graph, reverse_fraction=reverse_fraction)\n",
    "LP_train_X, LP_train_Y = balance_training_graph(training_graph_unbalanced, total_graph, directed=directed_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit model\n",
      "0.6605648266269324\n"
     ]
    }
   ],
   "source": [
    "Y_train = LP_train_Y\n",
    "Y_test = LP_test_Y\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def get_edge_representation(fu,fv):\n",
    "    return torch.sigmoid(torch.dot(fu,fv))\n",
    "\n",
    "\n",
    "neighborhoods_train = compute_neighborhoods_subgraph(training_graph_unbalanced, total_graph['nodes'], nb_size)\n",
    "neighborhoods_test = compute_neighborhoods_subgraph(test_graph_unbalanced, total_graph['nodes'], nb_size)\n",
    "node_features_np = get_node_features(total_graph, mode=feature_type)\n",
    "node_features_torch = torch.tensor(node_features_np).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # build representation of edge datasets using inner product of the representation of the two nodes\n",
    "    X_train = np.zeros((len(LP_train_X), 1))\n",
    "    Z_train = model.infer(node_features_torch, neighborhoods_train)\n",
    "    for i, edge in enumerate(LP_train_X):\n",
    "        u = edge[0]\n",
    "        v = edge[1]\n",
    "        X_train[i] = get_edge_representation(Z_train[u], Z_train[v])\n",
    "    X_test = np.zeros((len(LP_test_X), 1))\n",
    "    Z_test = model.infer(node_features_torch, neighborhoods_test)\n",
    "    for i, edge in enumerate(LP_test_X):\n",
    "        u = edge[0]\n",
    "        v = edge[1]\n",
    "        X_test[i] = get_edge_representation(Z_test[u], Z_test[v])\n",
    "        \n",
    "    print(\"fit model\")\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    Y_probs = classifier.predict_proba(X_test)[:,1]\n",
    "    roc_auc = roc_auc_score(Y_test, Y_probs)\n",
    "    print(roc_auc)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"../Results/graphsage/{}_metrics{}.csv\".format(dataset_name, reverse_fraction), \"w\") as file:\n",
    "    settings_str = \"Results for graphsage embedding generated with {} epochs, K={}, Q={}, nb_size={}\\n\".format(num_epochs, K, Q, nb_size)\n",
    "    file.write(settings_str)\n",
    "    header = \"Dataset; F1 macro; F1 micro; ROC-AUC \\n\"\n",
    "    file.write(header)\n",
    "    data_row = \"{dataset};{f1mac};{f1mic};{roc}\".format(dataset=dataset_name, f1mac=np.mean(f1_macro_list), f1mic=np.mean(f1_micro_list), roc=roc_auc)\n",
    "    file.write(data_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
