{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from load_data import load_toy, load_blogcatalog\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_name = \"BlogCatalog\"\n",
    "data_dir = \"../Data/\" + dataset_name\n",
    "\n",
    "total_graph = load_blogcatalog(data_dir)\n",
    "#total_graph = load_toy(data_dir)\n",
    "\n",
    "\n",
    "def get_similarity(total_graph):\n",
    "    \"\"\"Construct a dict of similar nodes, i.e. ones that are within distance 3 of each other\"\"\"\n",
    "    similarity_dict = {i:set() for i in range(total_graph['N_nodes'])}\n",
    "    for i in range(total_graph['N_nodes']):\n",
    "        if i%1000==0:\n",
    "            print(i)\n",
    "        nb = total_graph['edges'][i]\n",
    "        similarity_dict[i].update(nb)\n",
    "        for n in nb:\n",
    "            new_neighbors = total_graph['edges'][n]\n",
    "            similarity_dict[i].update(new_neighbors)\n",
    "            #for j in new_neighbors:\n",
    "            #    similarity_dict[i].update(total_graph['edges'][j])       \n",
    "    return similarity_dict\n",
    "\n",
    "def get_node_features(total_graph, mode=\"degree\"):\n",
    "    if mode==\"degree\":\n",
    "        node_features = np.zeros((total_graph['N_nodes'],2))\n",
    "        for i in range(total_graph['N_nodes']):\n",
    "            neighbors = total_graph['edges'][i]\n",
    "            degree = len(neighbors)\n",
    "            node_features[i,0] = degree\n",
    "            second_neighbors_count = 0\n",
    "            for n in neighbors:\n",
    "                second_neighbors_count += len(total_graph['edges'][n])\n",
    "            node_features[i,1] = second_neighbors_count\n",
    "    elif mode==\"degree_dist\":\n",
    "        node_features = total_graph['adj_matrix']  \n",
    "    elif mode==\"node_nr\":\n",
    "        node_features = np.zeros((total_graph['N_nodes'],total_graph['N_nodes']))\n",
    "        for i in range(total_graph['N_nodes']):\n",
    "            node_features[i,i] = 1\n",
    "    return node_features\n",
    "\n",
    "\n",
    "#sim_dict = get_similarity(total_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GraphSage(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, K):\n",
    "        super(GraphSage, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_dim, output_dim))\n",
    "        for i in range(K-1):\n",
    "            self.layers.append(nn.Linear(output_dim, output_dim))\n",
    "        self.K = K\n",
    "    \n",
    "    \n",
    "    def forward(self, batch, neighborhoods):\n",
    "        #need node_features, batch_nodes\n",
    "        batch_nodes = batch.batch\n",
    "        eps = 1e-9\n",
    "        B = [[] for k in range(K)]\n",
    "        B[K-1] = batch_nodes[:]\n",
    "        for k in range(K-1, 0, -1):\n",
    "            B[k-1] = B[k][:]\n",
    "            for node in B[k][:]:\n",
    "                B[k-1].extend(neighborhoods[node])\n",
    "\n",
    "        h = batch.x \n",
    "        N_nodes = h.shape[0]\n",
    "        for k in range(self.K):\n",
    "            for v in B[k]:        \n",
    "                neighborhood = neighborhoods[v]\n",
    "                hv = h[v].view(1, -1)\n",
    "                hN = h[neighborhood]\n",
    "                conc = torch.cat([hN, hv], dim=0)\n",
    "                aggregated = torch.mean(conc, dim=0, keepdim=True)\n",
    "                layer = self.layers[k]\n",
    "                hv = torch.nn.ReLU(layer(aggregated))\n",
    "                h[v] = hv\n",
    "\n",
    "            h = h / (torch.norm(h, axis=1, keepdims=True)+eps)\n",
    "        return h\n",
    "\n",
    "\n",
    "def compute_neighborhoods(edge_dict, N_nodes, nb_size):\n",
    "    neighborhoods = [[] for _ in range(N_nodes)]\n",
    "    for v, neighbors in edge_dict.items():\n",
    "        nb = len(neighbors)\n",
    "        sample_size = min(nb_size, nb)\n",
    "        if sample_size == 1:\n",
    "            sample_neighborhood = [neighbors[0]]\n",
    "        else:\n",
    "            neighborhood_ind = torch.randint(0, nb, (sample_size,))\n",
    "            sample_neighborhood = [neighbors[i] for i in neighborhood_ind.tolist()]\n",
    "        neighborhoods[v] = sample_neighborhood\n",
    "    return neighborhoods\n",
    "\n",
    "\n",
    "\n",
    "def custom_loss(Z, pos_samples, neg_samples):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10312, 10312])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "GraphSage.forward() missing 1 required positional argument: 'neighborhoods'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Extract input and target from subset\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m Z \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m custom_loss(Z, pos_samples, negative_samples)\n\u001b[1;32m     28\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python310/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python310/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: GraphSage.forward() missing 1 required positional argument: 'neighborhoods'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "\n",
    "node_features_np = get_node_features(total_graph, mode=\"degree_dist\")\n",
    "node_features_torch = torch.tensor(node_features_np)\n",
    "node_links_torch = torch.tensor(total_graph['edges_list']).T\n",
    "graph_data = torch_geometric.data.Data(node_features_torch, node_links_torch)\n",
    "\n",
    "print(graph_data.x.shape)\n",
    "# parameters\n",
    "num_epochs = 1\n",
    "K = 1   # number of iterations\n",
    "batch_size = 128\n",
    "input_size = node_features_np.shape[1]\n",
    "output_dim = 64\n",
    "\n",
    "# Create the model\n",
    "model = GraphSage(input_size, output_dim, K)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loader = LinkNeighborLoader(graph_data, batch_size=batch_size, num_neighbors=[10]*K, edge_label_index=graph_data.edge_index)  \n",
    "    for subset in loader:\n",
    "        optimizer.zero_grad()\n",
    "        # Extract input and target from subset\n",
    "        Z = model(input)\n",
    "        loss = custom_loss(Z, pos_samples, negative_samples)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NC_5folds = {}\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "nodes = np.array([i for i in range(total_graph['N_nodes'])])\n",
    "for i, (train_index, test_index) in enumerate(kf.split(nodes)):  \n",
    "    NC_5folds[i] = {\"train\":list(nodes[train_index]), \"test\":list(nodes[test_index])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression, SGDClassifier\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultioutput\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiOutputClassifier\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m  \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiLabelBinarizer\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python310/lib/python3.10/site-packages/sklearn/linear_model/__init__.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stochastic_gradient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SGDClassifier, SGDRegressor, SGDOneClassSVM\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ridge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ridge, RidgeCV, RidgeClassifier, RidgeClassifierCV, ridge_regression\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logistic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression, LogisticRegressionCV\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_omp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     39\u001b[0m     orthogonal_mp,\n\u001b[1;32m     40\u001b[0m     orthogonal_mp_gram,\n\u001b[1;32m     41\u001b[0m     OrthogonalMatchingPursuit,\n\u001b[1;32m     42\u001b[0m     OrthogonalMatchingPursuitCV,\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_passive_aggressive\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PassiveAggressiveClassifier\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python310/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_loss\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HalfBinomialLoss, HalfMultinomialLoss\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder, LabelBinarizer\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _fit_liblinear\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_array, check_consistent_length, compute_class_weight\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_random_state\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python310/lib/python3.10/site-packages/sklearn/svm/__init__.py:13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`sklearn.svm` module includes Support Vector Machine algorithms.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# See http://scikit-learn.sourceforge.net/modules/svm.html for complete\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# documentation.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#         of their respective owners.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# License: BSD 3 clause (C) INRIA 2010\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_classes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC, NuSVC, SVR, NuSVR, OneClassSVM, LinearSVC, LinearSVR\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bounds\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m l1_min_c\n\u001b[1;32m     16\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinearSVC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinearSVR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1_min_c\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python310/lib/python3.10/site-packages/sklearn/svm/_classes.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumbers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Integral, Real\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _fit_liblinear, BaseSVC, BaseLibSVM\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator, RegressorMixin, OutlierMixin\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearClassifierMixin, SparseCoefMixin, LinearModel\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python310/lib/python3.10/site-packages/sklearn/svm/_base.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# mypy error: error: Module 'sklearn.svm' has no attribute '_libsvm'\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# (and same for other imports)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _libsvm \u001b[38;5;28;01mas\u001b[39;00m libsvm  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _liblinear \u001b[38;5;28;01mas\u001b[39;00m liblinear  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _libsvm_sparse \u001b[38;5;28;01mas\u001b[39;00m libsvm_sparse  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator, ClassifierMixin\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:404\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from  sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def onehot(y, nclasses):\n",
    "    Y = np.zeros((y.shape[0], nclasses), dtype=int)\n",
    "    for i in range(y.shape[0]):\n",
    "        c = y[i]\n",
    "        Y[i,c-1] =  1\n",
    "    return Y\n",
    "\n",
    "\n",
    "def precision_and_recall(Y_true, Y_pred, nclasses):\n",
    "    # count true positives and false positives and false negatives\n",
    "    TP_list = [0]*nclasses\n",
    "    FP_list = [0]*nclasses\n",
    "    FN_list = [0]*nclasses\n",
    "    for j in range(nclasses):\n",
    "       for i, pred in enumerate(Y_pred):\n",
    "            if pred[j]==1 and Y_true[i][j]==1:\n",
    "                TP_list[j] += 1\n",
    "            elif pred[j]==1 and  Y_true[i][j]==0:\n",
    "                FP_list[j] += 1\n",
    "            elif pred[j]==0 and Y_true[i][j]==1:\n",
    "                FN_list[j] += 1 \n",
    "\n",
    "    return TP_list, FP_list, FN_list\n",
    "\n",
    "def compute_f1_macro(Y_true, Y_pred, nclasses):\n",
    "    TP_list, FP_list, FN_list = precision_and_recall(Y_true, Y_pred, nclasses)\n",
    "    f1_scores = [0]*nclasses\n",
    "    for k in range(nclasses):\n",
    "        if TP_list[k]==0:\n",
    "            continue\n",
    "        f1_scores[k] = TP_list[k]/(TP_list[k]+0.5*(FP_list[k]+FN_list[k])) \n",
    "    return np.sum(f1_scores)/nclasses\n",
    "\n",
    "\n",
    "def compute_f1_micro(Y_true, Y_pred, nclasses):\n",
    "    TP_list, FP_list, FN_list = precision_and_recall(Y_true, Y_pred, nclasses)\n",
    "    TP = np.sum(TP_list)\n",
    "    FP = np.sum(FP_list)\n",
    "    FN = np.sum(FN_list)\n",
    "    print(TP, FP, FN)\n",
    "    return TP/(TP + 0.5*(FN+FP))\n",
    "\n",
    "\n",
    "N_classes = total_graph['N_classes']\n",
    "mb = MultiLabelBinarizer(classes=[i for i in range(N_classes)])\n",
    "f1_macro_list = []\n",
    "f1_micro_list = []\n",
    "\n",
    "# 5-fold cross validation\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    node_features = get_node_features(total_graph, \"degree_dist\") \n",
    "    training_nodes = NC_5folds[i]['train']\n",
    "    test_nodes = NC_5folds[i]['test']\n",
    "    Z_train = tf.gather(batch_forward(node_features, training_nodes, K, model, neighborhoods), training_nodes).numpy()\n",
    "    print(\"klar med z train\")\n",
    "    Z_test = tf.gather(batch_forward(node_features, test_nodes, K, model, neighborhoods), test_nodes).numpy()\n",
    "    X_train = Z_train\n",
    "    X_test = Z_test\n",
    "    # For the datasets that only have one one label per node, it gives better results to not use multioutputclassifier\n",
    "    if not total_graph['Multioutput']:\n",
    "        Y_train_sequence = np.array([total_graph['groups'][node][0]  for node in training_nodes],dtype=int)\n",
    "        Y_test_sequence = np.array([total_graph['groups'][node][0] for node in test_nodes], dtype=int)\n",
    "        log_reg = LogisticRegression(multi_class=\"ovr\", max_iter=200)\n",
    "        Y_train = Y_train_sequence\n",
    "        Y_test = Y_test_sequence\n",
    "        log_reg.fit(X_train, Y_train)\n",
    "        Y_pred = log_reg.predict(X_test)\n",
    "        Y_pred = onehot(Y_pred, total_graph['N_classes'])\n",
    "        Y_test = onehot(Y_test, total_graph['N_classes'])\n",
    "    else:\n",
    "        print(\"hej\")\n",
    "        Y_train_sequence = [total_graph['groups'][node]  for node in training_nodes]\n",
    "        Y_test_sequence = [total_graph['groups'][node] for node in test_nodes]\n",
    "        Y_train = mb.fit_transform(Y_train_sequence)\n",
    "        Y_test = mb.fit_transform(Y_test_sequence)\n",
    "        log_reg = MultiOutputClassifier(SGDClassifier(max_iter=200))   #multi_class=\"ovr\",\n",
    "        log_reg.fit(X_train, Y_train)\n",
    "        Y_pred = log_reg.predict(X_test)\n",
    "  \n",
    "    f1_macro = compute_f1_macro(Y_test, Y_pred, N_classes)\n",
    "    f1_micro = compute_f1_micro(Y_test, Y_pred, N_classes)\n",
    "\n",
    "    f1_macro_list.append(f1_macro)\n",
    "    f1_micro_list.append(f1_micro)\n",
    "    print(f1_macro, f1_micro)\n",
    "    sys.exit()\n",
    "    \n",
    "print(np.mean(f1_micro_list))\n",
    "print(np.mean(f1_macro_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
